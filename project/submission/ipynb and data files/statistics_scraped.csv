Link,Description
https://www.classcentral.com/course/bayesian-statistics-6633,"This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will learn about the philosophy of the Bayesian approach as well as how to implement it for common types of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach. In particular, the Bayesian approach allows for better accounting of uncertainty, results that have more intuitive and interpretable meaning, and more explicit statements of assumptions. This course combines lecture videos, computer demonstrations, readings, exercises, and discussion boards to create an active learning experience. For computing, you have the choice of using Microsoft Excel or the open-source, freely available statistical package R, with equivalent content for both options. The lectures provide some of the basic mathematical development as well as explanations of philosophy and interpretation. Completion of this course will give you an understanding of the concepts of the Bayesian approach, understanding the key differences between Bayesian and Frequentist approaches, and the ability to do basic data analyses.
      


          Probability and Bayes' Theorem
    -In this module, we review the basics of probability and Bayes’ theorem. In Lesson 1, we introduce the different paradigms or definitions of probability and discuss why probability provides a coherent framework for dealing with uncertainty. In Lesson 2, we review the rules of conditional probability and introduce Bayes’ theorem. Lesson 3 reviews common probability distributions for discrete and continuous random variables.

Statistical Inference
    -This module introduces concepts of statistical inference from both frequentist and Bayesian perspectives. Lesson 4 takes the frequentist view, demonstrating maximum likelihood estimation and confidence intervals for binomial data. Lesson 5 introduces the fundamentals of Bayesian inference. Beginning with a binomial likelihood and prior probabilities for simple hypotheses, you will learn how to use Bayes’ theorem to update the prior with data to obtain posterior probabilities. This framework is extended with the continuous version of Bayes theorem to estimate continuous model parameters, and calculate posterior probabilities and credible intervals.

Priors and Models for Discrete Data
    -In this module, you will learn methods for selecting prior distributions and building models for discrete data. Lesson 6 introduces prior selection and predictive distributions as a means of evaluating priors. Lesson 7 demonstrates Bayesian analysis of Bernoulli data and introduces the computationally convenient concept of conjugate priors. Lesson 8 builds a conjugate model for Poisson data and discusses strategies for selection of prior hyperparameters.

Models for Continuous Data
    -This module covers conjugate and objective Bayesian analysis for continuous data. Lesson 9 presents the conjugate model for exponentially distributed data. Lesson 10 discusses models for normally distributed data, which play a central role in statistics. In Lesson 11, we return to prior selection and discuss ‘objective’ or ‘non-informative’ priors. Lesson 12 presents Bayesian linear regression with non-informative priors, which yield results comparable to those of classical regression."
https://www.classcentral.com/course/edx-statistics-and-r-2960,"This course teaches the R programming language in the context of statistical data and statistical analysis in the life sciences.
We will learn the basics of statistical inference in order to understand and compute p-values and confidence intervals, all while analyzing data with R code. We provide R programming examples in a way that will help make the connection between concepts and implementation. Problem sets requiring R programming will be used to test understanding and ability to implement basic data analyses. We will use visualization techniques to explore new data sets and determine the most appropriate approach. We will describe robust statistical techniques as alternatives when data do not fit assumptions required by the standard approaches. By using R scripts to analyze data, you will learn the basics of conducting reproducible research.
Given the diversity in educational background of our students we have divided the course materials into seven parts. You can take the entire series or individual courses that interest you. If you are a statistician you should consider skipping the first two or three courses, similarly, if you are biologists you should consider skipping some of the introductory biology lectures. Note that the statistics and programming aspects of the class ramp up in difficulty relatively quickly across the first three courses. We start with simple calculations and descriptive statistics. By the third course will be teaching advanced statistical concepts such as hierarchical models and by the fourth advanced software engineering skills, such as parallel computing and reproducible research concepts.
These courses make up 2 XSeries and are self-paced:
PH525.1x: Statistics and R for the Life Sciences
PH525.2x: Introduction to Linear Models and Matrix Algebra
PH525.3x: Statistical Inference and Modeling for High-throughput Experiments
PH525.4x: High-Dimensional Data Analysis
PH525.5x: Introduction to Bioconductor: annotation and analysis of genomes and genomic assays 
PH525.6x: High-performance computing for reproducible genomics
PH525.7x: Case studies in functional genomics
This class was supported in part by NIH grant R25GM114818.



            Read more"
https://www.classcentral.com/course/edx-fundamentals-of-statistics-11482,"Statistics is the science of turning data into insights and ultimately decisions. Behind recent advances in machine learning, data science and artificial intelligence are fundamental statistical principles. The purpose of this class is to develop and understand these core ideas on firm mathematical grounds starting from the construction of estimators and tests, as well as an analysis of their asymptotic performance. 
After developing basic tools to handle parametric models, we will explore how to answer more advanced questions, such as the following: 

How suitable is a given model for a particular dataset?
How to select variables in linear regression?
How to model nonlinear phenomena?
How to visualize high-dimensional data?

Taking this class will allow you to expand your statistical knowledge to not only include a list of methods, but also the mathematical principles that link them together, equipping you with the tools you need to develop new ones. 
This course is part of theMITx MicroMasters Program in Statistics and Data Science. Master the skills needed to be an informed and effective practitioner of data science. You will complete this course and three others from MITx, at a similar pace and level of rigor as an on-campus course at MIT, and then take a virtually-proctored exam to earn your MicroMasters, an academic credential that will demonstrate your proficiency in data science or accelerate your path towards an MIT PhD or a Master's at other universities. To learn more about this program, please visit https://micromasters.mit.edu/ds/.



            Read more"
https://www.classcentral.com/course/edx-six-sigma-define-and-measure-8450,"Understand the background and meaning of Six Sigma and the five steps of the DMAIC process improvement flow: Define, Measure, Analyse, Improve and Control. Discuss what ""Quality"" means and how to identify the Voice of the Customer.
You will learn how to set an improvement project goal, calculate process yield, and identify Critical to Quality parameters.
You will learn how to map a process and to use the necessary statistical techniques to establish the baseline performance of a process and to calculate the process capability.
To complement the lectures, we provide interactive exercises, which allow learners to see the statistics ""in action."" Learners then master the statistical concepts by completing practice problems. These are then reinforced using interactive case-studies, which illustrate the application of the statistics in quality improvement situations.
Upon successful completion of this program, learners will earn the TUM Lean and Six Sigma Yellow Belt certification, confirming mastery of Lean Six Sigma fundamentals to a Yellow Belt level.The material is based on the American Society of Quality (www.asq.org) Body of Knowledge up to a Green Belt Level. The Professional Certificate is designed as preparation for a Lean Six Sigma Green Belt exam.



Week 1: Six Sigma Introduction
Introduction to the Six Sigma Methodology and the DMAIC process improvement cycle. Understand the contributors to thecost of quality. Discuss the difference between defects and defectives in a process and how to calculate process yield, including comparison of processes of different complexity using the metric DPMO. 
Week 2: DEFINE - Defining the Problem
Discuss how to understand customer expectations, using the Kano Model to categorize quality characteristics. Start the first and difficult task of a Six Sigma project, Defining the Problem, and review the key content in a Project Charter. 
Week 3: MEASURE - Statistics Review
Review of random variables and probability distributions used commonly in quality engineering, such as Binomial, Poisson and Exponential. Cover descriptive statistics, emphasizing the importance of clearly communicating our results of our project. 
Week 4: MEASURE - Normal Distribution
Learn the characteristics of the Normal Distribution and how to use the Standard Normal to calculate probabilities related to normally distributed variables. Cover the Central Limit Theorem, and how it relates to sampling theory. 
Week 5: MEASURE - Process Mapping
Introduce Process Mapping, including SIPOC and Value Stream Mapping. We identify the Critical-to-Quality characteristic for a Six Sigma project 
Week 6: MEASURE - Measurement System Analysis
Learn the basics of Measurement Theory and Sampling Plans, including
Precision, Accuracy, Linearity, Bias, Stability, Gage Repeatability & Reproducibility 
Week 7: MEASURE - Process Capability
Introduction to Process Capability and the metrics CP/CPK for establishing our baseline process performance. 
Week 8: Quality Topics and Course Summary
Cover the basics of Tolerance Design and the risk assessemnt toolFailure Mode and Effects Analysis (FMEA).
Review the complete Six Sigma Roadmap before summarizing and closing the course."
https://www.classcentral.com/course/statistics-project-6100,"The capstone project will be an analysis using R that answers a specific scientific/business question provided by the course team. A large and complex dataset will be provided to learners and the analysis will require the application of a variety of methods and techniques introduced in the previous courses, including exploratory data analysis through data visualization and numerical summaries, statistical inference, and modeling as well as interpretations of these results in the context of the data and the research question. The analysis will implement both frequentist and Bayesian techniques and discuss in context of the data how these two approaches are similar and different, and what these differences mean for conclusions that can be drawn from the data.

A sampling of the final projects will be featured on the Duke Statistical Science department website.

Note: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone.
      


          About the Capstone Project
    -Welcome to the capstone project! This week's content is an introduction to the project assignment and goals. The readings in this week will introduce the data set that you will be analyzing for your project and the specific questions you will answer using data analysis techniques we learned in the previous courses. It is important to understand what we will be doing in the course before jumping into the detailed analysis. So we encourage you to start with the first lecture to get the big picture, and then delve into the specifics of the analysis. Enjoy, and good luck! Remember, if you have questions, you can post them on the discussion forums.

Exploratory Data Analysis (EDA)
    -This week you will work on conducting an exploratory analysis of the housing data. Exploratory analysis is an essential first step for familiarizing yourself with and understanding the data. 

In this week, you will complete a quiz which will guide you through certain important aspects of the data. The insights you gain through this assignment will help inform modeling in the future quizzes and peer assessments. 

Feel free to post questions about this assignment on the discussion forum. 

EDA and Basic Model Selection - Submission
    -This week we will dig deeper into our exploratory data analysis of the data. We now have all the information and data necessary to perform a deep dive into the EDA and it is time start your initial analysis report! We encourage you to start your analysis report (presented in peer-review format next week) early so you will have enough time to complete it. You will conduct exploratory data analysis, model selection, and model evaluation, and then complete a written report which answers several questions which will guide you through the process. This report will be your first peer-review assignment in this course. 

EDA and Basic Model Selection - Evaluation
    -Great work so far! We hope you will also learn as much from evaluating your peers' work as completing your own assignment. Happy learning!

Model Selection and Diagnostics
    -We are half way through the course! In this week, you will continue model selection and model diagnostics, which will serve a starting point for your final project. You will be assessed on your work through a quiz. If you have any questions so far, don't hesitate to post on the forum so that others can help and discuss the question together.

Out of Sample Prediction
    -In this week, you will gain experience using your model to perform out-of-sample prediction and validation.  The skills honed this week will guide you through your final analysis in the weeks to come.  Please feel free to go back to prior weeks and review the necessary background knowledge. 

Final Data Analysis - Submission
    -In the next two weeks, you will complete your final data analysis project. You will submit your answers using the Final Data Analysis peer review assignment link in Week 8.

Final Data Analysis - Evaluation
    -Congratulations on making through to the final week of the course! In this week, we will finish this data analysis project by completing the evaluation of three of your peers' assignments."
https://www.classcentral.com/course/business-statistics-analysis-capstone-7044,"The Business Statistics and Analysis Capstone is an opportunity to apply various skills developed across the four courses in the specialization to a real life data. The Capstone, in collaboration with an industry partner uses publicly available ‘Housing Data’ to pose various questions typically a client would pose to a data analyst.
Your job is to do the relevant statistical analysis and report your findings in response to the questions in a way that anyone can understand.
Please remember that this is a Capstone, and has a degree of difficulty/ambiguity higher than the previous four courses. The aim being to mimic a real life application as close as possible.
      


          Business Statistics and Analysis Capstone: An Introduction

Business Statistics and Analysis Capstone: Assessments 1 & 2

Business Statistics and Analysis Capstone: Assessment 3

Business Statistics and Analysis Capstone: Assessment 4"
https://www.classcentral.com/course/edx-foundations-of-data-analysis-part-1-statistics-using-r-4805,"In this first part of a two part course, we’ll walk through the basics of statistical thinking – starting with an interesting question. Then, we’ll learn the correct statistical tool to help answer our question of interest – using R and hands-on Labs. Finally, we’ll learn how to interpret our findings and develop a meaningful conclusion.
This course will consist of:

Instructional videos for statistical concepts broken down into manageable topics
Guided questions to help your understanding of the topic
Weekly tutorial videos for using R Scaffolded learning with Pre-Labs (using R), followed by Labs where we will answer specific questions using real-world datasets
Weekly wrap-up questions challenging both topic and application knowledge

We will cover basic Descriptive Statistics – learning about visualizing and summarizing data, followed by a “Modeling” investigation where we’ll learn about linear, exponential, and logistic functions. We will learn how to interpret and use those functions with basic Pre-Calculus. These two “units” will set the learner up nicely for the second part of the course: Inferential Statistics with a multiple regression cap.
Both parts of the course are intended to cover the same material as a typical introductory undergraduate statistics course, with an added twist of modeling. This course is also intentionally devised to be sequential, with each new piece building on the previous topics. Once completed, students should feel comfortable using basic statistical techniques to answer their own questions about their own data, using a widely available statistical software package (R).
With these new skills, learners will leave the course with the ability to use basic statistical techniques to answer their own questions about their own data, using a widely available statistical software package (R). Learners from all walks of life can use this course to better understand their data, to make valuable informed decisions.
Join us in learning how to look at the world around us. What are the questions? How can we answer them? And what do those answers tell us about the world we live in?



            Read more
          



Week One: Introduction to Data

Why study statistics?
Variables and data
Getting to know R and RStudio

Week Two: Univariate Descriptive Statistics

Graphs and distribution shapes
Measures of center and spread
The Normal distribution
Z-scores 

Week Three: Bivariate Distributions

The scatterplot
Correlation

Week Four: Bivariate Distributions (Categorical Data)

Contingency tables
Conditional probability
Examining independence

Week Five: Linear Functions

What is a function?
Least squares
The Linear function – regression 

Week Six: Exponential and Logistic Function Models

Exponential data
Logs
The Logistic function model
Picking a good mode"
https://www.classcentral.com/course/edx-probability-and-statistics-in-data-science-using-python-8213,"The job of a data scientist is to glean knowledge from complex and noisy datasets.
Reasoning about uncertainty is inherent in the analysis of noisy data. Probability and Statistics provide the mathematical foundation for such reasoning.
In this course, part of the Data Science MicroMasters program, you will learn the foundations of probability and statistics. You will learn both the mathematical theory, and get a hands-on experience of applying this theory to actual data using Jupyter notebooks.
Concepts covered included: random variables, dependence, correlation, regression, PCA, entropy and MDL."
https://www.classcentral.com/course/business-statistics-5243,"This course provides an analytical framework to help you evaluate key problems in a structured fashion and will equip you with tools to better manage the uncertainties that pervade and complicate business processes. The course aim to cover statistical ideas that apply to managers. We will consider two basic themes: first, is recognizing and describing variations present in everything around us, and then modeling and making decisions in the presence of these variations. The fundamental concepts studied in this course will reappear in many other classes and business settings. Our focus will be on  interpreting the meaning of the results in a business and managerial setting.

While you will be introduced to some of the science of what is being taught, the focus will be on applying the methodologies.  This will be accomplished through use of Excel and using data sets from many different disciplines, allowing you to see the use of statistics in very diverse settings. The course will focus not only on explaining these concepts but also understanding the meaning of the results obtained.

Upon successful completion of this course, you will be able to:
•	Test for beliefs about a population..
•	Compare differences between populations.
•	Use linear regression model for prediction. 
•	Learn how to use Excel for statistical analysis.

This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.
      


            Read more
          



          Course Orientation
    -In the course orientation, you will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.

Module 1: Hypothesis Testing
    -Watch any infomercial and you hear many outrageous promises.  Use this cream and your skin will look 80% firmer!  Use this supplement and you will lose 10 pounds in the first 10 days!  Are they telling you the truth?  Are they all lying? The only way to know the answer to any of these questions is to scientifically test the claim being made – that is what we call hypothesis testing and what we will learn in this module.

Module 2: Statistical Inference Based on Two Samples
    -Does the medicine a person is taking to treat his condition really work better than a sugar pill?  Is the new chip-enabled credit card more secure than the magnetic card? How do you know whether the claims being made about anything being “better than” or “faster than” a competitor are true?  In this module we will learn to make this comparison.

Module 3: Simple Linear Regression
    -Does your job involve a lot of sitting?  If so, you are at higher risk of coronary heart disease.  How do I know this?  We got to know the relationship between coronary heart disease and sitting when researchers studied a cohort of London bus drivers and bus conductors from 1947 to 1972.  If you want to know more, then read on!

Module 4: Multiple Linear Regression
    -You are trying to predict next month’s sales numbers.  You know that dozens, maybe even hundreds, of things like the weather, competitor’s promotions, rumors, etc. can impact the number. You talk to five people and each one has an idea about what makes the biggest impact, and the only thing they offer is “trust me.”  Do you wish there was a better way of doing this rather than relying on blind faith?  Well, there is.  We can use Multiple Regression to sort through this mess and bring the focus to factors that really do matter."
https://www.classcentral.com/course/edx-six-sigma-analyze-improve-control-8452,"Learn how to statistically analyze data with the Six Sigma methodology using inferential statistical techniques to determine confidence intervals and to test hypotheses based on sample data. You will also review cause and effect techniques for root cause analysis.
You will learn how to perform correlation and regression analyses in order to confirm the root cause and understand how to improve your process and plan designed experiments.
You will learn how to implement statistical process control using control charts and quality management tools, including the 8 Disciplines and Failure Modes and Effects Analysis to reduce risk and manage process deviations.
To complement the lectures, learners are provided with interactive exercises, which allow learners to see the statistics ""in action."" Learners then master the statistical concepts by completing practice problems. These are then reinforced using interactive case-studies, which illustrate the application of the statistics in quality improvement situations.
Upon successful completion of this program, learners will earn the TUM Lean and Six Sigma Yellow Belt certification, confirming mastery of Lean Six Sigma fundamentals to a Yellow Belt level. The material is based on the American Society of Quality (www.asq.org) Body of Knowledge up to a Green Belt Level. The Professional Certificate is designed as preparation for a Lean Six Sigma Green Belt exam.



Week 1: ANALYZE - Root Cause Analysis
 Introduction to methods for root cause analysis, including Cause and Effect (Fishbone diagrams) and Pareto Charts. We learn how to perform statistical correlations and regression analyses. ** 
Week 2: ANALYZE - Inferential Statistics**
Learn the inferential statistics techniques of confidence intervals and hypothesis testing in order to use sample data and draw conclusions about or process centering. 
Week 3: IMPROVE - Design of Experiments
Plan designed experiments and calculate the main and interaction effects. 
Week 4: MEASURE - Analysis of Variance
Review how to perform one-way Analysis of Variance (ANOVA) forcomparing the between-factor variaion to the within-factor variaion for a single factor experiment.
Use atwo-wayANOVA for testing the significance of the factor effects for a 2x2 DOE. 
Week 5: CONTROL - SPC and Control Charts
ImplementStatistical Process Control (SPC) & Control Chart Theory for monitoring process data and distinguishing between common cause variation and assignable cause variation. Construct X-bar and R Charts by calculating the upper and lower control limits and the center line. 
Week 6: CONTROL - Other Control Charts
Understandother control charts, including p-and c-charts and I/MR, and EWMA Charts and review of the Control and Reponse Plan for Six Sigma projects. 
Week 7: Quality Tools: FMEA, 8D, 5 Whys
Useseveral important tools used in quality management, including the 8 Disciplines (8D) and 5 Whys, and learn the concept behind Design for Six Sigma (DFSS). 
Week 8:Six Sigma Scenario and Course Summary
 Step through afullSix Sigma scenario, covering all phases of the DMAIC process improvement cycle."
https://www.classcentral.com/course/edx-explore-statistics-with-r-1836,"Do you want to learn how to harvest health science data from the Internet? Or learn to understand the world through data analysis? Start by learning R Statistics!
Skilled professionals who can process and analyze data are in great demand today. In this course you will explore concepts in statistics to make sense out of data. You will learn the practical skills necessary to find, import, analyze and visualize data. We will take a look under the hood of statistics and equip you with broad tools for understanding statistical inference and statistical methods. You will also perform some really complicated calculations and visualizations, following in the footsteps of Karolinska Institute’s researchers.
Statistical programming is an essential skill in our golden age of data abundance. Health science has become a field of big data, just like so many other fields of study. New techniques make it possible and affordable to generate massive data sets in biology. Researchers and clinicians can measure the activity for each of 30000 genes of a patient. They can read the complete genome sequence of a patient. Thanks to another trend of the decade, open access publishing, the results of such large scale health science are very often published for you to read free of charge. You can even access the raw data from open databases such as the gene expression database of the NCBI, National Center for Biotechnology Information.
We will dive into this data together. Learn how to use R, a powerful open source statistical programming language, and see why it has become the tool of choice in many industries in this introductory R statistics course. 



            Read more"
https://www.classcentral.com/course/edx-foundations-of-data-analysis-part-2-inferential-statistics-4804,"In the second part of a two part statistics course, we’ll learn how to take data and use it to make reasonable and useful conclusions. You’ll learn the basics of statistical thinking – starting with an interesting question and some data. Then, we’ll apply the correct statistical tool to help answer our question of interest – using R and hands-on Labs. Finally, we’ll learn how to interpret our findings and develop a meaningful conclusion.
We will cover basic Inferential Statistics – integrating ideas of Part 1. If you have a basic knowledge of Descriptive Statistics, this course is for you. We will learn how to sample data, examine both quantitative and categorical data with statistical techniques such as t-tests, chi-square, ANOVA, and Regression.
Both parts of the course are intended to cover the same material as a typical introductory undergraduate statistics course, with an added twist of modeling. This course is also intentionally devised to be sequential, with each new piece building on the previous topics. Once completed, students should feel comfortable using basic statistical techniques to answer their own questions about their own data, using a widely available statistical software package (R).
This course will consist of:

Instructional videos for statistical concepts broken down into manageable topics
Guided questions to help your understanding of the topic
Weekly tutorial videos for using R
Scaffolded learning with Pre-Labs (using R), followed by Labs where we will answer specific questions using real-world datasets
Weekly wrap-up questions challenging both topic and application knowledge

With these new skills, learners will leave the course with the ability to use basic statistical techniques to answer their own questions about their own data, using a widely available statistical software package (R). Learners from all walks of life can use this course to better understand their data, to make valuable informed decisions.
Join us in learning how to look at the world around us. What are the questions? How can we answer them? And what do those answers tell us about the world we live in?



            Read more
          



Week One: Introduction to Data

Why study statistics?
Variables and data
Getting to know R and RStudio

Week Two: Sampling

Why study statistics?
The sampling distribution
Central limit theorem
Confidence intervals

Week Three: Hypothesis Testing (One and Two Group Means)

What makes a hypothesis test?
Errors in testing
Alpha and critical values
Single sample test
Independent t-test and Dependent t-test

Week Four: Hypothesis Testing (Categorical Data)

The chi-square test
Goodness-of-Fit
Test-of-Independence

Week Five: Hypothesis Testing (More Than Two Group Means)

The ANOVA
One-way ANOVA
Two-way ANOVA

Week Six: Hypothesis Testing (Quantitative data)

Correlation
Simple (single variable) regression
Multiple regression"
https://www.classcentral.com/course/uva-darden-understanding-data-tools-8922,"Are you trying to understand data from your research? Learn how and when to conduct mediation, moderation, and conditional indirect effects analyses? Or, perhaps, how to theorize and test your theoretical models? If so, this is the course for you! We will walk you through the steps of conducting multilevel analyses using a real dataset and provide articles and templates designed to facilitate your learning. You'll leave with the tools you need to analyze and interpret the results of the datasets you collect as a researcher. 

By the end of this course, you will understand the differences between mediation and moderation and between moderated mediation and mediated moderation models (conditional indirect effects), and the importance of multilevel analysis. Most important, you will be able to run mediation, moderation, conditional indirect effect and multilevel models and interpret the results.

Note: This course is based on Version 2 of SPSS. You can still apply it to the latest version, but it’s no longer an exact match to the current interface.

This course is supported by the BRAD Lab at the Darden School of Business, which studies organizational behavior, marketing, business ethics, judgment and decision-making, behavioral operations, and entrepreneurship, among other areas. More: http://www.darden.virginia.edu/brad-lab/
      


          Mediation and Moderation
    -Welcome to the first week of our research methods course! We'll start with mediation analysis, following by parallel mediation, serial mediation, and moderation. Mediation is all about the mechanisms connecting the independent variable and dependent variable. Moderation refers to the circumstances under which the independent variable influences the dependent variable. By the end of this week, you will know how, when, and where the independent variable influences the dependent variable and how to theorize and conduct analysis using SPSS.

Conditional Indirect Effects
    -Now that you know more about mediation and moderation, let's take a look at conditional indirect effects models, which are a combination of mediation and moderation models. First we will get to the heart of the differences between moderated mediation and mediated moderation models.This will allow you to fully understand the relationships between independent variables and dependent variables. By the end of the module, you will be able to theorize about conditional indirect effect models on SPSS and to test which path of the mediation model is affected by the moderator. Then you'll dive into SPSS, run different models, and learn how to interpret the results. 


Multilevel Analysis
    -Now that you know how to run mediation, moderation, and conditional indirect effect analyses, we can turn our attention to multilevel models. Multilevel models are statistical models of parameters that vary at more than one level. Think about employees nested in departments, or departments nested in firms. You will learn the importance of multilevel analysis to your research and get familiar with multilevel analysis language. By the end of this module, you will be able to use HLM software to run multilevel models and interpret the results."
https://www.classcentral.com/course/edx-essential-statistics-for-data-analysis-using-excel-8248,"If you’re considering a career as a data analyst, you need to know about histograms, Pareto charts, Boxplots, Bayes’ theorem, and much more. In this applied statistics course, the second in our Microsoft Excel Data Analyst XSeries, use the powerful tools built into Excel, and explore the core principles of statistics and basic probability—from both the conceptual and applied perspectives. Learn about descriptive statistics, basic probability, random variables, sampling and confidence intervals, and hypothesis testing. And see how to apply these concepts and principles using the environment, functions, and visualizations of Excel.
As a data science pro, the ability to analyze data helps you to make better decisions, and a solid foundation in statistics and basic probability helps you to better understand your data. Using real-world concepts applicable to many industries, including medical, business, sports, insurance, and much more, learn from leading experts why Excel is one of the top tools for data analysis and how its built-in features make Excel a great way to learn essential skills.
Before taking this course, you should be familiar with organizing and summarizing data using Excel analytic tools, such as tables, pivot tables, and pivot charts. You should also be comfortable (or willing to try) creating complex formulas and visualizations. Want to start with the basics? Check out DAT205x: Introduction to Data Analysis using Excel. As you learn these concepts and get more experience with this powerful tool that can be extremely helpful in your journey as a data analyst or data scientist, you may want to also take the third course in our series, DAT206x Analyzing and Visualizing Data with Excel. This course includes excerpts from Microsoft Excel 2016: Data Analysis and Business Modeling from Microsoft Press and authored by course instructor Wayne Winston.edX offers financial assistance for learners who want to earn Verified Certificates but who may not be able to pay the fee. To apply for financial assistance, enroll in the course, then follow this link to complete an application for assistance.



            Read more
          



Module 1: Descriptive Statistics You will learn how to describe data using charts and basic statistical measures. Full use will be made of the new histograms, Pareto charts, Boxplots, and Treemap and Sunburst charts in Excel 2016. Module 2: Basic Probability You will learn basic probability including the law of complements, independent events, conditional probability and Bayes Theorem. Module 3: Random Variables You will learn how to find the mean and variance of random variables and then learn about the binomial, Poisson, and Normal random variables. We close with a discussion of the beautiful and important Central Limit Theorem. Module 4: Sampling and Confidence IntervalsYou will learn the mechanics of sampling, point estimation, and interval estimation of population parameters. Module 5: Hypothesis Testing You will learn null and alternative hypotheses, Type I and Type II error, One sample tests for means and proportions, Tests for difference between means of two populations, and the Chi Square Test for Independence."
https://www.classcentral.com/course/edx-probability-the-science-of-uncertainty-and-data-11480,"The world is full of uncertainty: accidents, storms, unruly financial markets, noisy communications. The world is also full of data. Probabilistic modeling and the related field of statistical inference are the keys to analyzing data and making scientifically sound predictions. 
Probabilistic models use the language of mathematics. But instead of relying on the traditional ""theorem-proof"" format, we develop the material in an intuitive -- but still rigorous and mathematically-precise -- manner. Furthermore, while the applications are multiple and evident, we emphasize the basic concepts and methodologies that are universally applicable. 
The course covers all of the basic probability concepts, including: 

multiple discrete or continuous random variables, expectations, and conditional distributions
laws of large numbers
the main tools of Bayesian inference methods
an introduction to random processes (Poisson processes and Markov chains)

The contents of this courseare heavily based upon the corresponding MIT class -- Introduction to Probability -- a course that has been offered and continuously refined over more than 50 years. It is a challenging class but will enable you to apply the tools of probability theory to real-world applications or to your research. 
This course is part of theMITx MicroMasters Program in Statistics and Data Science. Master the skills needed to be an informed and effective practitioner of data science. You will complete this course and three others from MITx, at a similar pace and level of rigor as an on-campus course at MIT, and then take a virtually-proctored exam to earn your MicroMasters, an academic credential that will demonstrate your proficiency in data science or accelerate your path towards an MIT PhD or a Master's at other universities. To learn more about this program, please visit https://micromasters.mit.edu/ds/.



            Read more
          



Unit 1: Probability models and axioms

Probability models and axioms 
Mathematical background: Sets; sequences, limits, and series; (un)countable sets.

Unit 2: Conditioning and independence

Conditioning and Bayes' rule 
Independence

Unit 3: Counting

Counting

Unit 4: Discrete random variables

Probability mass functions and expectations
Variance; Conditioning on an event; Multiple random variables 
Conditioning on a random variable; Independence of random variables

Unit 5: Continuous random variables

Probability density functions 
Conditioning on an event; Multiple random variables 
Conditioning on a random variable; Independence; Bayes' rule

Unit 6: Further topics on random variables

Derived distributions 
Sums of independent random variables; Covariance and correlation 
Conditional expectation and variance revisited; Sum of a random number of independent random variables

Unit 7: Bayesian inference

Introduction to Bayesian inference 
Linear models with normal noise 
Least mean squares (LMS) estimation 
Linear least mean squares (LLMS) estimation

Unit 8: Limit theorems and classical statistics

Inequalities, convergence, and the Weak Law of Large Numbers 
The Central Limit Theorem (CLT) 
An introduction to classical statistics

Unit 9: Bernoulli and Poisson processes

The Bernoulli process 
The Poisson process 
More on the Poisson process

Unit 10 (Optional): Markov chains

Finite-state Markov chains 
Steady-state behavior of Markov chains 
Absorption probabilities and expected time to absorption"
https://www.classcentral.com/course/evaluating-problems-8238,"The second course of the specialization EVALUATING PROBLEMS will show you how humans think and how to utilize different disciplinary approaches to tackle problems more effectively.  It advances your knowledge of your own field by teaching you to look at it in new ways. 

EVALUATING PROBLEMS is constructed in the following way: Week I. “Thinking about Thinking” – How problem solving evolved in nature, how the mechanics of our brains work, and the psychological biases that can emerge when we think. Week II. “Philosophy, Science, and Problem Solving” – How humans have historically approached problem solving, from ancient times to the present. Week III. “Approaching Problems in the Natural Sciences” – How people in the natural sciences deconstruct problems. Week IV. “Statistics and Problem Solving” – How statistics can be used to evaluate problems and think critically. Week V. “Approaching Problems in the Humanities” – How people in the social sciences and humanities deconstruct problems. Week VI. “Evaluating the Anthropocene” – How to evaluate the problems of the Anthropocene.
      


          The Evolution of Problem Solving
    -Welcome to the second course of our specialisation on solving complex problems! In this module, we will introduce you to modes of thinking, how our capacity for thinking has evolved, and the blindspots that can still arise out of our fragile, blindly evolved, mammalian brains.

Philosophy, Science, and Problem Solving
    -In this module we shall explore how human cultures evaluate complex problems, taking in global perspectives from history, looking at modern science, and examining cases where contemplation of problems go beyond science into the field of ethics and society.

Approaching Problems in the Natural Sciences
    -This module will look at how we evaluate problems in the sciences in its various branches, and contemplate how these methods may lend themselves to our own approach to complex problems.

Statistics and Problem Solving
    -In this module we will examine the very important role that statistics play in evaluating complex problems and certain skills and approaches that can be utilised in our own domains.

Approaching Problems in the Humanities
    -In this module, we will examine the similarities and differences regarding how we approach complex problems in different areas of the humanities. Given the extreme complexity of human society, a multi-faceted approach is not surprising, but we shall also examine the common threads that run through all approaches.

Evaluating the Anthropocene
    -In this module, we shall apply what we've learned to our examination of complex problems in the Anthropocene. In such an age and in such a tangled system an increasingly transdisciplinary approach is required to make the right judgements and arrive at the best solutions."
https://www.classcentral.com/course/linear-models-6180,"Welcome to the Advanced Linear Models for Data Science Class 1: Least Squares. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:

- A basic understanding of linear algebra and multivariate calculus.
- A basic understanding of statistics and regression models.
- At least a little familiarity with proof based mathematics.
- Basic knowledge of the R programming language.

After taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models.
      


          Background
    -We cover some basic matrix algebra results that we will need throughout the class. This includes some basic vector derivatives. In addition, we cover some some basic uses of matrices to create summary statistics from data. This includes calculating and subtracting means from observations (centering) as well as calculating the variance.


One and two parameter regression
    -In this module, we cover the basics of regression through the origin and linear regression. Regression through the origin is an interesting case, as one can build up all of multivariate regression with it.

Linear regression
    -In this lecture, we focus on linear regression, the most standard technique for investigating unconfounded linear relationships. 

General least squares
    -We now move on to general least squares where an arbitrary full rank design matrix is fit to a vector outcome.

Least squares examples
    -Here we give some canonical examples of linear models to relate them to techniques that you may already be using.

Bases and residuals
    -Here we give a very useful kind of linear model, that is decomposing a signal into a basis expansion."
https://www.classcentral.com/course/foundations-marketing-analytics-4370,"Who is this course for?  
This course is designed for students, business analysts, and data scientists who want to apply statistical knowledge and techniques to business contexts. For example, it may be suited to experienced statisticians, analysts, engineers who want to move more into a business role, in particular in marketing.

You will find this course exciting and rewarding if you already have a background in statistics, can use R or another programming language and are familiar with databases and data analysis techniques such as regression, classification, and clustering.
However, it contains a number of recitals and R Studio tutorials which will consolidate your competences, enable you to play more freely with data and explore new features and statistical functions in R.

Business Analytics, Big Data and Data Science are very hot topics today, and for good reasons. Companies are sitting on a treasure trove of data, but usually lack the skills and people to analyze and exploit that data efficiently. Those companies who develop the skills and hire the right people to analyze and exploit that data will have a clear competitive advantage.

It's especially true in one domain: marketing. About 90% of the data collected by companies today are related to customer actions and marketing activities.The domain of Marketing Analytics is absolutely huge, and may cover fancy topics such as text mining, social network analysis, sentiment analysis, real-time bidding, online campaign optimization, and so on.

But at the heart of marketing lie a few basic questions that often remain unanswered: (1) who are my customers, (2) which customers should I target and spend most of my marketing budget on, and (3) what's the future value of my customers so I can concentrate on those who will be worth the most to the company in the future.

That's exactly what this course will cover: segmentation is all about understanding your customers, scorings models are about targeting the right ones, and customer lifetime value is about anticipating their future value. These are the foundations of Marketing Analytics. And that's what you'll learn to do in this course.
      


            Read more
          



          Module 0 : Introduction to Foundation of Marketing Analytics
    -
In this short module, we will introduce the field of marketing analytics, and layout the structure of this course.

We will also take that opportunity to explore a retailing data set that we’ll be using throughout this course. We will setup the environment, load the data in R (we’ll be using the RStudio environment), and explore it using simple SQL statements.


Module 1 : Statistical segmentation
    -
In this module, you will learn the inner workings of statistical segmentation, how to compute statistical indicators about customers such as recency or frequency, and how to identify homogeneous groups of customers within a database.

We will alternate lectures and R tutorials, making sure that, by the end of this module, you will be able to apply every concept we will cover.


Module 2 : Managerial segmentation
    -
Statistical segmentation is an invaluable tool, especially to explore, summarize, or make a snapshot of an existing database of customers. But what most academics will fail to tell you is that this kind of segmentation is not the method of choice for many companies, and for good reasons.

In this module, you will learn to perform managerial segmentations, which are not built upon statistical techniques, but are an essential addition to your toolbox of marketing analyst.

You will also learn how to segment a database now, but also at any point in time in the past, and why it is useful to managers to do so.


Module 3 : Targeting and scoring models
    -
How can Target predict which of its customers are pregnant? How can a bank predict the likelihood you will default on their loan, or crash your car within the next five years, and price accordingly? And if your firm only has the budget to reach a few customers during a marketing campaign, who should it target to maximize profit?

The answer to all these questions is… by building a scoring model, and targeting your customers accordingly.

In this module, you will learn how to build a customer score, which in marketing usually combines two predictions in one : what is the likelihood that a customer will buy something, and if he does, how much will he buy for?


Module 4 : Customer lifetime value
    -In this module, you will learn how to use R to execute lifetime value analyses. You will learn to estimate what is called a transition matrix -which measures how customers transition from one segment to another- and use that information to make invaluable predictions about how a customer database is likely to evolve over the next few years, and how much money it should be worth."
https://www.classcentral.com/course/statistics-1349,"The Coursera course, Data Analysis and Statistical Inference has 
been revised and is now offered as part of Coursera Specialization “Statistics with R”. This Specialization consists of 4 courses and a capstone project. The courses can be taken separately:Introduction to Probability and Data (began in April 2016)Inferential Statistics (begins in May 2016)Linear Regression and Modeling (begins in June 2016)Bayesian Statistics (begins in July 2016) A completely new course, with additional faculty!Statistics Capstone Project (August 2016) (for learners who have passed the 4 previous courses, and earned certificate)You
 may enroll in a single course, or all of them, but each requires the 
knowledge and techniques from the previous courses. The assignments in 
these courses have suggested but not required deadlines, so you can work
 at your own schedule. Please check the Specialization page for other 
answers to your questions, and peek at the first course. We hope to see you in our new courses. The Statistics with R team.___________________________________________________The goals of this course are as follows:Recognize the importance of data collection, identify limitations in data collection methods, and determine how they affect the scope of inference.Use statistical software (R) to summarize data numerically and visually, and to perform data analysis.Have a conceptual understanding of the unified nature of statistical inference.Apply estimation and testing methods (confidence intervals and hypothesis tests) to analyze single variables and the relationship between two variables in order to understand natural phenomena and make data-based decisions.Model and investigate relationships between two or more variables within a regression framework.Interpret results correctly, effectively, and in context without relying on statistical jargon.Critique data-based claims and evaluate data-based decisions.Complete a research project that employs simple statistical inference and modeling techniques.



            Read more
          



Week 1: Unit 1 - Introduction to dataPart 1 – Designing studiesPart 2 – Exploratory data analysisPart 3 – Introduction to inference via simulationWeek 2: Unit 2 - Probability and distributionsPart 1 – Defining probabilityPart 2 – Conditional probabilityPart 3 – Normal distributionPart 4 – Binomial distributionWeek 3: Unit 3 - Foundations for inferencePart 1 – Variability in estimates and the Central Limit TheoremPart 2 – Confidence intervalsPart 3 – Hypothesis testsWeek 4: Finish up Unit 3 + MidtermPart 4 – Inference for other estimatorsPart 5 - Decision errors, significance, and confidenceWeek 5: Unit 4 - Inference for numerical variablesPart 1 – t-inferencePart 2 – PowerPart 3 – Comparing three or more means (ANOVA)Part 4 – Simulation based inference for meansWeek 6: Unit 5 - Inference for categorical variablesPart 1 – Single proportionPart 2 – Comparing two proportionsPart 3 – Inference for proportions via simulationPart 4 – Comparing three or more proportions (Chi-square)Week 7: Unit 6 - Introduction to linear regressionPart 1 – Relationship between two numerical variablesPart 2 – Linear regression with a single predictorPart 3 – Outliers in linear regressionPart 4 – Inference for linear regressionWeek 8: Unit 7 - Multiple linear regressionPart 1 – Regression with multiple predictorsPart 2 – Inference for multiple linear regressionPart 3 – Model selectionPart 4 – Model diagnosticsWeek 9: Review / catch-up weekBayesian vs. frequentist inferenceWeek 10: Final exam"
https://www.classcentral.com/course/edx-supply-chain-analytics-6367,"Supply chains are complex systems involving multiple businesses and organizations with different goals and objectives. Many different analytical methods and techniques are used by researchers and practitioners alike to better design and manage their supply chains. This business and management course introduces the primary methods and tools that you will encounter in your study and practice of supply chains. We focus on the application of these methods, not necessarily the theoretical underpinnings.
We will begin with an overview of introductory probability and decision analysis to ensure that students understand how uncertainty can be modeled. Next, we will move into basic statistics and regression. Finally, we will introduce optimization modeling from unconstrained to linear, non-linear, and mixed integer linear programming.
This is a hands-on course. Students will use spreadsheets extensively to apply these techniques and approaches in case studies drawn from actual supply chains ."
https://www.classcentral.com/course/biostats2-1033,"Learn fundamental concepts in data analysis and statistical inference, focusing on one and two independent samples.
      


          Hypothesis Testing
    -In this module, you'll get an introduction to hypothesis testing, a core concept in statistics. We'll cover hypothesis testing for basic one and two group settings as well as power. After you've watched the videos and tried the homework, take a stab at the quiz.

Two Binomials
    -In this module we'll be covering some methods for looking at two binomials. This includes the odds ratio, relative risk and risk difference. We'll discussing mostly confidence intervals in this module and will develop the delta method, the tool used to create these confidence intervals. After you've watched the videos and tried the homework, take a crack at the quiz!

Discrete Data Settings
    -In this module, we'll discuss testing in discrete data settings. This includes the famous Fisher's exact test, as well as the many forms of tests for contingency table data. You'll learn the famous observed minus expected squared over the expected formula, that is broadly applicable. 

Techniques
    -This module is a bit of a hodge podge of important techniques. It includes methods for discrete matched pairs data as well as some classical non-parametric methods."
https://www.classcentral.com/course/edx-data-science-inference-and-modeling-10349,"Statistical inference and modeling are indispensable for analyzing data affected by chance, and thus essential for data scientists. In this course, you will learn these key concepts through a motivating case study on election forecasting. 
This course will show you how inference and modeling can be applied to develop the statistical approaches that make polls an effective tool and we'll show you how to do this using R. You will learn concepts necessary to define estimates and margins of errors and learn how you can use these to make predictions relatively well and also provide an estimate of the precision of your forecast. 
Once you learn this you will be able to understand two concepts that are ubiquitous in data science: confidence intervals, and p-values. Then, to understand statements about the probability of a candidate winning, you will learn about Bayesian modeling. Finally, at the end of the course, we will put it all together to recreate a simplified version of an election forecast model and apply it to the 2016 election."
https://www.classcentral.com/course/business-data-5757,"This course provides an analytical framework to help you evaluate key problems in a structured fashion and will equip you with tools to better manage the uncertainties that pervade and complicate business processes. Specifically, you will be introduced to statistics and how to summarize data and learn concepts of frequency, normal distribution, statistical studies, sampling, and confidence intervals.

While you will be introduced to some of the science of what is being taught, the focus will be on applying the methodologies. This will be accomplished through the use of Excel and data sets from many different disciplines, allowing you to see the use of statistics in very diverse settings. The course will focus not only on explaining these concepts, but also understanding the meaning of the results obtained.

Upon successful completion of this course, you will be able to:

•	Summarize large data sets in graphical, tabular, and numerical forms.
•	Understand the significance of proper sampling and why you can rely on sample information.
•	Understand why normal distribution can be used in so many settings.
•	Use sample information to infer about the population with a certain level of confidence about the accuracy of the estimations.
•	Use Excel for statistical analysis.

This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.
      


            Read more
          



          Course Orientation
    -You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.

Module 1: Introduction and Summarizing Data
    -Data is all around you, but what is the data telling you? The first step in making better decisions and taking action is to get a good understanding of information you have gathered. In this module we will learn about some of the tools in statistics that help us achieve this.

Module 2: Descriptive Statistics and Probability Distributions
    -We all have heard the phrase that a ""picture is worth a thousand words,"" but you certainly don’t want one of those to be ""what exactly am I looking at?"" So, now that you know to use ""pictures"" to summarize your data, let’s make those pictures easier to understand.

Module 3: Sampling and Central Limit Theorem
    -You are charged with analyzing a market segment for your company. You and your team have figured out what variables you need to understand; you also have an idea what factors might be influencing these variables of interest. Now you are ready to do your analysis. But, wait! Where is the data? How do you begin to get the data? In this module we will review the means by which you can begin to produce data – the concepts of sampling and Central Limit Theorem – and will help you understand how to produce ""good"" sample data and why sample data will work.

Module 4: Inference
    -You have sample data and have done the analysis – you think you can say something about the population based on your sample study.  But, do you have a sense of what are the chances of you being right or wrong?  How can you be surer? What else should you have considered? In this module, you will learn how to find the answers to these questions."
https://www.classcentral.com/course/fe2-1015,"Financial Engineering is a multidisciplinary field involving finance and economics, mathematics, statistics, engineering and computational methods.  The emphasis of FE & RM Part II will be on the use of simple stochastic models to (i) solve portfolio optimization problems  (ii) price derivative securities in various asset classes including equities and credit and (iii) consider some advanced applications of financial engineering including algorithmic trading and the pricing of real options. We will also consider the role that financial engineering played during the financial crisis. 

We hope that students who complete the course and the prerequisite course (FE & RM Part I) will have a good understanding of the ""rocket science"" behind financial engineering. But perhaps more importantly, we hope they will also understand the limitations of this theory in practice and why financial models should always be treated with a healthy degree of skepticism.
      


          Mean-Variance Analysis and CAPM
    -Problem formulation and solution; the efficient frontier; including the risk-free asset; the Capital Asset Pricing Model (CAPM);implications of CAPM: α, β, security and capital market lines

Practical Issues in Implementing Mean Variance
    -Problems with mean-variance analysis; ETFs and leveraged ETFs; VaR and CVaR for asset allocation; survivorship bias, performance evaluation and other statistical pitfalls.

Equity Derivatives in Practice: Part I
    -Problems with mean-variance analysis; ETFs and leveraged ETFs; VaR and CVaR for asset allocation; survivorship bias, performance evaluation and other statistical pitfalls.

Equity Derivatives in Practice: Part II
    -More about Black-Scholes, the Greeks and delta-hedging; the volatility surface; pricing derivatives using the volatility surface; model calibration.

Credit Derivatives and Structured Products
    -Mechanics and pricing of CDOs; exotic structured credit securities including CDO-squared’s and CDO-cubed’s. Risk management of these products and their role in the financial crisis.

Other Applications of Financial Engineering
    -Real options; energy and commodities modeling; algorithmic trading.

Background Material
    -"
https://www.classcentral.com/course/data-to-insight-2129,"##
Data is everywhere and the lessons it contains can be the key to making good decisions.
Gain the skills and confidence to dive into data using computer software and start making discoveries.
You’ll learn key elements of data science so that you can start thinking like a statistician.
This course is suited to those new to these areas and those wanting a reminder and fresh perspectives. You will need to be comfortable thinking in terms of percentages and have a level of comfort with using computers.
We use free software. Parallel threads enable you to choose between using online software that requires only an internet connection and browser, installed “point-and-click” software, or learning to code in R."
https://www.classcentral.com/course/edx-data-science-probability-10348,"In this course, part of our Professional Certificate Program in Data Science,you will learn valuable concepts in probability theory. The motivation for this course is the circumstances surrounding the financial crisis of 2007-2008. Part of what caused this financial crisis was that the risk of some securities sold by financial institutions was underestimated. To begin to understand this very complicated event, we need to understand the basics of probability. 
We will introduce important concepts such as random variables, independence, Monte Carlo simulations, expected values, standard errors, and the Central Limit Theorem. These statistical concepts are fundamental to conducting statistical tests on data and understanding whether the data you are analyzing is likely occurring due to an experimental method or to chance. 
Probability theory is the mathematical foundation of statistical inference which is indispensable for analyzing data affected by chance, and thus essential for data scientists."
https://www.classcentral.com/course/edx-data-science-linear-regression-10352,"Linear regression is commonly used to quantify the relationship between two or more variables. It is also used to adjust for confounding. This course, part ofourProfessional Certificate Program in Data Science, covers how to implement linear regression and adjust for confounding in practice using R. 
In data science applications, it is very common to be interested in the relationship between two or more variables. The motivating case study we examine in this course relates to the data-driven approach used to construct baseball teams described in Moneyball. We will try to determine which measured outcomes best predict baseball runs by using linear regression. 
We will also examine confounding, where extraneous variables affect the relationship between two or more other variables, leading to spurious associations. Linear regression is a powerful technique for removing confounders, but it is not a magical process. It is essential to understand when it is appropriate to use, and this course will teach you when to apply this technique."
https://www.classcentral.com/course/erasmus-econometrics-4723,"Welcome!
Do you wish to know how to analyze and solve business and economic questions with data analysis tools? Then Econometrics by Erasmus University Rotterdam is the right course for you, as you learn how to translate data into models to make forecasts and to support decision making.

* What do I learn?
When you know econometrics, you are able to translate data into models to make forecasts and to support decision making in a wide variety of fields, ranging from macroeconomics to finance and marketing. Our course starts with introductory lectures on simple and multiple regression, followed by topics of special interest to deal with model specification, endogenous variables, binary choice data, and time series data.  You learn these key topics in econometrics by watching the videos with in-video quizzes and by making post-video training exercises. 

* Do I need prior knowledge?
The course is suitable for (advanced undergraduate) students in economics, finance, business, engineering, and data analysis, as well as for those who work in these fields. The course requires some basics of matrices, probability, and statistics, which are reviewed in the Building Blocks module. If you are searching for a MOOC on econometrics of a more introductory nature that needs less background in mathematics, you may be interested in the Coursera course “Enjoyable Econometrics” that is also from Erasmus University Rotterdam.

* What literature can I consult to support my studies?
You can follow the MOOC without studying additional sources. Further reading of the discussed topics (including the Building Blocks) is provided in the textbook that we wrote and on which the MOOC is based: Econometric Methods with Applications in Business and Economics, Oxford University Press. The connection between the MOOC modules and the book chapters is shown in the Course Guide – Further Information – How can I continue my studies.

* Will there be teaching assistants active to guide me through the course?
Staff and PhD students of our Econometric Institute will provide guidance in January and February of each year. In other periods, we provide only elementary guidance. We always advise you to connect with fellow learners of this course to discuss topics and exercises.

* How will I get a certificate?
To gain the certificate of this course, you are asked to make six Test Exercises (one per module) and a Case Project. Further, you perform peer-reviewing activities of the work of three of your fellow learners of this MOOC. You gain the certificate if you pass all seven assignments.

Have a nice journey into the world of Econometrics!
The Econometrics team
      


            Read more
          



          Welcome Module
    -Welcome!                                                                                                                                                                                          

Do you wish to know how to analyze and solve business and economic questions with data analysis tools? Then Econometrics by Erasmus University Rotterdam is the right course for you, as you learn how to translate data into models to make forecasts and to support decision making.                                                                                              
                                                                                                                                                                                                       What do I learn?                                                                                                                                                                         

When you know econometrics, you are able to translate data into models to make forecasts and to support decision making in a wide variety of fields, ranging from macroeconomics to finance and marketing. Our course starts with introductory lectures on simple and multiple regression, followed by topics of special interest to deal with model specification, endogenous variables, binary choice data, and time series data.  You learn these key topics in econometrics by watching the videos with in-video quizzes and by making post-video training exercises.                  

Do I need prior knowledge?                                                                                                                                                         

The course is suitable for (advanced undergraduate) students in economics, finance, business, engineering, and data analysis, as well as for those who work in these fields. The course requires some basics of matrices, probability, and statistics, which are reviewed in the Building Block module. If you are searching for a MOOC on econometrics of a more introductory nature that needs less background in mathematics, you may be interested in the Coursera course “Enjoyable Econometrics” that is also from Erasmus University Rotterdam.                                                         

Will there be teaching assistants active to guide me through the course?                                                                          
                                                                                                                                                                                                           We advise you to connect with fellow learners of this course to discuss topics and exercises. Staff of our Econometric Institute provides guidance in January and February of each year.                                                            

How will I get a certificate?                                                                                                                                                             
                                                                                                                                                                                                            To gain the certificate of this course, you are asked to make six Test Exercises (one per module) and a Case Project. Further, you perform peer-reviewing activities of the work of three of your fellow learners of this MOOC. You gain the certificate if your average grade score is at least 50%.                                                                                                  

Have a nice journey into the world of Econometrics!                                                                                                             
                                                                                                                                                                                                          The Econometrics team

Simple Regression

Multiple Regression

Model Specification

Endogeneity

Binary Choice

Time Series

Case Project
    -This Case Project is the final assignment of our MOOC. It is of an applied nature, and it asks you to answer practical questions by means of econometric methods. By doing the case, you will integrate various econometric methods and skills that were trained in our MOOC. 

OPTIONAL: Building Blocks
    -By studying this module, you get the required background on matrices, probability and statistics. Each topic is illustrated with simple examples, and you get hands-on training by doing the training exercise that concludes each lecture. Three lectures on matrices show you the basic terminology and properties of matrices, including transpose, trace, rank, inverse, and positive definiteness. Two lectures on probability teach you the basics of univariate and multivariate probability distributions, especially the normal and associated distributions, including mean, variance, and covariance. Finally, two lectures on statistics present you with the basic ideas of statistical inference, in particular parameter estimation and testing, including the use of matrix methods and probability methods."
https://www.classcentral.com/course/practical-time-series-analysis-10151,"Welcome to Practical Time Series Analysis!

Many of us are ""accidental"" data analysts. We trained in the sciences, business, or engineering and then found ourselves confronted with data for which we have no formal analytic training.  This course is designed for people with some technical competencies who would like more than a ""cookbook"" approach, but who still need to concentrate on the routine sorts of presentation and analysis that deepen the understanding of our professional topics. 

In practical Time Series Analysis we look at data sets that represent sequential information, such as stock prices, annual rainfall, sunspot activity, the price of agricultural products, and more.  We look at several mathematical models that might be used to describe the processes which generate these types of data. We also look at graphical representations that provide insights into our data. Finally, we also learn how to make forecasts that say intelligent things about what we might expect in the future.

Please take a few minutes to explore the course site. You will find video lectures with supporting written materials as well as quizzes to help emphasize important points. The language for the course is R, a free implementation of the S language. It is a professional environment and fairly easy to learn.

You can discuss material from the course with your fellow learners. Please take a moment to introduce yourself!

Time Series Analysis can take effort to learn- we have tried to present those ideas that are ""mission critical"" in a way where you understand enough of the math to fell satisfied while also being immediately productive. We hope you enjoy the class!
      


            Read more
          



          WEEK 1: Basic Statistics
    -During this first week, we show how to download and install R on Windows and the Mac. We review those basics of inferential and descriptive statistics that you'll need during the course.

Week 2: Visualizing Time Series, and Beginning to Model Time Series
    -In this week, we begin to explore and visualize time series available as acquired data sets. We also take our first steps on developing the mathematical models needed to analyze time series data.

Week 3: Stationarity, MA(q) and AR(p) processes
    -In Week 3, we introduce few important notions in time series analysis: Stationarity, Backward shift operator, Invertibility, and Duality. We begin to explore Autoregressive processes and Yule-Walker equations. 

Week 4: AR(p) processes, Yule-Walker equations, PACF
    -In this week, partial autocorrelation is introduced. We work more on Yule-Walker equations, and apply what we have learned so far to few real-world datasets.  

Week 5: Akaike Information Criterion (AIC), Mixed Models, Integrated Models
    -In Week 5, we start working with Akaike Information criterion as a tool to judge our models, introduce mixed models such as ARMA, ARIMA and model few real-world datasets. 

Week 6: Seasonality, SARIMA, Forecasting
    -In the last week of our course, another model is introduced: SARIMA. We fit SARIMA models to various datasets and start forecasting."
https://www.classcentral.com/course/financialengineering1-1014,"Financial Engineering is a multidisciplinary field drawing from finance and economics, mathematics, statistics, engineering and computational methods. The emphasis of FE & RM Part I will be on the use of simple stochastic models to price derivative securities in various asset classes including equities, fixed income, credit and mortgage-backed securities. We will also consider the role that some of these asset classes played during the financial crisis. A notable feature of this course will be an interview module with Emanuel Derman, the renowned ``quant'' and best-selling author of ""My Life as a Quant"". 

We hope that students who complete the course will begin to understand the ""rocket science"" behind financial engineering but perhaps more importantly, we hope they will also understand the limitations of this theory in practice and why financial models should always be treated with a healthy degree of skepticism. The follow-on course FE & RM Part II will continue to develop derivatives pricing models but it will also focus on asset allocation and portfolio optimization as well as other applications of financial engineering such as real options, commodity and energy derivatives and algorithmic trading.
      


          Course Overview
    -An introduction to the course.

Introduction to Basic Fixed Income Securities
    -Review of interest and basic fixed income securities; introduction to arbitrage pricing. 

Introduction to Derivative Securities
    -The mechanics of forwards, futures, swaps and options. Option pricing in the 1-period binomial model.

Option Pricing in the Multi-Period Binomial Model
    -Derivatives pricing in the binomial model including European and American options; handling dividends; pricing forwards and futures; convergence of the binomial model to Black-Scholes.

Term Structure Models I
    -Binomial lattice models of the short-rate; pricing fixed income derivative securities including caps, floors swaps and swaptions; the forward equations and elementary securities.

Term Structure Models II and Introduction to Credit Derivatives
    -Calibration of term-structure models; the Black-Derman-Toy and Ho-Lee models. Limitations of term-structure models and derivatives pricing models in general. Introduction to credit-default swaps (CDS) and the pricing of CDS and defaultable bonds.

Introduction to Mortgage Mathematics and Mortgage-Backed Securities
    -Basic mortgage mathematics; mechanics of mortgage-backed securities (MBS) including pass-throughs, principal-only and interest-only securities, and CMOs; pricing of MBS; MBS and the financial crisis.

Background Material"
https://www.classcentral.com/course/data-collection-analytics-project-6083,"In this course you will learn how to use survey weights to estimate descriptive statistics, like means and totals, and more complicated quantities like model parameters for linear and logistic regressions.  Software capabilities will be covered with R® receiving particular emphasis.  The course will also cover the basics of record linkage and statistical matching—both of which are becoming more important as ways of combining data from different sources.  Combining of datasets raises ethical issues which the course reviews.  Informed consent may have to be obtained from persons to allow their data to be linked. You will learn about differences in the legal requirements in different countries.
      


          Basic Estimation
    -After completing Modules 1 and 2 of this course you will understand how to estimate descriptive statistics, overall and for subgroups, when you deal with survey data.  We will review software for estimation (R, Stata, SAS) with examples for how to estimate things like means, proportions, and totals.  You will also learn how to estimate parameters in linear, logistic, and other models and learn software options with emphasis on R. Module 3 and 4 discuss how you can add additional data to your analysis. This requires knowing about record linkage techniques, and what it takes to get permission to link data.

Models
    -Module 2 covers how to estimate linear and logistic model parameters using survey data. After completing this module, you will understand how the methods used differ from the ones for non-survey data. We also cover the features of survey data sets that need to be accounted for when estimating standard errors of estimated model parameters.

Record Linkage
    -Module starts with the current debate on using more (linked) administrative records in the U.S. Federal Statistical System, and a general motivation for linking records. Several examples will be given on why it is useful to link data. Challenges of record linkage will be discussed. A brief overview over key linkage techniques is included as well.

Ethics
    -This module will discuss key issues in obtaining consent to record linkage. Failure to consent can lead to bias estimates. Current research examples will be given as well as practical suggestions on how to obtain linkage consent."
https://www.classcentral.com/course/edx-estadistica-aplicada-a-los-negocios-10270,"Este curso en línea brinda una introducción al análisis de datos para business intelligence. Aprenderás de herramientas y técnicas de estadística descriptiva e inferencial. Serás capaz de analizar data y gráficos para transformarla en información de valor que te permita obtener criterios para la toma de decisiones.
Con este curso podrás utilizar datos para cumplir objetivos concretos como :

Descubrir quién es el cliente que representa mayor valor para la empresa
Identificar cómo controlar los gastos
Identificar cómo hacer más rápida la cadena de producción
Saber que esperar sobre un producto que acaba de ser lanzado al mercado
Conocer cómo afecta determinado evento en las ventas
Saber cuál es el producto menos rentable para eliminarlo del portafolio
Identificar las mejores epocas para hacer esfuerzos de posicionamiento de determinado producto

Tendrás dominio de los conceptos básicos y aprenderás a utilizar estimadores , histogramas, escalas de medición , varianza, desviación estandar, probabilidad normal, probabilidad informal, valor esperado de una variable aleatoria, intervalos de confianza , distribución de Poisson y más.
Este es el primer curso del Programa de Certificación Profesional de Inteligencia de Negocios. El segundo curso es sobre Herramientas de Business Intelligence. Te recomendamos completar ambos para que adquieras conocimiento teórico y experiencia práctica. Tendrás el respaldo de la experiencia de Jorge Samayoa, Ph.D. por la Universidad de Purdue.



            Read more
          




Análisis de datos con Excel
Análisis de datos, medidas estadísticas, distribución de frecuencias y datos
Medidas de tendencia central (media, mediana, modo, mínimo, máximo, cuartil, varianza y desviación estandar)
Estimadores, intervalos de confianza y pruebas de hipótesis
Análisis regresional
Conceptos básicos de estadística
Probabilidad informal y variables aleatorias discretas y continuas
Probabilidad binomial, probabilidad geométrica, probabilidad de Poisson
Variables aleatorias continuas, probabilidad uniforme, probabilidad normal, probabilidad exponencial
Estadística aplicada a negocios"
https://www.classcentral.com/course/edx-introduction-to-r-for-data-science-3928,"R is rapidly becoming the leading language in data science and statistics. Today, R is the tool of choice for data science professionals in every industry and field. Whether you are full-time number cruncher, or just the occasional data analyst, R will suit your needs.
This introduction to R programming course will help you master the basics of R. In seven sections, you will cover its basic syntax, making you ready to undertake your own first data analysis using R. Starting from variables and basic operations, you will eventually learn how to handle data structures such as vectors, matrices, data frames and lists. In the final section, you will dive deeper into the graphical capabilities of R, and create your own stunning data visualizations. No prior knowledge in programming or data science is required.
What makes this course unique is that you will continuously practice your newly acquired skills through interactive in-browser coding challenges using the DataCamp platform. Instead of passively watching videos, you will solve real data problems while receiving instant and personalized feedback that guides you to the correct solution.
Enjoy! 
edX offers financial assistance for learners who want to earn Verified Certificates but who may not be able to pay the fee. To apply for financial assistance, enroll in the course, then follow this link to complete an application for assistance.



Section 1: Introduction to Basics
Take your first steps with R. Discover the basic data types in R and assign your first variable. 
Section 2: Vectors
Analyze gambling behaviour using vectors. Create, name and select elements from vectors. 
Section 3: Matrices
Learn how to work with matrices in R. Do basic computations with them and demonstrate your knowledge by analyzing the Star Wars box office figures. 
Section 4: Factors
R stores categorical data in factors. Learn how to create, subset and compare categorical data. 
Section 5: Data Frames
When working R, you'll probably deal with Data Frames all the time. Therefore, you need to know how to create one, select the most interesting parts of it, and order them. 
Section6: Lists
Lists allow you to store components of different types. Section 6 will show you how to deal with lists. 
Section 7: Basic Graphics
Discover R's packages to do graphics and create your own data visualizations."
https://www.classcentral.com/course/wharton-operations-analytics-4204,"This course is designed to impact the way you think about transforming data into better decisions. Recent extraordinary improvements in data-collecting technologies have changed the way firms make informed and effective business decisions. The course on operations analytics, taught by three of Wharton’s leading experts, focuses on how the data can be used to profitably match supply with demand in various business settings. In this course, you will learn how to model future demand uncertainties, how to predict the outcomes of competing policy choices and how to choose the best course of action in the face of risk. The course will introduce frameworks and ideas that provide insights into a spectrum of real-world business challenges, will teach you methods and software available for tackling these challenges quantitatively as well as the issues involved in gathering the relevant data.

This course is appropriate for beginners and business professionals with no prior analytics experience.
      


          Introduction, Descriptive and Predictive Analytics
    -In this module you’ll be introduced to the Newsvendor problem, a fundamental operations problem of matching supply with demand in uncertain settings. You'll also cover the foundations of descriptive analytics for operations, learning how to use historical demand data to build forecasts for future demand.  Over the week, you’ll be introduced to underlying analytic concepts, such as random variables, descriptive statistics, common forecasting tools, and measures for judging the quality of  your forecasts.

Prescriptive Analytics, Low Uncertainty
    -In this module, you'll learn how to identify the best decisions in settings with low uncertainty by building optimization models and applying them to specific business challenges. During the week, you’ll use algebraic formulations to concisely express optimization problems, look at how algebraic models should be converted into a spreadsheet format, and learn how to use spreadsheet Solvers as tools for identifying the best course of action. 

Predictive Analytics, Risk
    -How can you evaluate and compare decisions when their impact is uncertain? In this module you will learn how to build and interpret simulation models that can help you to evaluate complex business decisions in uncertain settings. During the week, you will be introduced to some common measures of risk and reward, you’ll use simulation to estimate these quantities, and you’ll learn how to interpret and visualize your simulation results.

Prescriptive Analytics, High Uncertainty 
    -This module introduces decision trees, a useful tool for evaluating decisions made under uncertainty. Using a concrete example, you'll learn how optimization, simulation, and decision trees can be used together to solve more complex business problems with high degrees of uncertainty. You'll also discover how the Newsvendor problem introduced in Week 1 can be solved with the simulation and optimization framework introduced in Weeks 2 and 3."
https://www.classcentral.com/course/data-modeling-regression-analysis-busine-13713,"The course will begin with what is familiar to many business managers and those who have taken the first two courses in this specialization. The first set of tools will explore data description, statistical inference, and regression. We will extend these concepts to other statistical methods used for prediction when the response variable is categorical such as win-don’t win an auction. In the next segment, students will learn about tools used for identifying important features in the dataset that can either reduce the complexity or help identify important features of the data or further help explain behavior. 
      


          Module 0:  Get Ready & Module 1: Introduction to Analytics and Evolution of Statistical Inference
    -This session is an overview of the business data analytics process and its components. We introduce you to different modeling paradigms and invite you to match problems to modeling paradigms. The module concludes with an overview of Rattle (an interface for the statistical package R) and its use for univariate analysis. 

Module 2: Dating with Data
    -This session focuses on identifying relationships between dependent and independent variables using a regression model. The goal is to find the best fitted model to the data to learn about the underlying relationship of variables in the population.  

Module 3: Model Development and Testing with Holdout Data
    -This session introduces the student to use of a holdout data set for evaluating model performance. Methods of improving the model are discussed with emphasis on variable selection. Nuances of modeling discrete predictor variables and response variables are discussed. 

Module 4: Curse of Dimensionality
    -There has been a tremendous increase in the way data generation via sensors, digital platforms, user-generated content, etc. are being used in the industry. For example, sensors continuously record data and store it for analysis at a later point. In the way data gets captured, there can be a lot of redundancy. With more variables, comes more trouble! There may be very little (or no) incremental information gained from these sources. This is the problem of a high number of unwanted dimensions. To avoid this pitfall, data transformation and dimension reduction comes to the rescue by examining and extracting fewer dimensions while ensuring that it conveys the full information concisely."
https://www.classcentral.com/course/meaningful-marketing-insights-6913,"With marketers are poised to be the largest users of data within the organization, there is a need to make sense of the variety of consumer data that the organization collects. Surveys, transaction histories and billing records can all provide insight into consumers’ future behavior, provided that they are interpreted correctly. In Introduction to Marketing Analytics, we introduce the tools that learners will need to convert raw data into marketing insights. The included exercises are conducted using Microsoft Excel, ensuring that learners will have the tools they need to extract information from the data available to them. The course provides learners with exposure to essential tools including exploratory data analysis, as well as regression methods that can be used to investigate the impact of marketing activity on aggregate data (e.g., sales) and on individual-level choice data (e.g., brand choices). 

To successfully complete the assignments in this course, you will require Microsoft Excel. If you do not have Excel, you can download a free 30-day trial here: https://products.office.com/en-us/try
      


          Meet Dr. Schweidel & Course Overview
    -In this module, students will be introduced to the instructor, Dr. David Schweidel and get and overview of the course. 

Exploring your Data with Visualization and Descriptive Statistics, Part 1
    -Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions.

Exploring your Data with Visualization and Descriptive Statistics, Part 2
    -Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions.

Regression Analysis for Marketing Data
    -In this module, you will be asked to determine the appropriate type of regression for different types of marketing data and will perform regression analysis to assess the impact of marketing actions on outcomes of interest, such as sales, traffic, and brand choices. You will also be asked to interpret regression output to understand overall model performance and importance of different predictors, as well as make predictions using the appropriate regression model.

From Analysis to Action
    -This final module will connect the results of regression analysis to marketing decisions. You will learn to build tools that allow users to evaluate outcomes based on different marketing decisions, as well as characterize the extent of uncertainty in outcomes based on the selected marketing decisions."
https://www.classcentral.com/course/edx-statistical-thinking-for-data-science-and-analytics-4913,"This statistics and data analysis course will pave the statistical foundation for our discussion on data science.
You will learn how data scientists exercise statistical thinking in designing data collection, derive insights from visualizing data, obtain supporting evidence for data-based decisions and construct models for predicting future trends from data.



Week 1 – Introduction to Data Science
Week 2 – Statistical Thinking

Examples of Statistical Thinking
Numerical Data, Summary Statistics
From Population to Sampled Data
Different Types of Biases
Introduction to Probability
Introduction to Statistical Inference 

Week 3 – Statistical Thinking 2

Association and Dependence
Association and Causation
Conditional Probability and Bayes Rule
Simpsons Paradox, Confounding
Introduction to Linear Regression
Special Regression Models

Week 4 – Exploratory Data Analysis and Visualization

Goals of statistical graphics and data visualization
Graphs of Data
Graphs of Fitted Models
Graphs to Check Fitted Models
What makes a good graph?
Principles of graphics

Week 5 – Introduction to Bayesian Modeling

Bayesian inference: combining models and data in a forecasting problem
Bayesian hierarchical modeling for studying public opinion
Bayesian modeling for Big Data"
https://www.classcentral.com/course/strategic-business-management-microecono-9220,"This course weds business strategy with the principles of microeconomics. It offers valuable a powerful toolbox together with cases and lessons across all major functions of business, management, from finance, operations management, and marketing to human resource management, organizational behavior, statistics, and, of course, business strategy.
      


           Introduction to Microeconomics for Managers

Supply and Demand

An Introduction to Consumer Theory

Production Theory

Perfect Competition

Monopoly and Monopolistic Competition

Oligopoly and Strategic Management

Strategic Capital Finance

Land, Labor, and Organizational Behavior

Public Goods, Externalities, and Income Redistribution

 Finals Week"
https://www.classcentral.com/course/strategic-business-management-macroecono-9221,"This course weds business strategy with the principles of macroeconomics. It offers valuable a powerful toolbox together with cases and lessons across all major functions of business, management, from finance, operations management, and marketing to human resource management, organizational behavior, statistics, and, of course, business strategy.
      


          An Overview of Modern Macroeconomics

The Aggregate Supply-Aggregate Demand Model 

The Keynesian Model and Fiscal Policy

Monetary Policy and Business Strategy

Inflation and the Warring Schools of Macroeconomics

Growth in the Developed and Developing Worlds

International Trade and the Gains (and Losses) From Trade

Exchange Rates, The Balance of Payments, and Trade Deficits

Macroeconomics and Public Finance - The Budget Deficit Dilemma

Strategic Business Cycle Managing and Investing 

Finals Week"
https://www.classcentral.com/course/big-data-emerging-technologies-11987,"Every time you use Google to search something, every time you use Facebook, Twitter, Instagram or any other SNS (Social Network Service), and every time you buy from a recommended list of products on Amazon.com you are using a big data system. In addition, big data technology supports your smartphone, smartwatch, Alexa, Siri, and automobile (if it is a newer model) every day. The top companies in the world are currently using big data technology, and every company is in need of advanced big data technology support. Simply put, big data technology is not an option for your company, it is a necessity for survival and growth. So now is the right time to learn what big data is and how to use it in advantage of your company. This 6 module course first focuses on the world’s industry market share rankings of big data hardware, software, and professional services, and then covers the world’s top big data product line and service types of the major big data companies. Then the lectures focused on how big data analysis is possible based on the world’s most popular three big data technologies Hadoop, Spark, and Storm. The last part focuses on providing experience on one of the most famous and widely used big data statistical analysis systems in the world, the IBM SPSS Statistics. This course was designed to prepare you to be more successful in businesses strategic planning in the upcoming big data era. Welcome to the amazing Big Data world!
      


          Big Data Rankings & Products
    -The first module “Big Data Rankings & Products” focuses on the relation and market shares of big data hardware, software, and professional services. This information provides an insight to how future industry, products, services, schools, and government organizations will be influenced by big data technology. To have a deeper view into the world’s top big data products line and service types, the lecture provides an overview on the major big data company, which include IBM, SAP, Oracle, HPE, Splunk, Dell, Teradata, Microsoft, Cisco, and AWS. In order to understand the power of big data technology, the difference of big data analysis compared to traditional data analysis is explained. This is followed by a lecture on the 4 V big challenges of big data technology, which deal with issues in the volume, variety, velocity, and veracity of the massive data. Based on this introduction information, big data technology used in adding global insights on investments, help locate new stores and factories, and run real-time recommendation systems by Wal-Mart, Amazon, and Citibank is introduced.

Big Data & Hadoop
    -The second module “Big Data & Hadoop” focuses on the characteristics and operations of Hadoop, which is the original big data system that was used by Google. The lectures explain the functionality of MapReduce, HDFS (Hadoop Distributed FileSystem), and the processing of data blocks. These functions are executed on a cluster of nodes that are assigned the role of NameNode or DataNodes, where the data processing is conducted by the JobTracker and TaskTrackers, which are explained in the lectures. In addition, the characteristics of metadata types and the differences in the data analysis processes of Hadoop and SQL (Structured Query Language) are explained. Then the Hadoop Release Series is introduced which include the descriptions of Hadoop YARN (Yet Another Resource Negotiator), HDFS Federation, and HDFS HA (High Availability) big data technology.

Spark
    -The third module “Spark” focuses on the operations and characteristics of Spark, which is currently the most popular big data technology in the world. The lecture first covers the differences in data analysis characteristics of Spark and Hadoop, then goes into the features of Spark big data processing based on the RDD (Resilient Distributed Datasets), Spark Core, Spark SQL, Spark Streaming, MLlib (Machine Learning Library), and GraphX core units. Details of the features of Spark DAG (Directed Acyclic Graph) stages and pipeline processes that are formed based on Spark transformations and actions are explained. Especially, the definition and advantages of lazy transformations and DAG operations are described along with the characteristics of Spark variables and serialization. In addition, the process of Spark cluster operations based on Mesos, Standalone, and YARN are introduced.

Spark ML & Streaming
    -The fourth module “Spark ML & Streaming” focuses on how Spark ML (Machine Learning) works and how Spark streaming operations are conducted. The Spark ML algorithms include featurization, pipelines, persistence, and utilities which operate on the RDDs (Resilient Distributed Datasets) to extract information form the massive datasets. The lectures explain the characteristics of the DataFrame-based API, which is the primary ML API in the spark.ml package. Spark ML basic statistics algorithms based on correlation and hypothesis testing (P-value) are first introduced followed by the Spark ML classification and regression algorithms based on linear models, naive Bayes, and decision tree techniques. Then the characteristics of Spark streaming, streaming input and output, as well as streaming receiver types (which include basic, custom, and advanced) are explained, followed by how the Spark Streaming process and DStream (Discretized Stream) enable big data streaming operations for real-time and near-real-time applications.

Storm
    -The fifth module “Storm” focuses on the characteristics and operations of Storm big data systems. The lecture first covers the differences in data analysis characteristics of Storm, Spark, and Hadoop technology. Then the features of Storm big data processing based on the nimbus, spouts, and bolts are described followed by the Storm streams, supervisor, and ZooKeeper details. Further details on Storm reliable and unreliable spouts and bolts are provided followed by the advantages of Storm DAG (Directed Acyclic Graph) and data stream queue management. In addition, the advantages of using Storm based fast real-time applications, which include real-time analytics, online ML (Machine Learning), continuous computation, DRPC (Distributed Remote Procedure Call), and ETL (Extract, Transform, Load) are introduced.

IBM SPSS Statistics Project
    -The sixth and last module “IBM SPSS Statistics Project” focuses on providing experience on one of the most famous and widely used big data statistical analysis systems in the world. First, the lecture starts with how to setup and use IBM SPSS Statistics, and continues on to describe how IBM SPSS Statistics can be used to gain corporate data analysis experience. Then the data processing statistical results of two projects based on using the IBM SPSS Statistics big data system is conducted. The projects are conducted so the student can discover new ways to use, analyze, and draw charts of the relationship between datasets, and also compare the statistical results using IBM SPSS Statistics."
https://www.classcentral.com/course/data-management-visualization-4184,"Whether being used to customize advertising to millions of website visitors or streamline inventory ordering at a small restaurant, data is becoming more integral to success. Too often, we’re not sure how use data to find answers to the questions that will make us more successful in what we do. In this course, you will discover what data is and think about what questions you have that can be answered by the data – even if you’ve never thought about data before. Based on existing data, you will learn to develop a research question, describe the variables and their relationships, calculate basic statistics, and present your results clearly. By the end of the course, you will be able to use powerful data analysis tools – either SAS or Python – to manage and visualize your data, including how to deal with missing data, variable groups, and graphs. Throughout the course, you will share your progress with others to gain valuable feedback, while also learning how your peers use data to answer their own questions.
      


          Selecting a research question
    -We would like to welcome you to Wesleyan University's Data Analysis and Interpretation Specialization. In this session, we will discuss the basics of data analysis. Your task will be to select a data set that you would like to work with and to review available code books that help you develop your own research question. You will also set up a Tumblr blog that will allow you to reflect on these experiences, submit assignments and share your work with others throughout the course. First, you may want to check out the welcome video

Writing your first program - SAS or Python
    -In this session, we will discuss how to write a basic program that allows you to load a data set and examine frequency distributions. Your task will be to write a program that helps you to explore the variables you have selected for your own research question. You may choose either Python or SAS. Both are made freely available, and we have created a helpful guide to support you in making the decision. Once you have selected your platform, just follow the instructions in the appropriate ""GETTING STARTED...."" file, and then check out this week's video lessons aimed at helping you write and run your first program. You need only view the lessons for one of the statistical platforms (SAS or Python).


Managing Data
    -In this session, we will help you to make and implement even more decisions with data. Statisticians often call this task 'data management', while computer scientists like the term 'data munging'. Whatever you call it, it is a vital and ongoing process when working with data. Your task will be to write a program that manages the variables you have selected for your own research question. 

Visualizing Data
    -In this session we will discuss descriptive statistics and get you visualizing your newly data managed variables individually and as graphs showing the relationships between them.  

Supplemental Materials (All Weeks)"
https://www.classcentral.com/course/media-data-4925,"How can we know which numbers to trust?
Increasingly, we’re bombarded with all sorts of data about how society is changing. From opinion poll trends and migration data to economic results and government debt levels.
On this course from the Sheffield Methods Institute at University of Sheffield, we’ll look at ways of cutting through the confusion to decide what numbers reveal, when and why they (sometimes deliberately) mislead, and how to determine what is ‘fake news.’
This course is open to anyone who wants to know how to make sense of social statistics and economic data in the media.
It will be particularly useful to first-year undergraduate students studying social science, as well as school leavers who are thinking about taking a social science or quantitative social science degree."
https://www.classcentral.com/course/bigdataschool-2482,"This is not a class as it is commonly understood; it is the set of materials from a summer school offered by Caltech and JPL, in the sense used by most scientists: an intensive period of learning of some advanced topics, not on an introductory level. The school will cover a variety of topics, with a focus on practical 
computing applications in research: the skills needed for a 
computational (""big data"") science, not computer science.  The specific 
focus will be on applications in astrophysics, earth science (e.g., 
climate science) and other areas of space science, but with an emphasis 
on the general tools, methods, and skills that would apply across other 
domains as well.  It is aimed at an audience of practicing researchers who already have a strong background in computation and data analysis.  The lecturers include computational science and 
technology experts from Caltech and JPL.Students can evaluate their own progress, but there will be no tests, exams, and no formal credit or certificates will be offered.



The anticipated schedule of lectures (subject to changes):Each bullet bellow corresponds to a set of materials that includes approximately 2 hours of video lectures, various links and supplementary materials, plus some on-line, hands-on exercises.1. Introduction to the school.  Software architectures.  Introduction to Machine Learning.2. Best programming practices.  Information retrieval.3. Introduction to R.  Markov Chain Monte Carlo.4. Statistical resampling and inference.5. Databases.6. Data visualization.7. Clustering and classification.8. Decision trees and random forests.9. Dimensionality reduction.  Closing remarks."
https://www.classcentral.com/course/fmri-1035,"In this course we will explore the intersection of statistics and functional magnetic resonance imaging, or fMRI, which is a non-invasive technique for studying brain activity. We will discuss the analysis of fMRI data, from its acquisition to its use in locating brain activity, making inference about brain connectivity and predictions about psychological or disease states. A standard fMRI study gives rise to massive amounts of noisy data with a complicated spatio-temporal correlation structure. Statistics plays a crucial role in understanding the nature of the data and obtaining relevant results that can be used and interpreted by neuroscientists.
      


          Week 1 Module 1: Introduction to fMRIModule 2: Basic MR PhysicsModule 3: Image FormationModule 4 K-SpaceModule 5: fMRI Signal and NoiseWeek 2 Module 6: fMRI Data StructureModule 7: Experimental DesignModule 8: Pre-processing IModule 9: Pre-processing IIWeek 3Module 10: The General Linear ModelModule 11: GLM EstimationModule 12: Model Building IModule 13: Model Building IIModule 14: Noise ModelsModule 15: InferenceWeek 4 Module 16: Group-level Analysis IModule 17: Group-level Analysis IIModule 18: Multiple ComparisonsModule 19: FWER CorrectionModule 20: FDR CorrectionModule 21: More Multiple ComparisonsWeek 5Module 22: Brain ConnectivityModule 23: Functional ConnectivityModule 24: Multivariate Decomposition MethodsModule 25: Effective ConnectivityModule 26: Comments on ConnectivityWeek 6 Module 27: Multi-voxel Pattern AnalysisModule 28: Performing MVPA IModule 29: Performing MVPA IIModule 30: MVPA Example Module 31: Farewell"
https://www.classcentral.com/course/edx-data-science-essentials-6512,"This course is part of the Microsoft Professional Program Certificate in Data Science and Microsoft Professional Program in Artificial Intelligence.
Demand for data science talent is exploding. Develop your career as a data scientist, as you explore essential skills and principles with experts from Duke University and Microsoft.
In this data science course, you will learn key concepts in data acquisition, preparation, exploration, and visualization taught alongside practical application oriented examples such as how to build a cloud data science solution using Microsoft Azure Machine Learning platform, or with R, and Python on Azure stack.edX offers financial assistance for learners who want to earn Verified Certificates but who may not be able to pay the fee. To apply for financial assistance, enroll in the course, then follow this link to complete an application for assistance.



          Explore the data science process – An Introduction • Understand data science thinking • Know the data science process • Use AML to create and publish a first machine learning experiment • Lab: Creating your first model in Azure Machine Learning Probability and statistics in data science • Understand and apply confidence intervals and hypothesis testing • Understand the meaning and application of correlation Know how to apply simulation • Lab: Working with probability and statistics • Lab: Simulation and hypothesis testing Working with data – Ingestion and preparation • Know the basics of data ingestion and selection • Understand the importance and process for data cleaning, integration and transformation • Lab: Data ingestion and selection - new • Lab: Data munging with Azure Machine Learning, R, and Python on Azure stack Data Exploration and Visualization • Know how to create and interpret basic plot types • Understand the process of exploring datasets • Lab: Exploring data with visualization with Azure Machine Learning, R and Python Introduction to Supervised Machine Learning • Understand the basic concepts of supervised learning • Understand the basic concepts of unsupervised learning • Create simple machine learning models in AML • Lab: Classification of people by income • Lab: Auto price prediction with regression • Lab: K-means clustering with Azure Machine Learning"
https://www.classcentral.com/course/linear-models-2-7476,"Welcome to the Advanced Linear Models for Data Science Class 2: Statistical Linear Models. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:

- A basic understanding of linear algebra and multivariate calculus.
- A basic understanding of statistics and regression models.
- At least a little familiarity with proof based mathematics.
- Basic knowledge of the R programming language.

After taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models.
      


          Introduction and expected values
    -In this module, we cover the basics of the course as well as the prerequisites. We then cover the basics of expected values for multivariate vectors. We conclude with the moment properties of the ordinary least squares estimates. 

The multivariate normal distribution
    -In this module, we build up the multivariate and singular normal distribution by starting with iid normals.

Distributional results
    -In this module, we build the basic distributional results that we see in multivariable regression.

Residuals
    -In this module we will revisit residuals and consider their distributional results. We also consider the so-called PRESS residuals and show how they can be calculated without re-fitting the model."
https://www.classcentral.com/course/descriptive-statistics-statistical-distr-7025,"The ability to understand and apply Business Statistics is becoming increasingly important in the industry. A good understanding of Business Statistics is a requirement to make correct and relevant interpretations of data. Lack of knowledge could lead to erroneous decisions which could potentially have negative consequences for a firm. This course is designed to introduce you to Business Statistics. We begin with the notion of descriptive statistics, which is summarizing data using a few numbers. Different categories of descriptive measures are introduced and discussed along with the Excel functions to calculate them. The notion of probability or uncertainty is introduced along with the concept of a sample and population data using relevant business examples. This leads us to various statistical distributions along with their Excel functions which are then used to model or approximate business processes. You get to apply these descriptive measures of data and various statistical distributions using easy-to-follow Excel based examples which are demonstrated throughout the course.

To successfully complete course assignments, students must have access to Microsoft Excel. 
________________________________________
WEEK 1
Module 1: Basic Data Descriptors
In this module you will get to understand, calculate and interpret various descriptive or summary measures of data. These descriptive measures summarize and present data using a few numbers. Appropriate Excel functions to do these calculations are introduced and demonstrated.

Topics covered include:
•	Categories of descriptive data
•	Measures of central tendency, the mean, median, mode, and their interpretations and calculations
•	Measures of spread-in-data, the range, interquartile-range, standard deviation and variance
•	Box plots
•	Interpreting the standard deviation measure using the rule-of-thumb and Chebyshev’s theorem
________________________________________
WEEK 2
Module 2: Descriptive Measures of Association, Probability, and Statistical Distributions
This module presents the covariance and correlation measures and their respective Excel functions. You get to understand the notion of causation versus correlation. The module then introduces the notion of probability and random variables and starts introducing statistical distributions.

Topics covered include:
•	Measures of association, the covariance and correlation measures; causation versus correlation
•	Probability and random variables; discrete versus continuous data
•	Introduction to statistical distributions
________________________________________
WEEK 3
Module 3: The Normal Distribution
This module introduces the Normal distribution and the Excel function to calculate probabilities and various outcomes from the distribution. 

Topics covered include:
•	Probability density function and area under the curve as a measure of probability
•	The Normal distribution (bell curve), NORM.DIST, NORM.INV functions in Excel
________________________________________
WEEK 4
Module 4: Working with Distributions, Normal, Binomial, Poisson
In this module, you'll see various applications of the Normal distribution. You will also get introduced to the Binomial and Poisson distributions. The Central Limit Theorem is introduced and explained in the context of understanding sample data versus population data and the link between the two.

Topics covered include:
•	Various applications of the Normal distribution
•	The Binomial and Poisson distributions
•	Sample versus population data; the Central Limit Theorem
      


            Read more
          



          Basic Data Descriptors

Descriptive Measures of Association, Probability, and Statistical Distributions

The Normal Distribution

Working with Distributions (Normal, Binomial, Poisson), Population and Sample Data"
https://www.classcentral.com/course/ds-10631,"Apache Spark is the de-facto standard for large scale data processing. This is the first course of a series of courses towards the IBM Advanced Data Science Specialization. We strongly believe that is is crucial for success to start learning a scalable data science platform since memory and CPU constraints are to most limiting factors when it comes to building advanced machine learning models.

In this course we teach you the fundamentals of Apache Spark using python and pyspark. We'll introduce Apache Spark in the first two weeks and learn how to apply it to compute basic exploratory and data pre-processing tasks in the last two weeks. Through this exercise you'll also be introduced to the most fundamental statistical measures and data visualization technologies.

This gives you enough knowledge to take over the role of a data engineer in any modern environment. But it gives you also the basis for advancing your career towards data science. 

Please have a look at the full specialization curriculum:
https://www.coursera.org/specializations/advanced-data-science-ibm

If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge.  To find out more about IBM digital badges follow the link ibm.biz/badging.


After completing this course, you will be able to:
•	Describe how basic statistical measures, are used to reveal  patterns within the data 
•	Recognize data characteristics, patterns, trends, deviations or inconsistencies, and potential outliers.
•	Identify useful techniques for working with big data such as dimension reduction and feature selection methods 
•	Use advanced tools and charting libraries to:
      o	improve efficiency of analysis of big-data with partitioning and parallel analysis 
      o	Visualize the data in an number of 2D and 3D formats (Box Plot, Run Chart, Scatter Plot, Pareto Chart, and Multidimensional Scaling)

For successful completion of the course, the following prerequisites are recommended: 
•	Basic programming skills in python
•	Basic math
•	Basic SQL (you can get it easily from https://www.coursera.org/learn/sql-data-science if needed)

In order to complete this course, the following technologies will be used:
(These technologies are introduced in the course as necessary so no previous knowledge is required.)
•	Jupyter notebooks (brought to you by IBM Watson Studio for free)
•	ApacheSpark (brought to you by IBM Watson Studio for free)
•	Python

We've been reported that some of the material in this course is too advanced. So in case you feel the same, please have a look at the following materials first before starting this course, we've been reported that this really helps.

Of course, you can give this course a try first and then in case you need, take the following courses / materials. It's free...

https://cognitiveclass.ai/learn/spark

https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/f8982db1-5e55-46d6-a272-fd11b670be38/view?access_token=533a1925cd1c4c362aabe7b3336b3eae2a99e0dc923ec0775d891c31c5bbbc68

This course takes four weeks, 4-6h per week
      


            Read more
          



          Introduction the course and grading environment

Tools that support BigData solutions

Scaling Math for Statistics on Apache Spark

Data Visualization of Big Data"
https://www.classcentral.com/course/gcp-big-data-ml-fundamentals-8234,"This 2-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities.

At the end of this course, participants will be able to:
• Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform
• Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform
• Employ BigQuery and Cloud Datalab to carry out interactive data analysis
• Choose between Cloud SQL, BigTable and Datastore
• Train and use a neural network using TensorFlow
• Choose between different data processing products on the Google Cloud Platform

Before enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following:
• A common query language such as SQL
• Extract, transform, load activities
• Data modeling
• Machine learning and/or statistics
• Programming in Python

Google Account Notes:
• Google services are currently unavailable in China.
      


          Introduction to the Data and Machine Learning on Google Cloud Platform Specialization .
    -Welcome to the Big Data and Machine Learning fundamentals on GCP course. Here you will learn the basics of how the course is structured and the four main big data challenges you will solve for.

Recommending Products using Cloud SQL and Spark
    -In this module you will have an existing Apache SparkML recommendation model that is running on-premise. You will learn about recommendation models and how you can run them in the cloud with Cloud Dataproc and Cloud SQL.

Predict Visitor Purchases with BigQuery ML
    -In this module, you will learn the foundations of BigQuery and big data analysis at scale. You will then learn how to build your own custom machine learning model to predict visitor purchases using just SQL with BigQuery ML. 

Create Streaming Data Pipelines with Cloud Pub/sub and Cloud Dataflow
    -In this module you will engineer and build an auto-scaling streaming data pipeline to ingest, process, and visualize data on a dashboard. Before you build your pipeline you'll learn the foundations of message-oriented architecture and pitfalls to avoid when designing and implementing modern data pipelines.

Classify Images with Pre-Built Models using Vision API and Cloud AutoML
    -Don't want to create a custom ML model from scratch? Learn how to leverage and extend pre-built ML models like the Vision API and Cloud AutoML for image classification.

Summary
    -In this final module, we will review the key challenges, solutions, and topics covered as part of this fundamentals course. We will also review additional resources and the steps you can take to get certified as a Google Cloud Data Engineer."
https://www.classcentral.com/course/wharton-business-financial-modeling-caps-5547,"In this Capstone you will recommend a business strategy based on a data model you’ve constructed. Using a data set designed by Wharton Research Data Services (WRDS), you will implement quantitative models in spreadsheets to identify the best opportunities for success and minimizing risk. Using your newly acquired decision-making skills, you will structure a decision and present this course of action in a professional quality PowerPoint presentation which includes both data and data analysis from your quantitative models.

Wharton Research Data Services (WRDS) is the leading data research platform and business intelligence tool for over 30,000 corporate, academic, government and nonprofit clients in 33 countries. WRDS provides the user with one location to access over 200 terabytes of data across multiple disciplines including Accounting, Banking, Economics, ESG, Finance, Insurance, Marketing, and Statistics.
      


          Getting Started
    -Welcome!  This opening module was designed to give you an overview of the Business and Financial Modeling Capstone, in which you will be working with historical financial data to calculate individual returns and summary statistics on those returns. The project has multiple steps, which are outlined below in the ""Project Prompt"", and culminates in a recommendation for portfolio allocation that you will prepare a presentation on. You will draw on elements from all courses to complete this project, and you can use your final presentation as a work sample to improve your current job or even find a new one.  Before moving on, complete the ""Project Scope Quiz."" The work you do this week enables you to understand the steps needed to successfully complete your final project.

Steps 1 and 2: Yahoo Finance
    -In this module, which correlates to Steps 1 and 2 in the Project Prompt, you'll be working with a historical data set to calculate performance data and to provide summary statistics on that data. These calculations will allow you to practice using Spreadsheets for financial calculations, and provides the foundational skills and numbers for the next steps of the project. First, you'll use the set to calculate daily returns on a set of securities. You'll then use your Spreadsheet skills to calculate summary statistics. You'll be given the opportunity to test your knowledge with a sample return to see if your calculations are correct.  And you may want to refresh your recollection of the content from the Specialization with the lectures included here.  The work you complete this week allows you to form the basis for comparing stock performance, which you will use in creating the investment portfolio for your final project as well as the comparison to the performance of a single stock.

Step 3: Creating an optimal risky portfolio on the efficient frontier
    -In this module, you'll go beyond calculating simple returns to tackle the more advanced task of finding the minimum variance and ""optimal risk portfolio"" weights for a portfolio of selected securities (note, the ""optimal risky portfolio"" is also known as an ""optimal portfolio"" or ""tangent portfolio"").  You'll follow the tasks in Step 3 in the Project Prompt and use the resources below to calculate the portfolio weights for two securities that results in the portfolio with the minimum variance; then, you'll calculate the ""optimal risky portfolio"" on the efficient frontier for these same two securities, then for all 10 stocks in the pool.  You'll be quizzed on your calculations and other insights that emerge from this exercise. The work you complete this week gives you practice in creating an optimal risky portfolio, which is a key component of your final project. Note:  There are a number of resources available on the internet providing step-by-step instructions on how to use Excel to create an ""optimal risky portfolio"" on the efficient frontier given a certain set of available assets.  We encourage you to attempt to use the skills you gained during the Specialization to work through these steps independently; you are, however, permitted to utilize third-party resources if you find it necessary.  We've included some lectures from the underlying Specialization courses concerning Solver, optimization, and other relevant topics.  

Step 4: Optional exercise using CAPM tables
    -The Capital Asset Pricing Model, or CAPM, is another tool used by investors to weigh the risks and rewards of potential investments.  In this optional module covering Step 4 in the Project Prompt, you can use CAPM as a vehicle to further strengthen your financial modeling skills, including using regression concepts.  You may revisit the Specialization lectures below touching on regression.  To test whether you've grasped the concepts in the CAPM model, this module includes a short quiz.  This assessment is formative, meaning your score will not count towards your final grade. The work you do this week may inform how you build the mixed asset portfolio of your final project, but it is not necessary to complete the final project.

Step 5: Creating Your Asset Allocation & Final Presentation
    -In this final module you are asked to move beyond a stock-only portfolio to one utilizing more diversified assets and to prepare a short presentation summarizing your findings.  As explained in Step 5 of the Project Prompt, you have $5 million to invest in the Vanguard Total Bond Market Index Fund (ticker: VBTLX) and Vanguard 500 Index (ticker: VFIAX) investment vehicles.  There are two assessments in this module. First, you'll complete a short quiz on the characteristics of your optimal risky portfolio.  Then, in the peer review component of this Capstone, you are tasked with preparing a short presentation that (i) explores how your portfolio of mixed asset class of funds compares to a single security (AAPL) and (ii) uses that comparison to discuss the importance of portfolio diversification."
https://www.classcentral.com/course/edx-predictive-analytics-4867,"Decision makers often struggle with questions such as: What should be the right price for a product? Which customer is likely to default in his/her loan repayment? Which products should be recommended to an existing customer? Finding right answers to these questions can be challenging yet rewarding.
Predictive analytics is emerging as a competitive strategy across many business sectors and can set apart high performing companies. It aims to predict the probability of the occurrence of a future event such as customer churn, loan defaults, and stock market fluctuations – leading to effective business management.
Models such as multiple linear regression, logistic regression, auto-regressive integrated moving average (ARIMA), decision trees, and neural networks are frequently used in solving predictive analytics problems. Regression models help us understand the relationships among these variables and how their relationships can be exploited to make decisions.
This course is suitable for students/practitioners interested in improving their knowledge in the field of predictive analytics. The course will also prepare the learner for a career in the field of data analytics. If you are in the quest for the right competitive strategy to make companies successful, then join us to master the tools of predictive analytics."
https://www.classcentral.com/course/social-media-data-analytics-7019,"Learner Outcomes: After taking this course, you will be able to:
- Utilize various Application Programming Interface (API) services to collect data from different social media sources such as YouTube, Twitter, and Flickr.
- Process the collected data - primarily structured - using methods involving correlation, regression, and classification to derive insights about the sources and people who generated that data.
- Analyze unstructured data - primarily textual comments - for sentiments expressed in them.
- Use different tools for collecting, analyzing, and exploring social media data for research and development purposes.

Sample Learner Story: Data analyst wanting to leverage social media data.
Isabella is a Data Analyst working as a consultant for a multinational corporation. She has experience working with Web analysis tools as well as marketing data. She wants to now expand into social media arena, trying to leverage the vast amounts of data available through various social media channels. Specifically, she wants to see how their clients, partners, and competitors view their products/services and talk about them. She hopes to build a new workflow of data analytics that incorporates traditional data processing using Web and marketing tools, as well as newer methods of using social media data.

Sample Job Roles requiring these skills: 
- Social Media Analyst
- Web Analyst
- Data Analyst
- Marketing and Public Relations 

Final Project Deliverable/ Artifact: The course will have a series of small assignments or mini-projects that involve data collection, analysis, and presentation involving various social media sources using the techniques learned in the class.
      


            Read more
          



          Introduction to Data Analytics
    -In this first unit of the course, several concepts related to social media data and data analytics are introduced. We start by first discussing two kinds of data - structured and unstructured. Then look at how structured data, the primary focus of this course, is analyzed and what one could gain by doing such analysis. Finally, we briefly cover some of the visualizations for exploring and presenting data.Make sure to go through the material for this unit in the sequence it's provided. First, watch the four short videos, then take the practice test, followed by the two quizzes. Finally, read the documents about installation and configuration of Python and R. This is very important - before proceeding to the next units, make sure you have installed necessary tools, and also learned how to install new packages/libraries for them. The course expects students to have programming experience in Python and R.

Collecting and Extracting Social Media Data
    -In this unit we will see how to collect data from Twitter and YouTube. The unit will start with an introduction to Python programming. Then we will use a Python script, with a little editing, to extract data from Twitter. A similar exercise will then be done with YouTube. In both the cases, we will also see how to create developer accounts and what information to obtain to use the data collection APIs.

Once again, make sure to go item-by-item in the order provided. Before beginning this unit, ensure that you have all the right tools (Python, R, Anaconda) ready and configured. The lessons depend on them and also your ability to install required packages.

Data Analysis, Visualization, and Exploration
    -In this unit, we will focus on analyzing and visualizing the data from various social media services. We will first use the data collected before from YouTube to do various statistics analyses such as correlation and regression. We will then introduce R - a platform for doing statistical analysis. Using R, then we will analyze a much larger dataset obtained from Yelp.

Make sure you have covered the material in the previous units before proceeding with this. That means, having all the tools (Anaconda, Python, and R) as well as various packages installed. We will also need new packages this time, so make sure you know how to install them to your Python or R. If needed, please review some basic concepts in statistics - specifically, correlation and regression - before or during working on this unit.

Case Studies
    -In the final unit of this course, we will work on two case studies - both using Twitter and focusing on unstructured data (in this case, text). The first case study will involve doing sentiment analysis with Python. The second case study will take us through basic text mining application using R. We wrap up the unit with a conclusion of what we did in this course and where to go next for further learning and exploration."
https://www.classcentral.com/course/data-mining-with-weka-7805,"Learn how to mine your own data
Today’s world generates more data than ever before! Being able to turn it into useful information is a key skill. This course introduces you to practical data mining using the Weka workbench. We’ll dispel the mystery that surrounds the subject. We’ll explain the principles of popular algorithms. We’ll show you how to use them in practical applications. You’ll get plenty of experience actually mining data during the course, and afterwards you’ll be well equipped to mine your own. Weka originated at the University of Waikato in NZ, and Ian Witten has authored a leading book on data mining.
This course is aimed at anyone who deals in data. It involves no computer programming, although you need some experience with using computers for everyday tasks. High school maths should be more than enough and you’ll need an understanding of some elementary statistics concepts (means and variances).
You will download the free Weka software during Week 1. It runs on any computer, under Windows, Linux, or Mac. It has been downloaded millions of times and is being used all around the world.
(Note: Depending on your computer and system version, you may need admin access to install Weka.)"
https://www.classcentral.com/course/edx-analisis-estadistico-con-excel-13320,"En este curso en línea el estudiante aprenderá los conceptos estadísticos básicos para realizar un análisis aplicado de datos, haciendo los cálculos en Excel y buscando la interpretación de cada una de las medidas calculadas, comenzando con un análisis exploratorio descriptivo de los datos hasta llevarlo a desarrollar estadística inferencial.
El énfasis del curso de análisis de datos y estadísticas es eminentemente práctico para que el estudiante pueda realizar el manejo adecuado de datos y tomar decisiones empresariales exitosas.



Lección 1: Análisis exploratorio de datos

Conceptos básicos estadísticos
Medidas de tendencia central
Medidas de posición
Medidas de dispersión
Medidas de forma

Lecci on 2: Aplicación de probabilidades

Conceptos básicos de probabilidades
Distribución binomial
Distribución normal

Lecci on 3: Cómo inferir a base de muestras y relacionando variables

Muestreo Estadístico:

¿Qué es inferencia estadística?
Cálculo de tamaño de la muestra
Cálculo de tamaño de la muestra en Excel
Esquemas de muestreo
Muestreo sistemático
Muestreo estratificado
Cómo presentar los resultados
Cómo presentar resultados con atributos

Análisis de regresión y correlación:

Análisis de regresión simple
Desarrollo de los mínimos cuadrados
Análisis de regresión múltiple

Lección 4: Números índice y confirmando supuestos
Números índice:

Índice agregado e índice simple
Índice compuesto

Teorías de las decisiones estadísticas:

Ejemplo de prueba de hipótesis
Pruebas utilizando estadístico Z
Prueba sobre la media de una población
Pruebas utilizando CHI cuadrado
Tablas de contingencia"
https://www.classcentral.com/course/data-science-crash-course-4392,"By now you have definitely heard about data science and big data. In this one-week class, we will provide a crash course in what these terms mean and how they play a role in successful organizations. This class is for anyone who wants to learn what all the data science action is about, including those who will eventually need to manage data scientists. The goal is to get you up to speed as quickly as possible on data science without all the fluff. We've designed this course to be as convenient as possible without sacrificing any of the essentials.

This is a focused course designed to rapidly get you up to speed on the field of data science. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.

After completing this course you will know. 

1. How to describe the role data science plays in various contexts
2. How statistics, machine learning, and software engineering play a role in data science
3. How to describe the structure of a data science project
4. Know the key terms and tools used by data scientists
5. How to identify a successful and an unsuccessful data science project
3. The role of a data science manager


Course cover image by r2hox. Creative Commons BY-SA: https://flic.kr/p/gdMuhT
      


          A Crash Course in Data Science
    -This one-module course constitutes the first ""week"" of the Executive Data Science Specialization. This is an intensive introduction to what you need to know about data science itself. You'll learn important terminology and how successful organizations use data science."
https://www.classcentral.com/course/datacamp-introduction-to-r-7630,"In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!



          Chapter One: learn how to use the console as a calculator and how to assign variables
Chapter Two: Learn how to create vectors in R, name them, select elements from them and compare different vectors.
Chapter Three: Learn how to work with matrices in R
Chapter Four: Learn how to create, subset and compare matrices.
Chapter Five: See how to create a data frame, select interesting parts of a data frame and order a data frame according to certain variables
Chapter Six: This final chapter will teach you how to create, name and subset these lists"
https://www.classcentral.com/course/edx-data-science-for-construction-architecture-and-engineering-19193,"The building industry is exploding with data sources that impact the energy performance of the built environment and health and well-being of occupants. Spreadsheets just don’t cut it anymore as the sole analytics tool for professionals in this field. Participating in mainstream data science courses might provide skills such as programming and statistics, however the applied context to buildings is missing, which is the most important part for beginners.
This course focuses on the development of data science skills for professionals specifically in the built environment sector. It targets architects, engineers, construction and facilities managers with little or no previous programming experience. An introduction to data science skills is given in the context of the building life cycle phases. Participants will use large, open data sets from the design, construction, and operations of buildings to learn and practice data science techniques.
Essentially this course is designed to add new tools and skills to supplement spreadsheets. Major technical topics include data loading, processing, visualization, and basic machine learning using the Python programming language, the Pandas data analytics and sci-kit learn machine learning libraries, and the web-based Colaboratory environment. In addition, the course will provide numerous learning paths for various built environment-related tasks to facilitate further growth.



Week 1: Introduction to Course and Python Fundamentals – In this introduction, an overview of key Python concepts is covered as well as the motivating factors for building industry professionals to learn to code. The NZEB at the NUS School of Design and Environment is introduced as an example of a building that uses various data science-related technologies in its design, construction, and operations.
Week 2: Introduction to the Pandas Data Analytics Library and Design Phase Application Examples – The foundational functions of Pandas are demonstrated in the context of the integrated design process through the processing of data from parametric EnergyPlus models. Further future learning path examples are introduced for the Design Phase including building information modeling (BIM) using Revit or Rhino, spatial analytics, and building performance modeling Python libraries.
Week 3: Pandas Analysis of Time-Series Data from IoT and Construction Phase Application Examples – Time-series analysis Pandas functions are demonstrated in the Construction Phase through the analysis of hourly IoT data from electrical energy meters. Further future learning path examples are introduced for the Construction Phase including project management, building management system (BMS) data analysis, and digital construction such as robotic fabrication.
Week 4: Statistics and Visualization Basics and Operations Phase Application Examples – Various statistical aggregations and visualization techniques using Pandas and the Seaborn library are demonstrated on Operations Phase occupant comfort data from the ASHRAE Thermal Comfort Database II. Further future learning path examples are introduced for the Operations Phase including energy auditing, IoT analysis, and occupant detection and reinforcement learning.
Week 5: Introduction to Machine Learning for the Built Environment – This concluding section gives an overview of the motivations and opportunities for the use of prediction in the built environment. Prediction, classification, and clustering using the sci-kit learn library is demonstrated on electrical meter and occupant comfort data. The course is concluded with suggestions on more in-depth Python, Data Science, and Statistics courses on EDx.
Development of this curriculum was led by Dr. Clayton Miller with support from NUS students Charlene Tan, Chun Fu, James Zhan, Matias Quintana, and Vanessa Neo."
https://www.classcentral.com/course/udacity-intro-to-data-science-1480,"The Introduction to Data Science class will survey the foundational topics in data science, namely:  Data Manipulation  Data Analysis with Statistics and Machine Learning  Data Communication with Information Visualization  Data at Scale -- Working with Big DataThe class will focus on breadth and present the topics briefly instead of focusing on a single topic in depth. This will give you the opportunity to sample and apply the basic techniques of data science.This course is also a part of our Data Analyst Nanodegree.Why Take This Course?You will have an opportunity to work through a data science project end to end, from analyzing a dataset to visualizing and communicating your data analysis.Through working on the class project, you will be exposed to and understand the skills that are needed to become a data scientist yourself.



Lesson 1: Introduction to Data ScienceIntroduction to Data ScienceWhat is a Data ScientistPi-Chaun (Data Scientist @ Google): What is Data Science?Gabor (Data Scientist @ Twitter): What is Data Science?Problems Solved by Data Science PandasDataframesCreate a New DataframeLesson 2: Data WranglingWhat is Data Wrangling?Acquiring DataCommon Data FormatsWhat are Relational Databases?Aadhaar Data Aadhaar Data and Relational DatabasesIntroduction to Databases SchemasAPI’sData in JSON FormatHow to Access an API efficiently Missing ValuesEasy ImputationImpute using Linear RegressionTip of the Imputation IcebergLesson 3: Data AnalysisStatistical RigorKurt (Data Scientist @ Twitter) - Why is Stats Useful?Introduction to Normal DistributionT TestWelch T TestNon-Parametric TestsNon-Normal DataStats vs. Machine LearningDifferent Types of Machine LearningPrediction with Regression Cost FunctionHow to Minimize Cost FunctionCoefficients of DeterminationLesson 4: Data VisualizationEffective Information VisualizationNapoleon's March on RussiaDon (Principal Data Scientist @ AT&T): Communicating FindingsRishiraj (Principal Data Scientist @ AT&T): Communicating Findings WellVisual EncodingsPerception of Visual CuesPlotting in PythonData ScalesVisualizing Time Series DataLesson 5: MapReduceBig Data and MapReduceBasics of MapReduceMapperReducerMapReduce with Aadhaar DataMapReduce with Subway Data"
https://www.classcentral.com/course/quantitative-research-9094,"In this course, you will obtain some insights about marketing to help determine whether there is an opportunity that actually exists in the marketplace and whether it is valuable and actionable for your organization or client.  

Week 1: Assess methods available for creating quantitative surveys, along with their advantages and disadvantages. Identify the type of questions that should be asked and avoid unambiguous survey questions.  

Week 2: Design, test, and implement a survey by identifying the target audience and maximizing response rates. You will have an opportunity to use Qualtrics, a survey software tool, to launch your own survey.

Week 3: Analyze statistical models that can be applied to your marketing data, so that you can make data-driven decisions about your marketing mix. 

Week 4: Predict most likely outcomes from the marketing decisions and match the type of analysis needed for your business problem. 

Take Quantitative Research as a standalone course or as part of the Market Research Specialization. You should have equivalent experience to completing the second course in this specialization, Qualitative Research, before taking this course. By completing the third class in the Specialization, you will gain the skills needed to succeed in the full program.
      


          Getting Started and Preparing to Design a Quantitative Survey for Market Research
    -In this module, you will be able to identify the business objectives and assess the suitability for quantitative research. You will be able to build on qualitative findings to help inform a survey. You will be able to discuss the various types of surveys and take steps to design and implement a survey. You will also be able to recognize and avoid common survey pitfalls.

Designing and Implementing a Quantitative Survey
    -In this module, you will be able to delineate the market segment and select your survey method. You will be able to identify the appropriate sample sources and determine appropriate sample size while maximizing your response rate. You will be able to design a questionnaire and appropriate response options. You will be able to perform quality control on survey questions and test and launch your survey.

Analyzing a Survey
    -In this module, you will be able to review your survey data and look for any errors. You will be able to analyze your survey conducting descriptive and inferential analysis techniques to test your hypotheses. You will be able to conduct association analysis and causal analysis with your data. Please note that this module features a fair amount of advanced statistics related math. Throughout the module, optional activities setup as practice quizzes have been provided to help reinforce what you're learning.

Assess Approaches and Interpret Quantitative Study Results
    -In this module, you will be able to assess the various approaches for analyzing your data. You will be able to consider multiple variables throughout your analysis and be able to interpret your results. Like the last module, this module features a fair amount of advanced statistics related math. Several optional activities setup as practice quizzes have been provided to help reinforce what you're learning."
https://www.classcentral.com/course/edx-high-dimensional-data-analysis-2949,"If you’re interested in data analysis and interpretation, then this is the data science course for you. We start by learning the mathematical definition of distance and use this to motivate the use of the singular value decomposition (SVD) for dimension reduction of high-dimensional data sets, and multi-dimensional scaling and its connection to principle component analysis. We will learn about the batch effect, the most challenging data analytical problem in genomics today, and describe how the techniques can be used to detect and adjust for batch effects. Specifically, we will describe the principal component analysis and factor analysis and demonstrate how these concepts are applied to data visualization and data analysis of high-throughput experimental data.
Finally, we give a brief introduction to machine learning and apply it to high-throughput, large-scale data. We describe the general idea behind clustering analysis and descript K-means and hierarchical clustering and demonstrate how these are used in genomics and describe prediction algorithms such as k-nearest neighbors along with the concepts of training sets, test sets, error rates and cross-validation.
Given the diversity in educational background of our students we have divided the series into seven parts. You can take the entire series or individual courses that interest you. If you are a statistician you should consider skipping the first two or three courses, similarly, if you are biologists you should consider skipping some of the introductory biology lectures. Note that the statistics and programming aspects of the class ramp up in difficulty relatively quickly across the first three courses. By the third course will be teaching advanced statistical concepts such as hierarchical models and by the fourth advanced software engineering skills, such as parallel computing and reproducible research concepts.
These courses make up 2 XSeries and are self-paced:
PH525.1x: Statistics and R for the Life Sciences
PH525.2x: Introduction to Linear Models and Matrix Algebra
PH525.3x: Statistical Inference and Modeling for High-throughput Experiments
PH525.4x: High-Dimensional Data Analysis
PH525.5x: Introduction to Bioconductor: annotation and analysis of genomes and genomic assays 
PH525.6x: High-performance computing for reproducible genomics
PH525.7x: Case studies in functional genomics
This class was supported in part by NIH grant R25GM114818.



            Read more"
https://www.classcentral.com/course/edx-fundamentals-of-six-sigma-quality-engineering-and-management-3310,"Since the introduction of mass production, the concept of “quality” has evolved from simple assembly line inspections to a broad approach to production and management involving an entire corporation. Quality has become a critical driver for long-term success through continuous process improvement and customer satisfaction. Quality Management today concerns the entire value chain, encompassing multi-tiered supplier networks and customer service and returns.
This business and management course balances the quantitative elements of quality engineering with a managerial approach to using quality in an organization to effect change. We cover the statistical basics needed for a Six-Sigma Green Belt certification, following the well-known process-improvement cycle: Define, Measure, Analyze, Improve, and Control. The most important quality methods and techniques are taught, including sampling, statistical process control, process capability, regression analysis, and design of experiments. Quality management is examined, from the viewpoint of quality incorporated into product design, measuring and controlling quality in production and improving quality, using interactive, guided projects and case studies. The course closes with the presentation of a full Six-Sigma project.
The contents of this course are essentially the same as those of the corresponding TUM class (Quality Engineering and Management) and will enable you to immediately understand and apply quality concepts in your work and research.
WHY TAKE THIS COURSE? 

Quality issues appear everywhere and affect the entire organisation.
You'll quickly master quality concepts so that you can apply quality tools and drive quality programs within your own organisation.
You'll gain the fundamentals for a Six-Sigma Green Belt in a manageable and positive learning environment, along with the chance to discuss with staff and fellow learners worldwide.




            Read more"
https://www.classcentral.com/course/business-intelligence-data-analytics-15127,"‘Megatrends’ heavily influence today’s organisations, industries and societies, and your ability to generate insights in this area is crucial to your organisation’s success into the future. This course will introduce you to analytical tools and skills you can use to understand, analyse and evaluate the challenges and opportunities ‘megatrends’ will inevitably bring to your organisation. Via structured learning activities you will explore how these trends can be addressed through sustainability-oriented innovation. You will be introduced to key data analytics concepts such as systems thinking, multi-level perspectives and multidisciplinary methods for envisioning futures, and apply them to specific real-world challenges you and your organisation may face. And there’ll be a focus on future-proofing skills such as teamwork, collaboration with diverse stakeholders and accounting for judgements made within ethical decision-making frameworks.
      


          Basics of insight generation
    -Organisations and governments everywhere want to exploit data to predict behaviors and extract valuable real-world insights. Billions of devices and social media conversations are fueling the rate at which humanity is producing data. Therefore, we need more skills to understand data and make our systems, policies and governance models more efficient. In this week, we will highlight the potential of generating insights with the help of data in allowing individuals, businesses, and governments to make effective decisions.

Basic statistics: Foundations of quantitative insights
    -In week 2, we’ll focus on  basic statistics. It’s one of the most important components of Data Analytics and it’s crucial to have a clear understanding of all the related concepts to be successful in the data industry. Statistics provide us with a set of tools that offer ways to convert quantitative data and qualitative data into information that we can use to generate insights. 

The normal distribution and histograms
    - Businesses must constantly strive to offer “better” products and services than their competitors. One of the oldest and time-proven techniques by which we can visualise and think about quality in a methodological way is via normal distributions or bell curves. So in week 3,  we’ll start by learning about histograms and the normal curve  and then have a look at empirical rule which gives us a quick rough estimate about the spread of the given data. Finally, we’ll learn about the measures that quantify the interrelationships between two data variables. Correlation and covariance are two important measures that quantify the relationship between variables and we’ll study both.  

Data visualisation
    -Visualisation is a key technique which can provide answers hidden in data. In this week, you will explore various data visualisations available and how to use them for analysis. These techniques will empower you to create compelling stories and dashboards from your data that the non-analyst community can also understand easily. As a person working in the data industry, you don’t just need to deal with data and solve data-driven problems but the incumbent also needs to convince company executives and government officials of the right decisions to make. These executives/officials may not be well versed in data science, so the incumbent must but be able to present and visualise the data’s story in a way they will understand. And this module will help you achieve that.

Advanced charts and dashboards
    -This week we learn how to create bar and bullet charts, and dashboards. Data visualization helps to tell stories by curating data into a form easier to understand. A good visualisation tells a story, by removing the noise from data and highlighting the useful information. 

Demand forecasting
    -This week we’ll look at how, by using predictive modelling, we can generate actionable insights that when implemented will provide businesses with a predictable future outcome. Predictive modeling is a group of methods and algorithms that you can employ to forecast an outcome. Utilising basic predictive modelling techniques, we will also explore consumer demand forecasting."
https://www.classcentral.com/course/dataanalysis-386,"You have probably heard that this is the era of “Big Data”. Stories about
    companies or scientists using data to recommend movies, discover who is
    pregnant based on credit card receipts, or confirm the existence of the
    Higgs Boson regularly appear in Forbes, the Economist, the Wall Street
    Journal, and The New York Times. But how does one turn data into this type
    of insight? The answer is data analysis and applied statistics. Data analysis
    is the process of finding the right data to answer your question, understanding
    the processes underlying the data, discovering the important patterns in
    the data, and then communicating your results to have the biggest possible
    impact. There is a critical shortage of people with these skills in the
    workforce, which is why Hal Varian (Chief Economist at Google) says that
    being a statistician will be the sexy job for the next 10 years.
    
This course is an applied statistics course focusing on data analysis.
    The course will begin with an overview of how to organize, perform, and
    write-up data analyses. Then we will cover some of the most popular and
    widely used statistical methods like linear regression, principal components
    analysis, cross-validation, and p-values. Instead of focusing on mathematical
    details, the lectures will be designed to help you apply these techniques
    to real data using the R statistical programming language, interpret the
    results, and diagnose potential problems in your analysis. You will also
    have the opportunity to critique and assist your fellow classmates with
    their data analyses.
      


            Read more"
https://www.classcentral.com/course/swayam-introduction-to-operations-research-7902,"Operations Research (OR) is a discipline that helps to make better decisions in complex scenarios by the application of a set of advanced analytical methods. It couples theories, results and theorems of mathematics, statistics and probability with its own theories and algorithms for problem solving. Applications of OR techniques spread over various fields in engineering, management and public systems.This course introduces the students to the following topicsLinear Programming,Transportation problemsAssignment problems.Advanced topics on duality.At the end of this course students will be able to understand, formulate linear programming problems and applications.INTENDED AUDIENCE: Any Interested LearnersPRE-REQUISITES: NONEINDUSTRIES/COMPANIES THAT WILL VALUE THIS COURSE: NONE 
      


COURSE LAYOUT Unit 1:  Linear Programming (LP): Terminology and formulationsUnit 2:  Graphical and Algebraic solutions to LPUnit 3:  Simplex Algorithm: Algebraic form, Tabular form, Types of LPs, Matrix methodUnit 4:  Duality: Writing the dual of an LP, Primal-Dual relationshipsUnit 5:  Dual: Basic understanding, significance, interpretation, Dual Simplex algorithmUnit 6:  Transportation ProblemUnit 7:  Assignment ProblemUnit 8:  Solving LPs using Solver, Sensitivity analysisCourse PatternA unit will contain a number of topicsEach topic will have a video of 15-20 minutes duration.There will be two assignments for each unitMost of the assignment questions will be multiple choice/fill in the blanks/writing a number as answers etc.There may be a few questions on LP formulations where the candidate will have to submit the assignment in e-format."
https://www.classcentral.com/course/sql-data-science-capstone-17298,"Data science is a dynamic and growing career field that demands knowledge and skills-based in SQL to be successful. This course is designed to provide you with a solid foundation in applying SQL skills to analyze data and solve real business problems.

Whether you have successfully completed the other courses in the Learn SQL Basics for Data Science Specialization or are taking just this course, this project is your chance to apply the knowledge and skills you have acquired to practice important SQL  querying and solve problems with data. You will participate in your own personal or professional journey to create a portfolio-worthy piece from start to finish. You will choose a dataset and develop a project proposal. You will explore your data and perform some initial statistics you have learned through this specialization. You will uncover analytics for qualitative data and consider new metrics that make sense from the patterns that surface in your analysis. You will put all of your work together in the form of a presentation where you will tell the story of your findings. Along the way, you will receive feedback through the peer-review process. This community of fellow learners will provide additional input to help you refine your approach to data analysis with SQL and present your findings to clients and management.
      


          Getting Started and Milestone 1: Project Proposal and Data Selection/Preparation
    -In this first milestone, you will select your client and import your dataset. You will begin to explore your data to understand it and make assumptions about your data. You will draft a project proposal to act as a guide as you explore your data and prove or disprove your hypotheses.

Milestone 2: Descriptive Stats & Understanding Your Data
    -In this milestone, you will start to execute your project proposal. You will start looking at your data and perform initial statistic models to explore your data and determine what you have available to you. 

Milestone 3: Beyond Descriptive Stats (Dive Deeper/Go Broader)
    -In this milestone, you will go beyond the descriptive statistics you completed in the last milestone. This milestone is really about diving deeper to analyze your data, beyond descriptive stats. Maybe you need to analyze qualitative data or textual data to get a full picture.

Milestone 4: Presenting Your Findings (Storytelling)
    -In this milestone, you will present your findings. You will identify your audience and create a presentation tailored to them. You will be able to tell the story of analyses and make recommendations."
https://www.classcentral.com/course/data-science-k-means-clustering-python-13623,"Organisations all around the world are using data to predict behaviours and extract valuable real-world insights to inform decisions. Managing and analysing big data has become an essential part of modern finance, retail, marketing, social science, development and research, medicine and government.

This MOOC, designed by an academic team from Goldsmiths, University of London, will quickly introduce you to the core concepts of Data Science to prepare you for intermediate and advanced Data Science courses. It focuses on the basic mathematics, statistics and programming skills that are necessary for typical data analysis tasks. 

You will consider these fundamental concepts on an example data clustering task, and you will use this example to learn basic programming skills that are necessary for mastering Data Science techniques. During the course, you will be asked to do a series of mathematical and programming exercises and a small data clustering project for a given dataset.
      


          Week 1: Foundations of Data Science: K-Means Clustering in Python
    -This week we will introduce you to the course and to the team who will be guiding you through the course over the next 5 weeks. The aim of this week's material is to gently introduce you to Data Science through some real-world examples of where Data Science is used, and also by highlighting some of the main concepts involved.

Week 2: Means and Deviations in Mathematics and Python

Week 3: Moving from One to Two Dimensional Data

Week 4: Introducing Pandas and Using K-Means to Analyse Data

Week 5: A Data Clustering Project"
https://www.classcentral.com/course/edx-data-analysis-for-social-scientists-6842,"This course is now part of two independent MITx MicroMasters programs. For both MicroMasters programs, learners will need to first enroll in and pass this course. However, each program will then require different final assessments for a course certificate toward the full MicroMasters credential:

1.MicroMasters in Data, Economics, and Development Policy (DEDP).
To pursue the DEDP MicroMasters credential, pass this course, create aMicroMasters in DEDP profile, and pass an additional in-person proctored exam.
To learn more about the DEDP program and how it integrates with MIT’s new blended Master’s degree, please visithttps://micromasters.mit.edu/dedp/.
2.MicroMasters in Statistics and Data Science (SDS).
To pursue the SDS MicoMasters credential, pass this course, and enroll in and pass the final assessment at14.310Fx Data Analysis in Social Sciences-Assessment on EdX.
Complete all 4 courses and the capstone exam in the SDS program to accelerate your path towards graduate studies at MIT or other universities. To learn more, please visithttps://micromasters.mit.edu/ds.
This statistics and data analysis course will introduce you to the essential notions of probability and statistics. We will cover techniques in modern data analysis: estimation, regression and econometrics, prediction, experimental design, randomized control trials (and A/B testing), machine learning, and data visualization. We will illustrate these concepts with applications drawn from real world examples and frontier research. Finally, we will provide instruction for how to use the statistical package R and opportunities for students to perform self-directed empirical analyses.
This course is designed for anyone who wants to learn how to work with data and communicate data-driven findings effectively.
Course Previews:
Our course previews are meant to give prospective learners the opportunity to get a taste of the content and exercises that will be covered in each course. If you are new to these subjects, or eager to refresh your memory, each course preview also includes some available resources. These resources may also be useful to refer to over the course of the semester.
A score of 60% or above in the course previews indicates that you are ready to take the course, while a score below 60% indicates that you should further review the concepts covered before beginning the course.
Please use the this link to access the course preview.



            Read more
          



14.310x – Data Analysis for Social Scientists
Week One: Introduction
Week Two: Fundamentals of Probability, Random Variables, Joint Distributions and Collecting Data
Week Three: Describing Data, Joint and Conditional Distributions of Random Variables
Week Four: Functions and Moments of a Random Variables & Intro to Regressions
Week Five: Special Distributions, the Sample Mean, the Central Limit Theorem
Week Six: Assessing and Deriving Estimators - Confidence Intervals, and Hypothesis Testing
Week Seven: Causality, Analyzing Randomized Experiments, & Nonparametric Regression
Week Eight: Single and Multivariate Linear Models
Week Nine: Practical Issues in Running Regressions, and Omitted Variable Bias
Week Ten: Endogeneity, Instrumental Variables, and Experimental Design 
Week Eleven: Intro to Machine Learning and Data Visualization
Optional: Writing an Empirical Paper"
https://www.classcentral.com/course/france-universite-numerique-analyse-des-donnees-multidimensionnelles-4716,"A propos du cours Cette 6ème édition du cours d'analyse de données multidimensionnelles débutera le 2 mars 2020. Ce cours vise à comprendre et appliquer les méthodes fondamentales de l'analyse des données : analyse en composantes principales, analyse factorielle des correspondances, analyse des correspondances multiples, classification ascendante hiérarchique. Une extension vers l'analyse factorielle multiple permettra d'aborder le traitement de données plus complexes (nouveauté par rapport à la première édition). Conçu en vue des applications, ce cours donne une large place aux exemples et à la mise en œuvre logicielle (logiciel FactoMineR de R). La présentation des méthodes recourt le moins possible au formalisme mathématique en privilégiant l'approche géométrique. L'objectif est de rendre les participants autonomes dans la mise en œuvre et l'interprétation d'analyses exploratoires multidimensionnelles.



Plan du cours Semaine 1 : Analyse en composantes principales Données, problématique et exemples Recherche d'une représentation des individus Interprétation de la représentation des individus grâce aux variables Représentation des variables Aides à l'interprétation Mise en œuvre sous FactoMineR  Semaine 2 : Analyse factorielle des correspondances Données, notations, questions Liaison et indépendance entre deux variables qualitatives Comment l'AFC appréhende-t-elle l'écart à l'indépendance ? Nuages des lignes et des colonnes et leur représentation Pourcentages d'inertie et inerties en AFC Représentation simultanée des lignes et des colonnes  Aides à l'interprétation  Mise en œuvre sous FactoMineR  Semaine 3 : Analyse des correspondances multiples  Données, objectifs et problématique  Transformation du tableau des données  Représentation des individus  Représentation des modalités comme aide à l'interprétation de la représentation des individus  Nuage des modalités et sa représentation optimale  Représentation simultanée des deux nuages  Interprétation des valeurs propres  Représentation des variables  Aides à l'interprétation  Tableau de Burt  Mise en œuvre sous FactoMineR  Semaine 4 : Classification  Données, définitions  Principe de construction d'un arbre hiérarchique  Algorithme de partitionnement : les K-means  Consolidation des classes  Classification sur données de grande dimension  Analyse factorielle et classification  Caractérisation des classes d'individus  Mise en œuvre sous FactoMineR  Semaine 5 : Analyse Factorielle Multiple  Données, problématique  Equilibre des groupes et choix d'une pondération des variables  Etude et représentation des groupes de variables  Représentation des points partiels  Représentation des analyses séparées  Prise en compte de groupes de variables qualitatives  Prise en compte de tableaux de contingence  Aide à l'interprétation  Mise en œuvre sous FactoMineR"
https://www.classcentral.com/course/swayam-business-statistics-12992,"This course will introduce you to business statistics, or the application of statistics in the workplace. Statistics is a course in the methods for gathering, analyzing, and interpreting data. You’ll also explore basic probability concepts, including measuring and modeling uncertainty, and you’ll use various data distributions, along with the Linear Regression Model, to analyze and inform business decisions.INTENDED AUDIENCE : Students of B.Tech. M.Tech, PhD and working professionalsPREREQUISITES : NoneINDUSTRY SUPPORT : None 
      


COURSE LAYOUT Week 1:Introduction, data collection and presenting data in tablesWeek 2:Numerical descriptive measures and basic probabilityWeek 3: Discrete and continuous probability distributionsWeek 4: Sampling and sampling distributionsWeek 5: Confidence interval estimationWeek 6: One sample tests and hypothesis testingWeek 7: Two sample tests meansWeek 8: Two sample tests proportions and varianceWeek 9: ANOVAWeek 10: Chi-Square testsWeek 11: Simple linear regressionWeek 12: Multiple regression basics"
https://www.classcentral.com/course/data-science-8118,"Work with airline data to learn the fundamentals of the R platform.
We live in a data driven world. So how can we make the most of it? Have you ever wondered how data-driven decisions are made?
This course will use airline data to demonstrate key concepts involved in the analysis of big data. In this course you will learn how to use the R platform to manage data. The course serves as an introduction to the R software. It lays the foundation for anyone to begin studying data science and its applications, or to prepare learners to take more advanced courses related to data science, such as machine learning and computational statistics.
This course is for anyone who is interested in discovering more about data.  Beginners in using R are welcome.  Prior experience is not required.
You will need to download, install, and use the R platform.  A desktop or laptop computer is also needed to take this course as opposed to a mobile device."
https://www.classcentral.com/course/edx-foundations-of-data-science-computational-thinking-with-python-10319,"We live in an era of unprecedented access to data. Understanding how to organize and leverage the vast amounts of information at our disposal are critical skills that allow us to infer upon the world and make informed decisions. This course will introduce you to such skills. 
To work with large amounts of data, you will need to harness the power of computation through programming. This course teaches you basic programming skills for manipulating data. You will learn how to use Python to organize and manipulate data in tables, and to visualize data effectively. No prior experience with programming or Python is needed, nor is any statistics background necessary.
The examples given in the course involve real world data from diverse settings. Not all data is numerical – you will work with different types of data from a variety of domains. Though the term “data science” is relatively new, the fundamental ideas of data science are not. The course includes powerful examples that span the centuries from the Victorian era to the present day. 
This course emphasizes learning through doing: you will work on large real-world data sets through interactive assignments to apply the skills you learn. Throughout, the underlying thread is that data science is a way of thinking, not just an assortment of methods. You will also hone your interpretation and communication skills, which are essential skills for data scientists."
https://www.classcentral.com/course/python-data-processing-7751,"This course (The English copy of ""用Python玩转数据"" )  is mainly for non-computer majors. It starts with the basic syntax of Python, to how to acquire data in Python locally and from network, to how to present data, then to how to conduct basic and advanced statistic analysis and visualization of data, and finally to how to design a simple GUI to present and process data, advancing level by level. 

This course, as a whole, based on Finance data and through the establishment of popular cases one after another, enables learners to more vividly feel the simplicity, elegance, and robustness of Python. Also, it discusses the fast, convenient and efficient data processing capacity of Python in humanities and social sciences fields like literature, sociology and journalism and science and engineering fields like mathematics and biology, in addition to business fields. Similarly, it may also be flexibly applied into other fields.

The course has been updated. Updates in the new version are : 

1) the whole course has moved from Python 2.x to Python 3.x 
2) Added manual webpage fetching and parsing. Web API is also added. 
3) Improve the content order and enrich details of some content especially for some practice projects.

Note: videos are in Chinese (Simplified) with English subtitles. All other materials are in English.
      


          Welcome to learn Data Processing Using Python!
    -Hi, guys, welcome to learn “Data Processing Using Python”(The English version of ""用Python玩转数据"", url is https://www.coursera.org/learn/hipython/home/welcome)!In this course, I tell in a manner that enables non-computer majors to understand how to utilize this simple and easy programming language – Python to rapidly acquire, express, analyze and present data based on SciPy, Requests, Beautiful Soup libraries etc. Many cases are provided to enable you to easily and happily learn how to use Python to process data in many fields. 【Nov 18, 2019 @ @ @ @ @ @ @ Hi, all! The content is planned to be updated in the last two months. This update is relatively large, including practical operation and explanation of Python based cases, vector operation and broadcast ideas of numpy package and common applications, multiple links of data exploration and preprocessing (including in module 4), data analysis and data mining cases based on pandas, some of which are directly modified on the original video Some of them are presented in the form of expanded videos, especially the newly recorded videos, which have a lot of content to say, take a long time, and will be a little hard to learn. I have completed all the updates on Dec 16, 2019. Thank you for your patience. I hope you will enjoy the new version.】

Basics of Python
    -Hi, guys, welcome to learn Module 01 “Basics of Python”! I’ll first guide you to have a glimpse of its simplicity for learning as well as elegance and robustness. Less is more: the author of Python must know this idea well. After learning this module, you can master the basic language structures, data types, basic operations, conditions, loops, functions and modules in Python. With them, we can write some useful programs! 

Data Acquisition and Presentation
    -Welcome to learn Module 02 “Data Acquisition and Presentation”! After learning this module, you can master the modes of acquiring local data and network data in Python and use the basic and yet very powerful data structure sequence, string, list and tuple in Python to fast and effectively present data and simply process data. 

Powerful Data Structures and Python Extension Libraries
    -Welcome to learn Module 03 “Powerful Data Structures and Python Extension Libraries”! Have you felt you are closer to using Python to process data? After learning this module, you can master the intermediate-level and advanced uses of Python: data structure dictionaries and sets. In some applications, they can be very convenient. What’s special here is that, you can also feel the charm of such concise and efficient data structures: ndarray, Series and DataFrame in the most famous and widely applied scientific computing package SciPy in Python. 

Python Data Statistics and Mining
    -Welcome to learn Module 04 “Python data statistics and mining”! In this module, I will show you, over the entire process of data processing, the unique advantages of Python in data processing and analysis, and use many cases familiar to and loved by us to learn about and master methods and characteristics. After learning this module, you can preprocess the data and fast and effectively mine your desired or expected or unknown results from a large amount of data, and can also present those data in various images. In addition, the data statistics modes of all third party packages in Python are extraordinarily and surprisingly strong, but we, as average persons, can still understand and possess them. 

Object Orientation and Graphical User Interface
    -Welcome to Module 05 “Object Orientation and Graphical User Interface”! In this module, I will guide you to understand what object orientation is and the relationship between graphical user interface and object orientation. Learners are only required to understand the concepts so that you can more freely and easily pick up various new functions in future. No program writing is required here. Besides, you also need to master the basic framework of GUI, common components and layout management. After learning them, you will find development with GUI is actually not remote. It has an Easter egg, too ~~~"
https://www.classcentral.com/course/edx-big-data-analysis-with-apache-spark-3026,"Organizations use their data to support and influence decisions and build data-intensive products and services, such as recommendation, prediction, and diagnostic systems. The collection of skills required by organizations to support these functions has been grouped under the term ‘data science’.
This statistics and data analysis course will attempt to articulate the expected output of data scientists and then teach students how to use PySpark (part of Spark) to deliver against these expectations. The course assignments include log mining, textual entity recognition, and collaborative filtering exercises that teach students how to manipulate data sets using parallel processing with PySpark.
This course covers advanced undergraduate-level material. It requires a programming background and experience with Python (or the ability to learn it quickly). All exercises will use PySpark (the Python API for Spark), and previous experience with Spark equivalent to Introduction to Apache Spark, is required."
https://www.classcentral.com/course/edx-ut-7-01x-foundations-of-data-analysis-2244,"*Note - This is an Archived course*

In a world that’s full of data, we have many questions: How long do animals in a shelter have to wait until they are adopted? Can we model the growth of internet usage in a country? Do films with a more adult rating make more money that other rated films?

Luckily, the world is also full of data to help us answer those questions. This course will walk through the basics of statistical thinking – starting with an interesting question. Then, we’ll learn the correct statistical tool to help answer our question of interest – using R and hands-on Labs. Finally, we’ll learn how to interpret our findings and develop a meaningful conclusion.

This course will consist of instructional videos for statistical concepts broken down into manageable chunks – each followed by some guided questions to help your understanding of the topic. Most weeks, the instructional section will be followed by tutorial videos for using R, which we’ll then apply to a hands-on Lab where we will answer a specific question using real-world datasets.

We’ll cover basic Descriptive Statistics in our first “Unit” – learning about visualizing and summarizing data. Unit two will be a “modeling” investigation where we’ll learn about linear, exponential, and logistic functions. We’ll learn how to interpret and use those functions with a little bit of Pre-Calculus (but we’ll keep it very basic). Finally in the third Unit, we’ll learn about Inferential statistical tests such as the t-test, ANOVA, and chi-square.

This course is intended to have the same “punch” as a typical introductory undergraduate statistics course, with an added twist of modeling. This course is also intentionally devised to be sequential, with each new piece building on the previous topics. Once completed, students should feel comfortable using basic statistical techniques to answer their own questions about their own data, using a widely available statistical software package (R).

I hope you’ll join me in learning how to look at the world around us – what are the questions? How can we answer them? And what do those answers tell us about the world we live in?

How long is the course?
The course is scheduled to run from November 4, 2014 to February 6, 2015. While this time frame covers 13 weeks, there may be a break from December 22, 2014 to January 1, 2015 to align with the University of Texas at Austin's winter break.

Do I need a Windows PC?
No. A Mac or a PC will work just fine. You’ll need to download both R and RStudio and install them. Both pieces of software have a PC and a Mac version.

Are there any specific technology requirements?
Access to a computer with internet access that you can install software on (R and RStudio). You may also need a calculator.

Is there a text book associated with the course?
Yes and no. The text that we will be using is a custom created open source text that will be embedded into the edX course as PDF readings.

This is a past/archived course. At this time, you can only explore this course in a self-paced fashion. Certain features of this course may not be active, but many people enjoy watching the videos and working with the materials. Make sure to check for reruns of this course.
      


            Read more"
https://www.classcentral.com/course/edx-sabermetrics-101-introduction-to-baseball-analytics-1837,"This course will cover the theory and the fundamentals of the emerging science of Sabermetrics. We will discuss the game of baseball, not through consensus or a fan’s conventional wisdom, but by searching for objective knowledge in baseball performance. These and other areas of sabermetrics will be analyzed and better understood with current and historical baseball data. 
The course also serves as applied introduction to the basics of data science, an emerging field of scholarship, that requires skills in computation, statistics, and communicating results of analyses. Using baseball data, the basics of statistical regression, the R Language, and SQL will be covered.
This course was successfully taught on the edX platform as a MOOC in 2014. This course has also been successfully taught at the Experimental College at Tufts University since 2004.  Many of its former students have gone on to careers writing about baseball and working in various MLB baseball operations and analytics departments."
https://www.classcentral.com/course/edx-case-study-dna-methylation-data-analysis-2980,"In the PH525 case studies, we will explore the data analysis of an experimental protocol in depth, using various open source software, including R and Bioconductor. We will explain how to start with raw data, and perform the standard processing and normalization steps to get to the point where one can investigate relevant biological questions. Throughout the case studies, we will make use of exploratory plots to get a general overview of the shape of the data and the result of the experiment.
We will learn the basic steps in analyzing DNA methylation data, including reading the raw data, normalization, and finding regions of differential methylation across multiple samples.
This class was supported in part by NIH grant R25GM114818.
This course is part of a larger set of 8 total courses running Self-Paced through September 15th, 2015:

PH525.1x: Statistics and R for the Life Sciences
PH525.2x: Introduction to Linear Models and Matrix Algebra
PH525.3x: Advanced Statistics for the Life Sciences
PH525.4x: Introduction to Bioconductor
PH525.5x: Case study: RNA-seq data analysis
PH525.6x: Case study: Variant Discovery and Genotyping
PH525.7x: Case study: ChIP-seq data analysis
PH525.8x: Case study: DNA methylation data analysis
HarvardX requires individuals who enroll in its courses on edX to abide by the terms of the edX honor code. HarvardX will take appropriate corrective action in response to violations of the edX honor code, which may include dismissal from the HarvardX course; revocation of any certificates received for the HarvardX course; or other remedies as circumstances warrant. No refunds will be issued in the case of corrective action for such violations. Enrollees who are taking HarvardX courses as part of another program will also be governed by the academic policies of those programs.
HarvardX pursues the science of learning. By registering as an online learner in an HX course, you will also participate in research about learning. Read our research statement to learn more.
Harvard University and HarvardX are committed to maintaining a safe and healthy educational and work environment in which no member of the community is excluded from participation in, denied the benefits of, or subjected to discrimination or harassment in our program. All members of the HarvardX community are expected to abide by Harvard policies on nondiscrimination, including sexual harassment, and the edX Terms of Service. If you have any questions or concerns, please contact harvardx@harvard.edu and/or report your experience through the edX contact form.



            Read more"
https://www.classcentral.com/course/riskandreturn-4004,"This second course in the specialization will last six weeks and will focus on the second main building block of financial analysis and valuation: risk. The notion of risk and statistics are intimately related and we will spend a fair amount of time on the development of some statistical concepts and tools, namely distribution theory and regression analysis. This time will be well spent because these concepts and tools are also commonly used in many applications in the real world. The foundational idea of diversification will then be used to develop a framework for evaluating risk and establishing a relationship between risk and return. Apart from developing a keen appreciation of risk for making thoughtful decisions in an institutional context, this course will contain a lot of material and examples that will enable the learner to make smart personal investing decisions.  The course will again have time included for assimilation and two final exams.

This course is the second in a sequence of four courses that comprise a Specialization on Valuation & Investing.
      


Overview of Specialization & CourseThis module contains detailed videos and syllabi of both the Specialization and this course. This specialization has been designed to enable you to learn and apply the powerful tools of modern finance to both personal and professional situations. The courses within progress linearly and build on each other and it is important for you to get an understanding of why this specialization may be relevant to your goals, again both personal and professional. Please review the videos and syllabi as they will give you a sense of the specialization and how this specific course fits within. The teaching style and philosophy of the instructors is also presented to you (hopefully) in sufficient detail. Most importantly, it will give you enough information for you to make a decision about whether you want to take this course, by itself or as part of a specialization.Module 1Risk and return is best understood in a real context; we will therefore spend the first module on understanding the most common vehicle of investment: bonds. Module 2This second module will be spent understanding the features and valuation of stocks.Module 3In this module, we will develop a theory of risk based on the intuitive concept of diversification. Since knowledge of Statistics is key to understanding risk, we will introduce some important concepts of applications.Module 4In this module, we will introduce one of the most widely known models called CAPM (Capital Asset Pricing Model), which provides a simple and powerful relation between risk and return. Module 5This week will be spent on a short wrap video of the course and time for assimilation and review by learners to be prepared to take the final exams. In the past, learners have really valued this time and hence it is built into this on-demand structure as well.Module 6This is finals week. Please note that there are two exams and you need to attempt both."
https://www.classcentral.com/course/statistics-international-business-8041,"This course introduces core areas of statistics that will be useful in business and for several MBA modules. It covers a variety of ways to present data, probability, and statistical estimation. You can test your understanding as you progress, while more advanced content is available if you want to push yourself.

This course forms part of a specialisation from the University of London designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in international business, or in further study. 

If completed successfully, your certificate from this specialisation can also be used as part of the application process for the University of London Global MBA programme, particularly for early career applicants. If you would like more information about the Global MBA, please visit https://mba.london.ac.uk/.

This course is endorsed by CMI
      


          Using Graphs to Describe Data
    -In our study of statistics, we learn many methods to help us summarize, analyze, and interpret data with the aim of making informed decisions in an uncertain environment.  In this first week we introduce tables and graphs that help us get a handle of data. These tools provide visual support for better decision making. With this in mind, we will guide you through the concept of decisions based on incomplete information. Beginning from here, we will introduce you to the concept of population vs. sample, of parameter vs. statistic and of descriptive statistics vs. inferential statistics. We will then go through the concept of describing data, and we will introduce the idea of creating and interpreting graphs to describe categorical and continuous random variables. 

Using Measures to Describe Data
    -This week we will describe and summarize the information in the data using numerical values or measures that are able to summarise information. This is a crucial extension to the analysis of the previous week. While graphs are informative it is usually crucial for improved understanding of the data at hand to discuss their numerical properties. In this week, we will look at a range of measures, such as measures of central tendency, the range, variance, standard deviation, and so on.

Probability and Probability Distributions
    -Probability theory is a young arrival in mathematics- and probability applied to practice is almost non-existent as a discipline. We should all understand probability, and this lecture will help you to do that. It’s important for you to understand first that the world in which your future occurs is not deterministic- and there are future outcomes where a probability model cannot be developed…

This week, we will cover the basic definition of probability, the rules of probability,random variables, -probability density functions, expectations of a random variable and Bivariate random variables.




Statistical Estimation
    -For statistical analysis to work properly, it’s essential to have a proper sample, drawn from a population of items of interest that have measured characteristics. This week, we will cover statistical estimation, sampling distribution of the mean, point estimation, interval estimation, hypothesis testing, the Null hypothesis and look at some real life examples of their use."
https://www.classcentral.com/course/edx-cs115x-advanced-apache-spark-for-data-science-and-data-engineering-5854,"Gain a deeper understanding of Spark by learning about its APIs, architecture, and common use cases.  This statistics and data analysis course will cover material relevant to both data engineers and data scientists.  You’ll learn how Spark efficiently transfers data across the network via its shuffle, details of memory management, optimizations to reduce compute costs, and more.  Learners will see several use cases for Spark and will work to solve a variety of real-world problems using public datasets.  After taking this course, you should have a thorough understanding of how Spark works and how you can best utilize its APIs to write efficient, scalable code.  You’ll also learn about a wide variety of Spark’s APIs, including the APIs in Spark Streaming."
https://www.classcentral.com/course/machine-learning-asset-management-alternative-dat-17098,"Over-utilization of market and accounting data over the last few decades has led to portfolio crowding, mediocre performance and systemic risks, incentivizing financial institutions which are looking for an edge to quickly adopt alternative data as a substitute to traditional data. This course introduces the core concepts around alternative data, the most recent research in this area, as well as practical portfolio examples and actual applications. The approach of this course is somewhat unique because while the theory covered is still a main component, practical lab sessions and examples of working with alternative datasets are also key. This course is fo you if you are aiming at carreers prospects as a data scientist in financial markets, are looking to enhance your analytics skillsets to the financial markets, or if you are interested in cutting-edge technology and research as  they apply to big data. The required background is: Python programming, Investment theory , and Statistics. This course will enable you to learn new data and research techniques applied to the financial markets while strengthening data science and python skills.
      


          Consumption
    -The consumption module introduces students to the basics of consumption-based alternative data. 
By aggregating online and offline consumer purchase activity and behavioral datasets including geolocation data (e.g., cell locations, satellite imagery etc.), transaction data (e.g.,  credit card transaction logs and point of sale data), as well as consumer interaction with brands and products on social media, researchers can learn about company performance ahead of official company earning announcements. 
Such information may be extremely useful and can provide investment and risk management advantages. This module reviews the theoretical aspects of various consumption datasets, and provides practical demonstrations of relevant data analytics.

Textual Analysis for Financial Applications
    -Module 2 is an introduction to text mining as well as a demonstration of how to get from data retrieval (web scraping) to financial market insights. Some of the classic text mining methodologies are covered such as vectorization of text (the bag of words approach), stop words for filtering, and term frequency-inverse document frequency (TF-IDF). Students will learn how text can be mathematically represented, and regularized/filtered to reduce noise. Measures of text-similarity will be covered in theoretical and practice sessions. Lab sessions go through examples of web scraping data, regularizing with the described techniques and finally, insights will be derived from the textual data.

Processing Corporate Filings
    -Module 3 is a practical extension of the text mining lessons to 10-K and 13-F, two of the most commonly researched corporate filings. This type of data can be extremely daunting when used by individual analysts due to the sheer size of the documents, but module 3 describes the methodologies for quantitatively analyzing these documents with Python code. Both the 10-K and 13-F documents are worked through, and within the lab sessions it is demonstrated how one can automatically pull this kind of data as well as define metrics around them. We investigate implementations of research in this field around similarity of given companies 10-K statements over time as well as similarity between fund holdings from the 13-F in the lab.

Using Media-Derived Data
    -The final module introduces both sentiment analysis in the context of textual data as well as network analysis in the context of connectivity of firms. Sentiment analysis is an avenue of potentially fruitful information that when done correctly can display what a general population might believe about a company (through for example social media) or even whether the company itself is positive or negative on future outlook (through analysis of tone in corporate filings). Network analysis, as shown in the research of course instructors and his colleagues, can be used to accurately capture how a financial network is oriented and what companies might perform well because of other firm’s mentioning them as a threat. The lab session of this module extends the corporate filings analysis to examine sentiment while also introducing a set of tweets which are then transformed into a network representation."
https://www.classcentral.com/course/swayam-data-science-for-engineers-10096,"Learning Objectives :Introduce R as a programming languageIntroduce the mathematical foundations required for data scienceIntroduce the first level data science algorithmsIntroduce a data analytics problem solving frameworkIntroduce a practical capstone case studyLearning Outcomes:Describe a flow process for data science problems (Remembering)Classify data science problems into standard typology (Comprehension)Develop R codes for data science solutions (Application)Correlate results to the solution approach followed (Analysis)Assess the solution approach (Evaluation)Construct use cases to validate approach and identify modifications required (Creating)INTENDED AUDIENCE:  Any interested learnerPREREQUISITES: 10 hrs of pre-course material will be provided, learners need to practise this to be ready to take the course.INDUSTRY SUPPORT: HONEYWELL, ABB, FORD, GYAN DATA PVT. LTD. 
      


COURSE LAYOUT Week 1: Course philosophy and introduction to RWeek 2: Linear algebra for data science 1. Algebraic view - vectors, matrices, product of matrix & vector, rank, null space, solution of over-determined  set of equations and pseudo-inverse) 2. Geometric view - vectors, distance, projections, eigenvalue decompositionWeek 3:Statistics (descriptive statistics, notion of probability, distributions, mean, variance, covariance, covariance  matrix, understanding univariate and multivariate normal distributions, introduction to hypothesis testing, confidence  interval for estimates)Week 4: OptimizationWeek 5: 1. Optimization 2. Typology of data science problems and a solution frameworkWeek 6: 1. Simple linear regression and verifying assumptions used in linear regression 2. Multivariate linear regression, model assessment, assessing importance of different variables, subset selectionWeek 7: Classification using logistic regressionWeek 8: Classification using kNN and k-means clustering"
https://www.classcentral.com/course/edx-optimization-methods-in-business-analytics-6735,"Optimization is the search for the best and most effective solution. In this mathematics course, we will examine optimization through a Business Analytics lens. You will be introduced to the to the theory, algorithms, and applications of optimization. Linear and integer programming will be taught both algebraically and geometrically, and then applied to problems involving data. Students will develop an understanding of algebraic formulations, and use Julia/JuMP for computation. Theoretical components of the course are made approachable, and require no formal background in linear algebra or calculus.
The recommended audience for this course is undergraduates, as well as professionals interested in using optimization software. The content in this course has applications in logistics, marketing, project management, finance, statistics and machine learning.
Most of the course material will be covered in lecture and recitation videos, and only an optional textbook, available at no cost, will be used.
Students interested in the material prior to deciding on course enrollment can visit the MIT Open Courseware version of 15.053 Spring 2013. The topics of the 2013 subject were optimization modeling, algorithms, and theory. As a six week subject, 15.053x covers about half of the material of the 2013 subject. The primary focus of 15.053x is optimization modeling."
https://www.classcentral.com/course/data-analysis-tools-4169,"In this course, you will develop and test hypotheses about your data. You will learn a variety of statistical tests, as well as strategies to know how to apply the appropriate one to your specific data and question. Using your choice of two powerful statistical software packages (SAS or Python), you will explore ANOVA, Chi-Square, and Pearson correlation analysis. This course will guide you through basic statistical principles to give you the tools to answer questions you have developed. Throughout the course, you will share your progress with others to gain valuable feedback and provide insight to other learners about their work.
      


          Hypothesis Testing and ANOVA
    -This session starts where the Data Management and Visualization course left off. Now that you have selected a data set and research question, managed your variables of interest and visualized their relationship graphically, we are ready to test those relationships statistically. The first group of videos describe the process of hypothesis testing which you will use throughout this course to test relationships between different kinds of variables (quantitative and categorical). Next, we show you how to test hypotheses in the context of Analysis of Variance (when you have one quantitative variable and one categorical variable). Your task will be to write a program that manages any additional variables you may need and runs and interprets an Analysis of Variance test. Note that if your research question does not include one quantitative variable, you can use one from your data set just to get some practice with the tool. If your research question does not include a categorical variable, you can categorize one that is quantitative.

Chi Square Test of Independence
    -This session shows you how to test hypotheses in the context of a Chi-Square Test of Independence (when you have two categorical variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a Chi-Square Test of Independence. Note that if your research question only includes quantitative variables, you can categorize those just to get some practice with the tool. 

Pearson Correlation
    -This session shows you how to test hypotheses in the context of a Pearson Correlation (when you have two quantitative variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a correlation coefficient. Note that if your research question only includes categorical variables, you can choose other variables from your data set just to get some practice with the tool. 

Exploring Statistical Interactions
    -In this session, we will discuss the basic concept of statistical interaction (also known as moderation). In statistics, moderation occurs when the relationship between two variables depends on a third variable. The effect of a moderating variable is often characterized statistically as an interaction; that is, a third variable that affects the direction and/or strength of the relation between your explanatory (X) and response (Y) variable. Your task will be to test your own research question in the context of one or more potential moderating variables."
https://www.classcentral.com/course/novoed-finance-468,"We live in an uncertain world. Every day, we need to make decisions about alternatives whose consequences cannot be predicted with certainty. Here are a few examples:

You have saved 2000 dollars from your summer internship. Should you put it under your mattress, buy a Certificate of Deposit, Apple stock or an S&P 500 index fund?
You manage a mutual fund specializing in technology stocks. Which proportion of the fund's total assets should you invest in each of the stocks recommended by your analysts?
You work for a venture capital firm that wants to exit an investment. How can you compute the fair value the firm's share in the venture?

In each of these situations, you need to commit resources (time, money, effort, etc.) in the face of uncertainty about the future. This course develops concepts and tools to address these types of situations. The focus is on basic principles and how they are applied in practice. No prior knowledge of finance required. A basic preparation in mathematics (probability, statistics, and optimization) is desirable; however many technical concepts and tools will be developed or reviewed in the course. The course is appropriate for engineering or science students wishing to apply their quantitative skills to develop a basic understanding of financial modeling and markets. This is a 10 week course. There will be several short (5-30 minutes) lectures each week. Challenges covering the lecture material will be given each week. There will also be two projects that involve real financial data. Solutions will be posted online. Submissions will be evaluated by fellow participants. The following topics will be covered:

Time is money: understand basic interest rates
Evaluating investments: present value and internal rate of return
Fixed-income markets: bonds, yield, duration, portfolio immunization
Term structure of interest rates 5. Measuring risk: volatility and value at risk
Designing optimal security portfolios




            Read more"
https://www.classcentral.com/course/edx-analyzing-data-with-python-12531,"LEARN TO ANALYZE DATA WITH PYTHON
Learn how to analyze data using Python. This coursewill take you from the basics of Python to exploring many different types of data. You will learn how to prepare data for analysis, perform simple statistical analyses, create meaningful data visualizations, predict future trends from data, and more!



COURSE SYLLABUS
Module 1 - Importing Datasets

Learning Objectives
Understanding the Domain
Understanding the Dataset
Python package for data science
Importing and Exporting Data in Python
Basic Insights from Datasets

Module 2 - Cleaning and Preparing the Data

Identify and Handle Missing Values
Data Formatting
Data Normalization Sets
Binning
Indicator variables

Module 3 - Summarizing the Data Frame

Descriptive Statistics
Basic of Grouping
ANOVA
Correlation
More on Correlation

Module 4 - Model Development

Simple and Multiple Linear Regression
Model EvaluationUsingVisualization
Polynomial Regression and Pipelines
R-squared and MSE for In-Sample Evaluation
Prediction and Decision Making

Module 5 - Model Evaluation

Model Evaluation
Over-fitting, Under-fitting and Model Selection
Ridge Regression
Grid Search
Model Refinement"
https://www.classcentral.com/course/predictive-modeling-analytics-7043,"Welcome to the second course in the Data Analytics for Business specialization! 

This course will introduce you to some of the most widely used predictive modeling techniques and their core principles. By taking this course, you will form a solid foundation of predictive analytics, which refers to tools and techniques for building statistical or machine learning models to make predictions based on data. You will learn how to carry out exploratory data analysis to gain insights and prepare data for predictive modeling, an essential skill valued in the business. 

You’ll also learn how to summarize and visualize datasets using plots so that you can present your results in a compelling and meaningful way. We will use a practical predictive modeling software, XLMiner, which is a popular Excel plug-in. This course is designed for anyone who is interested in using data to gain insights and make better business decisions. The techniques discussed are applied in all functional areas within business organizations including accounting, finance, human resource management, marketing, operations, and strategic planning. 

The expected prerequisites for this course include a prior working knowledge of Excel, introductory level algebra, and basic statistics.
      


           Exploratory Data Analysis and Visualizations
    -At the end of this module students will be able to: 1. Carry out exploratory data analysis to gain insights and prepare data for predictive modeling 2. Summarize and visualize datasets using appropriate tools 3. Identify modeling techniques for prediction of continuous and discrete outcomes. 4. Explore datasets using Excel 5. Explain and perform several common data preprocessing steps         6. Choose appropriate graphs to explore and display datasets  

Predicting a Continuous Variable
    -This module introduces regression techniques to predict the value of continuous variables. Some fundamental concepts of predictive modeling are covered, including cross-validation, model selection, and overfitting. You will also learn how to build predictive models using the software tool XLMiner.

Predicting a Binary Outcome
    -This module introduces logistic regression models to predict the value of binary variables. Unlike continuous variables, a binary variable can only take two different values and predicting its value is commonly called classification. Several important concepts regarding classification are discussed, including cross validation and confusion matrix, cost sensitive classification, and ROC curves. You will also learn how to build classification models using the software tool XLMiner.

Trees and Other Predictive Models
    -This module introduces more advanced predictive models, including trees and neural networks. Both trees and neural networks can be used to predict continuous or binary variables. You will also learn how to build trees and neural networks using the software tool XLMiner."
https://www.classcentral.com/course/opensap-introduction-to-statistics-for-data-science-17229,"This course will provide you with the knowledge to understand some of the basic statistical concepts and practices that are the foundations of data science and the way we analyze data. We’ll also be highlighting how statistics can be misused and abused, leading to accidental misunderstandings or deliberate distortions to support a particular prejudiced view.
Throughout this course, you’ll be looking at how data can be summarized in a variety of ways to give you a descriptive overview of large data sets and their variations. We’ll explore how data distributions can be understood and compared. You’ll also learn how you can start finding patterns in the data, where changes in one variable may be (partly) explained by changes in another. In addition, you’ll learn a little about how to estimate the likelihood of certain outcomes based on certain prior information. Finally, we’ll link this up to some of the common tools available that make these kinds of analyses easier.
The course is spread over six weeks and consists of lectures and weekly assignments. Statistics can be a complex subject, but in this course, you’ll revisit the fundamental principles and start to appreciate how it can be used in your everyday life. We’ll focus primarily on the key principles.
 



Week 1: Introduction to Statistics
Week 2: Descriptive Statistics
Week 3: Correlation and Linear Regression
Week 4: Introduction to Probability
Week 5: Probability Distributions
Week 6: Connecting to Your SAP Solutions
Week 7: Final Exam"
https://www.classcentral.com/course/edx-capstone-exam-for-statistics-and-data-science-11484,This capstone exam is the final part of the MITx MicroMasters Program in Statistics and Data Science. Complete the four courses in this program and take this virtually-proctored exam to earn your MicroMasters credential and demonstrate your proficiency in data science or accelerate your path towards an MIT PhD or a Master's at other universities.
https://www.classcentral.com/course/edx-introduction-to-apache-spark-5855,"Spark is rapidly becoming the compute engine of choice for big data. Spark programs are more concise and often run 10-100 times faster than Hadoop MapReduce jobs. As companies realize this, Spark developers are becoming increasingly valued.
This statistics and data analysis course will teach you the basics of working with Spark and will provide you with the necessary foundation for diving deeper into Spark. You’ll learn about Spark’s architecture and programming model, including commonly used APIs. After completing this course, you’ll be able to write and debug basic Spark applications. This course will also explain how to use Spark’s web user interface (UI), how to recognize common coding errors, and how to proactively prevent errors. The focus of this course will be Spark Core and Spark SQL.
This course covers advanced undergraduate-level material. It requires a programming background and experience with Python (or the ability to learn it quickly). All exercises will use PySpark (the Python API for Spark), but previous experience with Spark or distributed computing is NOT required. Students should take this Python mini-quiz before the course and take this Python mini-course if they need to learn Python or refresh their Python knowledge."
https://www.classcentral.com/course/foundations-strategic-business-analytics-4371,"Who is this course for?  
This course is designed for students, business analysts, and data scientists who want to apply statistical knowledge and techniques to business contexts. For example, it may be suited to experienced statisticians, analysts, engineers who want to move more into a business role. 

You will find this course exciting and rewarding if you already have a background in statistics, can use R or another programming language and are familiar with databases and data analysis techniques such as regression, classification, and clustering.
However, it contains a number of recitals and R Studio tutorials which will consolidate your competences, enable you to play more freely with data and explore new features and statistical functions in R.

With this course, you’ll have a first overview on Strategic Business Analytics topics. We’ll discuss a wide variety of applications of Business Analytics. From Marketing to Supply Chain or Credit Scoring and HR Analytics, etc. We’ll cover many different data analytics techniques, each time explaining how to be relevant for your business.

We’ll pay special attention to how you can produce convincing, actionable, and efficient insights. We'll also present you with different data analytics tools to be applied to different types of issues.
By doing so, we’ll help you develop four sets of skills needed to leverage value from data: Analytics, IT, Business and Communication. 

By the end of this MOOC, you should be able to approach a business issue using Analytics by (1) qualifying the issue at hand in quantitative terms, (2) conducting relevant data analyses, and (3) presenting your conclusions and recommendations in a business-oriented, actionable and efficient way.

Prerequisites : 1/ Be able to use R or to program 2/ To know the fundamentals of databases, data analysis (regression, classification, clustering)

We give credit to Pauline Glikman, Albane Gaubert, Elias Abou Khalil-Lanvin (Students at ESSEC BUSINESS SCHOOL) for their contribution to this course design.
      


            Read more
          



          Introduction to Strategic Business Analytics
    - In this module, we will introduce you to the course and instructional approach. You will learn that Strategic Business Analytics relies on four distinct skills: IT, Analytics, Business and Communication.

Finding groups within Data
    -In this module, you will learn how identifying groups of observations enables you to improve business efficiency. You will then learn to create those groups in a business-oriented and actionable way. We will use examples to illustrate various concepts. The assessments will also provide you with opportunities to replicate these examples.

Factors leading to events
    -In this module, you will learn why using rigorous statistical methods to understand the relationship between different events is crucial. 

We’ll cover two examples: first, using a credit scoring example, you will learn how to derive information about what makes an individual more or less likely to have a strong credit score? Then, in a second example drawn from HR Analytics, you will learn to estimate what makes an employee more or less likely to leave the company. 
As usual, we invite you to replicate those examples thanks to the recital 
and to use the assessments provided at the end of the module to strengthen your understanding of these concepts.

Predictions and Forecasting
    - In this module you will learn more about the importance of forecasting the future.

You will learn through examples from various sectors: first, using the previous examples of credit scoring and HR Analytics, you will learn to predict what will happen. Then, you will be introduced to predictive maintenance using survival analysis via a case discussion. Finally, we’ll discuss seasonality in the context of the first example discussed in this MOOC: using analytics for managing your supply chain and logistics better. 


Recommendation production and prioritization
    -So far, you’ve learnt to use Business Analytics to glean important information relevant to the success of your business. In this module, you’ll learn more about how to present your Business Analytics work to a business audience. This module is also important for your final capstone project presentation.You’ll learn that it is important to find an angle, and tell a story.Instead of presenting a list of results that are not connected to each other, you will learn to take your audience by the hand and steer it to the recommendations you want to conclude on.You’ll learn to structure your story and your slides, and master the most used visualization tips and tricks. The assessment at the end of this module will provide an opportunity for you to practice these methods and to prepare the first step of the capstone project."
https://www.classcentral.com/course/fast-track-finance-1-6404,".
      


Chapter 1: Time Value of MoneyIn this chapter, we introduce the framework of time value of money (TVM) in a carefully structured way, using relatively simple applications.   Chapter 2: Time Value of Money - ApplicationsIn this chapter, we build on Chapter 1 and move quickly to more advanced applications of TVM focusing on situations we confront on a daily basis.Chapter 3: Decision CriteriaIn this chapter, we develop an understanding of alternative decision criteria - Net Present Value (NPV), Internal Rate of Return (IRR), Profitability, etc. - that are commonly used in the real world, with a focus on a critical evaluation of their strengths and weaknesses.Chapter 4: Decision Making & Cash FlowsThis chapter contains a basic introduction to the key determinants of cash flows using accounting principles. Chapter 5: Bond ValuationThis chapter introduces you to the most common vehicle of investment/financing: bonds.  Though introductory, we attempt to cover a fair amount of content and with real world applications.Chapter 6: Stock ValuationThis chapter provides an exposure to the characteristics and the valuation of stocks, again with an eye toward real world applications.Chapter 7: What is Risk?In this chapter, we will develop a theory of risk based on the intuitive concept of diversification. Since knowledge of Statistics is key to understanding risk, we will introduce some important concepts of applications.Chapter 8: Diversification and CAPMIn this chapter, we will introduce one of the most widely known models called CAPM (Capital Asset Pricing Model), which provides a simple and powerful relation between risk and return.Chapter 9: Leverage, WACC & ValuationThis Chapter attempts to put together a cash flows and risk and return in a world with leverage financing. The content is our first exposure to valuation with a full-blown application."
https://www.classcentral.com/course/discrete-math-and-analyzing-social-graphs-17336,"The main goal of this course is to introduce topics in Discrete Mathematics relevant to Data Analysis.

We will start with a brief introduction to combinatorics, the branch of mathematics that studies how to count. Basics of this topic are critical for anyone working in Data Analysis or Computer Science. We will illustrate new knowledge, for example, by counting the number of features in data or by estimating the time required for a Python program to run.

Next, we will apply our knowledge in combinatorics to study basic Probability Theory. Probability is everywhere in Data Analysis and we will study it in much more details later. Our goals for probability section in this course will be to give initial flavor of this field.

Finally, we will study the combinatorial structure that is the most relevant for Data Analysis, namely graphs. Graphs can be found everywhere around us and we will provide you with numerous examples. We will mainly concentrate in this course on the graphs of social networks. We will provide you with relevant notions from the graph theory, illustrate them on the graphs of social networks and will study their basic properties. In the end of the course we will have a project related to social network graphs.

As prerequisites we assume only basic math (e.g., we expect you to know what is a square or how to add fractions), basic programming in Python (functions, loops, recursion), common sense and curiosity. Our intended audience are all people that work or plan to work in Data Analysis, starting from motivated high school students.
      


            Read more
          



          Basic Combinatorics
    -Suppose we need to count certain objects. Can we do anything better than just list all the objects? Do we need to create a list of all our data entries to check whether we have enough data to teach our ML model? Is there a way to tell whether our algorithm will run in a reasonable time before implementing and actually running it? All these questions are addressed by a mathematical field called Combinatorics. In this module we will give an introduction to this field that will help us to answer basic versions of the above questions.

Advanced Combinatorics
    -In the first week we have already considered most of the standard settings in Combinatorics, that allow us to address many counting problems. However, successful application of this knowledge on practice requires considerable experience in this kind of problems. The goal of this module is twofold. First, we study extensively more advanced combinatorial settings. We discuss in more details binomial coefficients. Also, we address one more standard setting, combinations with repetitions. The second gaol of the course is to practice counting. We will gain some experience in this by discussing various problems in Combinatorics.

Discrete Probability
    -Probability theory is a mathematical foundation of Statistics, the core of Data Science. During this week we study discrete probability, the first chapter of the probability theory, closely related to combinatorics. We discuss random experiments, their outcomes and events, introduce the notion of probability and some basic rules that follow immediately from the combinatorial results studied before. We also study simple probabilistic models like coin-tossing that will be used later.

Introduction to Graphs
    -Graphs represent objects and relations between them in a compact geometric form. Objects are represented by vertices of a graph and relations correspond to edges. Applications of graphs include geoinformational systems (vertices are cities, edges are roads), social network analysis (people and friendship relations), chemistry (graphs of molecular structure), computer network topology, and many more. During this week, we introduce basic notions of graph theory and discuss basic algorithms on graphs.

Basic Graph Parameters
    -Graph parameters, also called graph properties and graph invariants, are values (usually numerical), which are calculated for a given graph and depend only on its abstract structure (not, say, on a particular way of drawing the graph on a plane). Graph parameters are useful in data science, since they reduce a big amount of data (the graph) to a small one (the parameter), while conveying important information about the graph. We discuss some of the basic graph parameters in this module.

Graphs of Social Networks
    -In this final part of the course we discuss a Python library for working with graphs, called NetworkX. In NetworkX, one can create and modify graphs, compute graph parameters, visualize graphs, etc. We shall show how NetworkX is used to operate on graphs coming from a real-world dataset."
https://www.classcentral.com/course/udacity-model-building-and-validation-3256,"This course will teach you how to start from scratch in answering questions about the real world using data. Machine learning happens to be a small part of this process. The model building process involves setting up ways of collecting data, understanding and paying attention to what is important in the data to answer the questions you are asking, finding a statistical, mathematical or a simulation model to gain understanding and make predictions. All of these things are equally important and model building is a crucial skill to acquire in every field of science. The process stays true to the scientific method, making what you learn through your models useful for gaining an understanding of whatever you are investigating as well as make predictions that hold true to test. We will take you on a journey through building various models. This process involves asking questions, gathering and manipulating data, building models, and ultimately testing and evaluating them.Why Take This Course?Many of you may have already taken a course in machine learning or data science or are familiar with machine learning models.In this course we will take a more general approach, walking through the questioning, modeling and validation steps of the model building process. The goal is to get you to practice thinking in depth about a problem and coming up with your own solutions. Many examples we will attempt may not have one correct answer but will require you to work through the problems applying the methods we hope to illustrate throughout this class.



            Read more
          



Lesson 1 - Introduction to the QMV ProcessLearn about the Question, Modeling, and Validation (QMV) process of data analysis. Understand the basics behind each step and apply the QMV process to analyze on how Udacity employees choose candies!Lesson 2 - Question PhaseWe will drill in on the questioning phase of the QMV process. We’ll teach you how to turn a vague question into a statistical one that can be analyzed with statistics and machine learning. You will also analyze a Twitter dataset and try to predict when a person will tweet next! Lesson 3 - Modeling PhaseBuilding upon lesson 2, you will learn how to build rigorous mathematical, statistical, and machine learning models so you can make accurate predictions. You look through the recently released U.S. medicare dataset for anomalous transactions.Lesson 4 - Validation PhaseSo how do you tell if your model is doing well? In this lesson, we will teach you some of the fundamental and important metrics that you can use to grade the performance of the models that you’ve build. You will analyze the AT&T connected cars data set and see if you can tell which driver is which by analyzing their driving patterns.Final Project - Identify Hacking Attempts from Network Flow LogsYou will create a program that examines log data of net flow traffic, and produces a score, from 1 to 10, describing the degree to which the logs suggest a brute force attack is taking place on a server."
https://www.classcentral.com/course/accounting-data-analytics-python-17297,"This course focuses on developing Python skills for assembling business data. It will cover some of the same material from Introduction to Accounting Data Analytics and Visualization, but in a more general purpose programming environment (Jupyter Notebook for Python), rather than in Excel and the Visual Basic Editor. These concepts are taught within the context of one or more accounting data domains (e.g., financial statement data from EDGAR, stock data, loan data, point-of-sale data).
The first half of the course picks up where Introduction to Accounting Data Analytics and Visualization left off: using in an integrated development environment to automate data analytic tasks. We discuss how to manage code and share results within Jupyter Notebook, a popular development environment for data analytic software like Python and R. We then review some fundamental programming skills, such as mathematical operators, functions, conditional statements and loops using Python software. 
The second half of the course focuses on assembling data for machine learning purposes.  We introduce students to Pandas dataframes and Numpy for structuring and manipulating data. We then analyze the data using visualizations and linear regression. Finally, we explain how to use Python for interacting with SQL data.
      


          INTRODUCTION TO THE COURSE
    -In this module, you will become familiar with the course, your instructor and your classmates, and our learning environment. This orientation module will also help you obtain the technical skills required to navigate and be successful in this course.

MODULE 1: FOUNDATIONS
    -This module serves as the introduction to the course content and the course Jupyter server, where you will run your analytics scripts. First, you will read about specific examples of how analytics is being employed by Accounting firms. Next, you will learn about the capabilities of the course Jupyter server, and how to create, edit, and run notebooks on the course server. After this, you will learn how to write Markdown formatted documents, which is an easy way to quickly write formatted text, including descriptive text inside a course notebook.

MODULE 2: INTRODUCTION TO PYTHON
    -This module focuses on the basic features in the Python programming language that underlie most data analytics programs (or scripts). First, you will read about why accounting students should learn to write computer programs. In the first lesson, you will also learn the basic concepts of the Python programming language, including how to create variables, basic data types and mathematical operators, and how to document your programs with comments. Next, you will learn about Boolean and logical operators in Python and how they can be used to control the flow of a Python program by using conditional statements. Finally, you will learn about functions and how they can simplify developing and maintaining programs. You will also learn how to create and call functions in Python.

MODULE 3: INTRODUCTION TO PYTHON PROGRAMMING
    -In this module you will learn about working with fundamental data structures in Python: strings, tuples, lists, and dictionaries. You will also learn about how to write loops for performing repetitive tasks.

MODULE 4: PYTHON PROGRAMMING
    -In this module you will learn about creating and using modules, which is a group of functions. You will then learn about two of the most important modules for data analytics: NumPy and Pandas. NumPy performs numerical calculations on large data arrays. Pandas simplifies procedures for working with panel data, also known as dataframes.

MODULE 5: DATA ANALYSIS WITH PYTHON
    -This module focuses on using the Pandas dataframe to do some fundamental dataframe tasks including saving and reading dataframes, pivot table functions, filtering functions, and calculating descriptive statistics.


MODULE 6: INTRODUCTION TO VISUALIZATION IN PYTHON
    -In this module you will learn some basic elements of creating data visualizations in Python. You will then learn how to use the Matplotlib and Seaborn modules to help create some of the most commonly used one- and two-dimensional data visualizations.

MODULE 7: PRODUCTION DATA ANALYTICS
    -In this module you'll learn about the CRISP decision making framework to approach real-world problems. You'll also learn how to use linear regression to find and quantify relationships.

MODULE 8: INTRODUCTION TO DATABASES IN PYTHON
    -This module focuses on relational database management systems (RDBMS) and how to interact with those using Python."
https://www.classcentral.com/course/edx-fa19-statistical-modeling-and-regression-analysis-8996,"Regression Analysis is the most common statistical modeling approach used in data analysis and it is the basis for more advanced statistical and machine learning modeling.In this course, you will be given fundamental grounding in the use of widely used tools in regression analysis. You will learn the basics of regression analysis such as linear regression, logistic regression, Poisson regression, generalized linear regression and model selection.Throughout this course, you will be exposed to not only fundamental concepts of regression analysis but also many data examples using the R statistical software. Thus by the end of this course, you will also be familiar with the implementation of regression models using the R statistical software along with interpretation for the results derived from such implementations.This course is more about the opportunity for individual discovery than it is about mastering a fixed set of techniques.



Weeks 1-2: Introduction to the most basic regression: Simple Linear Regression with data examplesWeeks 3-4: Introduction to the Analysis of Variance (ANOVA) Model with data examplesWeeks 5-8: Introduction to most popular regression model: Multiple Linear Regression with data examplesWeeks 9-11: Introduction to Logistic Regression and Poisson Regression within the more general regression approach, generalized linear model, with data examplesWeeks 12-14: Introduction to multiple approaches to variable selection illustrated with an extensive data analysis example"
https://www.classcentral.com/course/coursera-big-data-cloud-computing-cdn-emerging-technologies-3931,"This is a notice to inform you that the “Big Data, Cloud Computing, & CDN Emerging Technologies” course will close for new learner enrollment on September 17, 2018. Since you have already enrolled, you will continue to see it on your Coursera Dashboard as long as you remain enrolled in the course. If you are interested in earning a Course Certificate for this course, please upgrade or apply for Financial Aid by September 16, 2018, if you have not already done so. In order to earn a Course Certificate, you will need to complete all graded assignments, including peer reviews, by March 17, 2019. After that point, no new assignment submissions will be accepted for Certificate credit. The reason to close this course is because a new upgraded ""Emerging Technologies: From Smartphones to IoT to Big Data"" Specialization has been prepared and will be launched in a few weeks. This new upgraded Specialization includes 4 courses titled “Big Data Emerging Technologies,” “Smart Device & Mobile Emerging Technologies,” ""IoT (Internet of Things) Wireless & Cloud Computing Emerging Technologies,"" and ""AR (Augmented Reality) & Video Streaming Services Emerging Technologies."" These four courses (which all include projects) include the contents of the former courses, but with much more new state-of-the-art technologies added. Among these courses, I highly recommended you to take the “Big Data Emerging Technologies” course (because it covers details on Apache Hadoop, Spark, Storm, ML (Machine Learning) data analysis technology and IBM SPSS Statistics projects), the ""IoT (Internet of Things) Wireless & Cloud Computing Emerging Technologies"" course (because it covers details on Cloud Computing, MEC, Fog computing, Cloudlets, and AWS (Amazon Web Service) EC2 cloud projects), and the ""AR (Augmented Reality) & Video Streaming Services Emerging Technologies"" course (because it covers details on H.264/MPEG-4 AVC, MPEG-DASH, CDN video streaming technology). While we hope that you will be able to complete the course, you can find more information about requesting a refund (https://learner.coursera.help/hc/en-us/articles/209819043-Request-a-refund) or unenrolling from a course (https://learner.coursera.help/hc/en-us/articles/208279756-Unenroll-from-a-course) in our Learner Help Center. We sincerely thank you for your interest and contributions to this course. In addition, we sincerely hope you will be interested in the new courses as well.
      


            Read more
          



Cloud ComputingThese lectures focus on the major features and functionalities of Cloud Computing. The lectures start with first answering the question “What does Cloud Computing do?” and then provide Cloud Computing Application Examples, and then provide description on the Cloud Models IaaS (Infrastructure as a Service), PaaS (Platform as a Service), SaaS (Software as a Service), as well as VM (Virtual Machine). Next, the lecture covers Cloud Services based on examples of the Google Cloud, Amazon Cloud, and the iCloud.Big DataThese lectures focus on the features and architecture of Big Data. The lectures start with providing Big Data examples, which include the past event of the H1N1 flu virus spread prevention, smartphone marketing, and industrial examples of Big Data in use by Wal-Mart, Amazon.com, and Citibank. To explain the major technical challenges of Big Data, the influence of data Volume, Variety, Velocity, and Veracity are discussed. Then, to provide a realistic overview of a reliable shared storage Big Data analysis system, the architecture of Hadoop is described. The description includes details on MapReduce and HDFS (Hadoop Distributed FileSystem), which are the two major components of Hadoop.CDN (Content Delivery Network)Everybody using smartphones, PCs, and the Internet have already been using CDN (Content Delivery Network) services, and will continue to use CDN services for their entire life. Obviously, CDN technology is very important. These lectures focus on the market, services, features, operations, and architecture of CDN technology. The lectures start with CDN Motivation and Structure, then the lecture provides details on CDN Procedures and Hierarchical Content Delivery Models. Next, the CDN Market and Major Service Providers are introduced followed by CDN Research & Development topics."
https://www.classcentral.com/course/investments-fundamentals-6754,"In this course, we will discuss fundamental principles of trading off risk and return, portfolio optimization, and security pricing. We will study and use risk-return models such as the Capital Asset Pricing Model (CAPM) and multi-factor models to evaluate the performance of various securities and portfolios. Specifically, we will learn how to interpret and estimate regressions that provide us with both a benchmark to use for a security given its risk (determined by its beta), as well as a risk-adjusted measure of the security’s performance (measured by its alpha). Building upon this framework, market efficiency and its implications for patterns in stock returns and the asset-management industry will be discussed. Finally, the course will conclude by connecting investment finance with corporate finance by examining firm valuation techniques such as the use of market multiples and discounted cash flow analysis. The course emphasizes real-world examples and applications in Excel throughout. This course is the first of two on Investments that I am offering online (“Investments II: Lessons and Applications for Investors” is the second course).

The over-arching goals of this course are to build an understanding of the fundamentals of investment finance and provide an ability to implement key asset-pricing models and firm-valuation techniques in real-world situations. Specifically, upon successful completion of this course, you will be able to:
•	Explain the tradeoffs between risk and return
•	Form a portfolio of securities and calculate the expected return and standard deviation of that portfolio
•	Understand the real-world implications of the Separation Theorem of investments
•	Use the Capital Asset Pricing Model (CAPM) and 3-Factor Model to evaluate the performance of an asset (like stocks) through regression analysis
•	Estimate and interpret the ALPHA (α) and BETA (β) of a security, two statistics commonly reported on financial websites
•	Describe what is meant by market efficiency and what it implies for patterns in stock returns and for the asset-management industry
•	Understand market multiples and income approaches to valuing a firm and its stock, as well as the sensitivity of each approach to assumptions made
•	Conduct specific examples of a market multiples valuation and a discounted cash flow valuation

This course was previously entitled “Financial Evaluation and Strategy: Investments” and was part of a previous specialization entitled ""Improving Business and Finances Operations"", which is now closed to new learner enrollment. “Financial Evaluation and Strategy: Investments” received an average rating of 4.8 out of 5 based on 199 reviews over the period August 2015 through August 2016. You can view a detailed summary of the ratings and reviews for this course in the Course Overview section. 

This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.
      


            Read more
          



          Course Overview 
    -In this module, you will become familiar with the course, your instructor, your classmates, and our learning environment. The orientation also helps you obtain the technical skills required for the course.

Module 1: Investments Toolkit and Portfolio Formation
    -In Module 1, we will build the fundamentals of portfolio formation.  After providing a brief refresher of basic investment concepts (our toolkit), a summary of historical patterns of stock returns and government securities in the U.S. is provided.  We then consider general examples of portfolio choice to highlight the tradeoffs between “risk” and return.  We end the module with a discussion of dominated assets and efficient portfolio formation, emphasizing real-world examples and practice in Excel solving for the optimal portfolio given certain constraints (such as the amount of volatility we will accept in our portfolio). 

Module 2: Motivating, Explaining, & Implementing the Capital Asset Pricing Model (CAPM)
    -In Module 2, we will develop the financial intuition that led to the Capital Asset Pricing Model (CAPM), starting with the Separation Theorem of Investments.  We will understand that in a CAPM setting, only the market-wide risk of an asset is priced – securities with greater sensitivity to the market are required by investors to yield higher returns on average.  We will also learn how to interpret regressions that provide us with both a benchmark to use for a security given its risk (determined by its beta), as well as a risk-adjusted measure of the security’s performance (measured by its alpha).

Module 3: Testing the CAPM, Multifactor Models, & Market Efficiency
    -In Module 3, we will discuss different asset-pricing models, the pros and cons of each, and market efficiency.  In particular, we will test the effectiveness of the Capital Asset Pricing Model (CAPM) and examine survey data concerning its use by chief financial officers (CFOs) of firms.  Predictable patterns in stock returns, such as the size and value effects, will also be examined and the Fama-French 3-Factor Model will be introduced.  Market efficiency will be discussed in this module, as well as its implications for the asset-management industry and observed patterns in stock returns.

Module 4: Investment Finance and Corporate Finance: Firm Valuation
    -In Module 4, we will learn about the two key approaches to valuing a company or stock: market multiples and discounted cash flow.  We will learn how to value perpetuities and will discuss how caution should be exercised in terms of projecting both the growth in long-term cash flows and the riskiness of those cash flows – two key components of the perpetuity formula.  Finally, to gain experience with the market multiples approach, we will estimate a value of Google at the time of its initial public offering (IPO) back in 2004 using market data on Yahoo! as a comparable firm.

Course Conclusion
    -In this module, we say goodbye to the Investments course as key takeaways from the course are reviewed. A tease is also provided to topics that will be covered in Professor Weisbenner's second course on Investments."
https://www.classcentral.com/course/big-data-machine-learning-5421,"##
Many people have big data but only some people know what to do with it. Why? Well, the big problem is that the data is big—the size, complexity and diversity of datasets increases every day. This means we need new solutions for analysing data.
This course equips you for working with these solutions by introducing you to selected statistical and machine learning techniques used for analysing large datasets and extracting information. We also expose you to three software packages so you can develop your coding skills by completing practical exercises.
You will enjoy this course most and benefit from the learning experience if you have a basic understanding of statistics and mathematics at a university undergraduate level.
You will be using the following free tools. Please review the product websites below to ensure your system meets the minimum requirements:
R and R Studio Desktop (open source edition)
You will complete practical exercises using R Studio, so you’ll need to be familiar enough with R to:

install a package
import data
read and run starter code
develop a solution or read through a solution and gain understanding from it.

NOTE: You must first have a working installation of R to use R Studio.
H2O Flow
H2O Flow can be used as a stand-alone package for big data analytics or can be used in conjunction with R. This package will allow you to tackle larger problems that you might encounter in your own work.
WEKA
WEKA is a popular workbench for machine learning and statistical analysis. It comprises a very wide range of tools that are suitable for big data analysis.
Knowing R, H2O Flow and WEKA will give you a powerful, flexible and scalable set of tools to manipulate and analyse big data.



            Read more"
https://www.classcentral.com/course/datan-1386,"The course was created with the support of Sberbank 
This is an
unconventional course in modern Data Analysis, Machine Learning and Data
Mining. Its contents are heavily influenced by the idea that data analysis
should help in enhancing and augmenting knowledge of the domain as represented
by the concepts and statements of relation between them. According to this
view, two main pathways for data analysis are summarization, for developing and
augmenting concepts, and correlation, for enhancing and establishing relations.
The term summarization embraces here both simple summaries like totals and
means and more complex summaries: the principal
components of a set of features and cluster structures in a set of entities.
Similarly, correlation covers both bivariate and multivariate relations between
input and target features including Bayes classifiers.

The view of the data as a subject of
computational data analysis that is adhered to here has emerged quite recently.
Typically, in sciences and in statistics, a problem comes first, and then the
investigator turns to data that might be useful in advancing towards a
solution. Yet nowadays the situation is reversed frequently, especially with
the advent of Big Data. Typical questions then are: Take a look at this data
set - what sense can be made out of it? – Is there any structure in the data
set? Can these features help in predicting those? This is more reminiscent to a
traveler’s view of the world rather than that of a scientist. The scientist
sits at his desk, gets reproducible signals from the universe and tries to accommodate
them into a great model of the universe. The traveler deals with what come on
their way – here is the data analysis niche. 
A textbook by the instructor along these lines has been published by
Springer-London in 2011: “Core
concepts in data analysis is clean and devoid
of any fuzziness. The author presents his theses with a refreshing clarity
seldom seen in a text of this sophistication. … To single out just one of the
text’s many successes: I doubt readers will ever encounter again such a detailed
and excellent treatment of correlation concepts. (Computing Reviews of
ACM, June 2011).”



            Read more
          



Week 1. Intro: Examples of data and data analysis problems;
  visualization.                            
Week 2. 1D analysis. Feature scales.
  Histogram. Two common types of histograms: Gaussian and Power Law. Central
  values. Minkowski distance and data recovery view. Validation with Bootstrap.
                                    
Week 3-4. 2D analysis cases: 
(Both
  quantitative: Scatter-plot, linear regression, correlation and determinacy
  coefficients: meaning and properties.  Both
  nominal: Contingency table, Quetelet index, Pearson chi-squared coefficient,
  its double meaning and visualization).                                                               
Week 5-6. Learning multivariate correlations
(Bayes
  approach and Naïve Bayes classifier with a Bag-of-words text model; Decision
  trees and criteria for building them.)                                              
Week 7. Principal components (PCA) and SVD
(SVD model behind
  PCA: student marks as the product of subject factor scores and subject loadings.
  Application to deriving a hidden underlying factor. Data visualization with
  PCA. Conventional PCA and data normalization issues.)
 
Week 8. Clustering with k-means
(K-Means
  iterations and K-Means features    
K-Means
  criterion. Anomalous clusters and intelligent K-Means.)"
https://www.classcentral.com/course/molecularevolution-3555,"In the previous course in the Specialization, we learned how to compare genes, proteins, and genomes.  One way we can use these methods is in order to construct a ""Tree of Life"" showing how a large collection of related organisms have evolved over time.

In the first half of the course, we will discuss approaches for evolutionary tree construction that have been the subject of some of the most cited scientific papers of all time, and show how they can resolve quandaries from finding the origin of a deadly virus to locating the birthplace of modern humans.

In the second half of the course, we will shift gears and examine the old claim that birds evolved from dinosaurs.  How can we prove this?  In particular, we will examine a result that claimed that peptides harvested from a T. rex fossil closely matched peptides found in chickens. In particular, we will use methods from computational proteomics to ask how we could assess whether this result is valid or due to some form of contamination.

Finally, you will learn how to apply popular bioinformatics software tools to reconstruct an evolutionary tree of ebolaviruses and identify the source of the recent Ebola epidemic that caused global headlines.
      


          Week 1: Introduction to Evolutionary Tree Construction
    -Welcome to our class!In this class, we will consider the following two central biological questions (the computational approaches needed to solve them are shown in parentheses):Weeks 1-3: Which Animal Gave Us SARS? (Evolutionary tree construction)Weeks 4-5: Was T. rex Just a Big Chicken? (Combinatorial Algorithms)In Week 6, you will complete a Bioinformatics Application Challenge to apply evolutionary tree construction algorithms in order to determine the origin of the recent ebola outbreak in Africa.As in previous courses, each of these two chapters is accompanied by a Bioinformatics Cartoon created by talented artist Randall Christopher and serving as a chapter header in the Specialization's bestselling print companion. You can find the first chapter's cartoon at the bottom of this message. What do stick bugs and bats have to do with deadly viruses? And how can bioinformatics be used to stop these viruses in their tracks? Start learning today and find out!

Week 2: More Algorithms for Constructing Trees from Distance Matrices
    -Welcome to Week 2 of class!

Last week, we started to see how evolutionary trees can be constructed from distance matrices.  This week, we will encounter additional algorithms for this purpose, including the neighbor-joining algorithm, which has become one of the top-ten most cited papers in all of science since its introduction three decades ago.

Week 3: Constructing Evolutionary Trees from Characters
    -Welcome to week 3 of class!

Over the last two weeks, we have seen several different algorithms for constructing evolutionary trees from distance matrices.

This week, we will conclude the current chapter by considering what happens if we use properties called ""characters"" instead of distances. We will also see how to infer the ancestral states of organisms in an evolutionary tree, and consider whether it is possible to define an efficient algorithm for this task.

Week 4
    -Welcome to week 4 of the class!

Did birds evolve from dinosaurs? Over the next two weeks, we will see how we could analyze molecular evidence in support of this theory. You can find this week's Bioinformatics Cartoon from Randall Christopher at the bottom of this E-mail. Why does the T. rex look so much like a chicken? And why is the monkey typing frantically? Keep learning to find out!



Week 5: Resolving the T. rex Peptides Mystery? 
    -Welcome to week 5 of class!

Last week, we asked whether it is possible for dinosaur peptides to survive locked inside of a fossil for 65 million years. This week, we will see what this question has to do with statistics; in the process, we will see how a monkey typing out symbols on a typewriter can be used to address it.

Week 6: Bioinformatics Application Challenge
    -Welcome to the sixth and final week of the course!

In this week's Bioinformatics Application Challenge, we will use reconstruct an evolutionary tree of ebolaviruses and use it to determine the origin of the pathogen that caused the recent outbreak in Africa."
https://www.classcentral.com/course/futurelearn-learn-to-code-for-data-analysis-3997,"Learn to code in Python and analyse real, open data
This hands-on course will teach you how to write your own computer programs, one line of code at a time. You’ll learn how to access open data, clean it and analyse it, and produce visualisations. You will also learn how to write up and share your analyses, privately or publicly.
You will install free software to learn to code in Python, a widely used programming language. You will write up analyses and do coding exercises using the popular Jupyter Notebook platform. And you will look at real data from the World Health Organisation, the World Bank and other organisations.
The course does not assume prior experience in programming or data analysis. Basic familiarity with a spreadsheet application will be an advantage.
The course does not require any knowledge of statistics, but you need to have basic numeracy skills, like writing arithmetic expressions, using percentages and understanding scientific notation. If you wish to brush up on your numeracy skills, we recommend the FutureLearn course Basic Science: Understanding Numbers from The Open University.
To study this course you will use specialist software. You can use the software online, via a free account on a website, or offline, by downloading and installing a free software package. You will receive instructions about both options via email before the course starts. The online solution requires a good internet connection and has some limitations.
The offline software has no limitations and is the recommended option. However, you will need access to a desktop or laptop computer on which you can install software. The software is free and there are versions available for Windows, Mac and Linux platforms. You will need about 3 GB of free disk space to download and install the software, and to store datasets that will be provided in the course.
Whether you choose the online or offline software option, you will need to be proficient in basic computer tasks, like creating folders, downloading files and copying them to specific folders, etc. In terms of accessibility, you will be asked to use your web browser and to type code.



            Read more"
https://www.classcentral.com/course/business-statistics-2-5245,"In this course, you will learn the language of uncertainty and the statistical methods for making inferences and decisions on the basis of limited information and data. The course will build on concepts covered in the first part and move to prediction and inferential statistics.  In particular, you will learn about hypothesis testing, testing for independent and paired samples, simple linear regression, and multiple linear regression enabling you to make informed predictions.

You will:
•	Understand to perform hypothesis testing and draw conclusions and communicate the results
•	Understand how to compare two population to test whether a significant difference exists between the population
•	Understand the concept of correlation and build linear regression models and make predictions."
https://www.classcentral.com/course/global-statistics-11053,"The number of composite indices that are constructed and used internationally is growing very fast; but whilst the complexity of quantitative techniques has increased dramatically, the education and training in this area has been dragging and lagging behind. As a consequence, these simple numbers, expected to synthesize quite complex issues, are often presented to the public and used in the political debate without proper emphasis on their intrinsic limitations and correct interpretations. 

In this course on global statistics, offered by the University of Geneva jointly with the ETH Zürich KOF, you will learn the general approach of constructing composite indices and some of resulting problems. We will discuss the technical properties, the internal structure (like aggregation, weighting, stability of time series), the primary data used and the variable selection methods.  These concepts will be illustrated using a sample of the most popular composite indices. We will try to address not only statistical questions but also focus on the distinction between policy-, media- and paradigm-driven indicators.
      


          Welcome module
    -Welcome to the first module of this course. In this welcome module, you will be introduced with the Professors that will take part in this course on composites indices. We explain the rationale for composite indices (CIs) and show how they can be of interest. This course is open to NGO members, politicians, journalists, students and all persons interested in understanding, creating and/or interpreting CIs. By the end of this first module, you will have an overview of the content of the course week by week. 

Some introductory issues
    -This module contains four lessons. The first lesson is an introduction to CIs. It defines what a CI is, introduces their mathematical notation and reviews some core historical aspects of their development,   the need and use of CIs. The second lesson focuses on the demand for CIs while the third lesson develops a qualitative framework for the construction of CIs. More specifically, the intrinsic quality of CIs is discussed by reviewing their pros and cons. Finally, the last lesson of this introductory module sketches the steps involved in the construction of a CI. 
Learning outcomes: by the end of this module you will have a clear idea what a CI is (definition, ingredients, history, objective), know why it is needed and where it is used (needs and demand), be familiar with the quality requirements and have a first idea steps involved in the construction of a CI.


The steps of constructing a composite index
    -This module is organized along four lessons. The objective of this module is to familiarize you with the key steps to undertake when constructing a CI. The first lesson will develop a theoretical framework to support CIs’ construction. Notably, it will cover topics such as variables selection and data issues. The second lesson will introduce a unifying approach to construct CI by discussing aspects related to transformation functions and the elasticity of substitution. The entire third lesson will be devoted to an essential aspect in the construction of a CI:  the choice of weights. Finally, the module will conclude by addressing questions arising after the construction of a CI. For instance, lesson four will discuss how to assess the robustness of the resulting CI. By the end of this module you will be familiar with all the most important technical (or say statistical) steps involved in constructing CIs. 

Globalization and Youth labour market indices (ETH Zurich/KOF)
    -In this module, you will discover two popular indices developed by ETH Zurich: the Young Labour Market Index and the KOF Globalization index.
In the first lesson of this week, you will learn more about the Youth Labour Market Index (YLMI). The KOF YLMI captures various aspects of the youth labour market situation of countries across the world. You will learn which indicators are included in the KOF YLMI and how these are aggregated into a single index. Furthermore, you will get to know an online tool that invites you to analyse the youth labour market situation yourself.
In the second lesson of this module, you will learn about the KOF Globalization Index which is a widely used composite indicator that measures the degree of globalization for every country in the world since 1970. It distinguishes between three dimensions of globalization: Economic, social and political globalization. In the following module, you will learn why it is important to measure globalization and what the different stages in constructing the KOF Globalization Index are. A critical discussion of the Index sums up the module.


Export Potential Assessment (ITC)
    -This module focuses on trade indices developed by the International Trade Centre, the Export Potential Index (EDI) and the Product Diversification index (PDI).
Frictions often create a gap between what a country could export and what it does export to markets around the world. Trade advisers could better address these frictions and help firms realize greater exports if they knew exactly which products and markets offer best chances. During this week, you will learn about the Export Potential Assessment (EPI and PDI), which indicates products, sectors and markets for trade development activities for over 200 countries and 4,000 products. Based upon an assessment of the exporting country’s supply capacity, the target market’s demand and tariff conditions as well as the bilateral links between the exporting country and the target market, it provides a ranking of untapped opportunities.


Liner shipping connectivity indices (UNCTAD) and Human development index (UNDP)
    -During this week you will be exploring two indices. The first index, the Liner Shipping (Bilateral) Connectivity Index (LSCI/LSBCI) computed each year by UNCTAD since 2004. It provides an overall indicator of a country maritime connectivity related to liner shipping. Throughout this lesson, we give some insights on why the LSCI and LSBCI were developed. We also cover the methodology to build both indices. We then discuss some stylized facts.
The second index presented this week is the Human Development Index (HDI) developed by UNDP. During this lesson, you will be slightly introduced with the history of the HDI. We explain the steps of constructing the HDI, i.e. choosing the three dimensions (health, education and living conditions) composing the HDI and their respective indicators, normalizing the indicators and aggregating the indicators and dimensional sub-indices using different methods. Then, we use a practical example to calculate the HDI for one country. At the end, we discuss some limitations of the HDI and give some elements for future improvement."
https://www.classcentral.com/course/excel-data-analysis-7004,"The use of Excel is widespread in the industry. It is a very powerful data analysis tool and almost all big and small businesses use Excel in their day to day functioning. This is an introductory course in the use of Excel and is designed to give you a working knowledge of Excel with the aim of getting to use it for more advance topics in Business Statistics later. The course is designed keeping in mind two kinds of learners -  those who have very little functional knowledge of Excel and those who use Excel regularly but at a peripheral level and wish to enhance their skills. The course takes you from basic operations such as reading data into excel using various data formats, organizing and manipulating data, to some of the more advanced functionality of Excel. All along, Excel functionality is introduced using easy to understand examples which are demonstrated in a way that learners can become comfortable in understanding and applying them.

To successfully complete course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. 
________________________________________
WEEK 1
Module 1: Introduction to Spreadsheets
In this module, you will be introduced to the use of Excel spreadsheets and various basic data functions of Excel.

Topics covered include:
•	Reading data into Excel using various formats
•	Basic functions in Excel, arithmetic as well as various logical functions
•	Formatting rows and columns
•	Using formulas in Excel and their copy and paste using absolute and relative referencing
________________________________________
WEEK 2
Module 2: Spreadsheet Functions to Organize Data
This module introduces various Excel functions to organize and query data. Learners are introduced to the IF, nested IF, VLOOKUP and the HLOOKUP functions of Excel. 

Topics covered include:
•	IF and the nested IF functions
•	VLOOKUP and HLOOKUP
•	The RANDBETWEEN function
________________________________________
WEEK 3
Module 3: Introduction to Filtering, Pivot Tables, and Charts
This module introduces various data filtering capabilities of Excel. You’ll learn how to set filters in data to selectively access data. A very powerful data summarizing tool, the Pivot Table, is also explained and we begin to introduce the charting feature of Excel.

Topics covered include:
•	VLOOKUP across worksheets
•	Data filtering in Excel
•	Use of Pivot tables with categorical as well as numerical data
•	Introduction to the charting capability of Excel
________________________________________
WEEK 4
Module 4: Advanced Graphing and Charting
This module explores various advanced graphing and charting techniques available in Excel. Starting with various line, bar and pie charts we introduce pivot charts, scatter plots and histograms. You will get to understand these various charts and get to build them on your own.

Topics covered include
•	Line, Bar and Pie charts
•	Pivot charts
•	Scatter plots
•	Histograms
      


            Read more
          



          Introduction to Spreadsheets
    -Introduction to spreadsheets, reading data, manipulating data. Basic spreadsheet operations and functions.

Spreadsheet Functions to Organize Data
    -Introduction to some more useful functions such as the IF, nested IF, VLOOKUP and HLOOKUP functions in Excel.

Introduction to Filtering, Pivot Tables, and Charts
    -Introduction to the Data filtering capabilities of Excel, the construction of Pivot Tables to organize data and introduction to charts in Excel.

Advanced Graphing and Charting
    -Constructing various Line, Bar and Pie charts. Using the Pivot chart features of Excel. Understanding and constructing Histograms and Scatterplots."
https://www.classcentral.com/course/edx-principles-statistical-and-computational-tools-for-reproducible-data-science-9489,"Today the principles and techniques of reproducible research are more important than ever, across diverse disciplines from astrophysics to political science. No one wants to do research that can’t be reproduced. Thus, this course is really for anyone who is doing any data intensive research. While many of us come from a biomedical background, this course is for a broad audience of data scientists. 
To meet the needs of the scientific community, this course will examine the fundamentals of methods and tools for reproducible research. Led by experienced faculty from the Harvard T.H. Chan School of Public Health, you will participate in six modules that will include several case studies that illustrate the significant impact of reproducible research methods on scientific discovery. 
This course will appeal to students and professionals in biostatistics, computational biology, bioinformatics, and data science. The course content will blend video lectures, case studies, peer-to-peer engagements and use of computational tools and platforms (such as R/RStudio, and Git/Github), culminating in a final presentation of a final reproducible research project. 
We’ll cover Fundamentals of Reproducible Science; Case Studies; Data Provenance; Statistical Methods for Reproducible Science; Computational Tools for Reproducible Science; and Reproducible Reporting Science. These concepts are intended to translate to fields throughout the data sciences: physical and life sciences, applied mathematics and statistics, and computing. 
Consider this course a survey of best practices: we’d like to make you aware of pitfalls in reproducible data science, some failure - and success - stories in the past, and tools and design patterns that might help make it all easier. But ultimately it’ll be up to you to take the skills you learn from this course to create your own environment in which you can easily carry out reproducible research, and to encourage and integrate with similar environments for your collaborators and colleagues. We look forward to seeing you in this course and the research you do in the future!



            Read more
          



Module 1: Introduction to Course Overview Introduction to faculty Project assignment Module 2: Fundamentals of Reproducible Science Why reproducible research matters Definitions and concepts Factors affecting reproducibility Module 3: Case Studies in Reproducible Research Potti 2006 Baggerly and Coombes 2007 Ioannidis 2009 Reproducible Reporting Module 4: Data Provenance Project design Journal requirements and mechanisms Repositories Privacy and security Module 5: Statistical Methods for Reproducible Science Prediction Models Coefficient of determination Brier score AUC Concordance in survival analysis Cross validation Bootstrap Module 6: Computational Tools for Reproducible Science R and Rstudio Python Git and GitHub Creating a repository Data sources Dynamic report generation Workflows Course Conclusion Final Project: Write a reproducible report that could be submitted at a peer review journal"
https://www.classcentral.com/course/mitx-statistics-and-data-science-18342,"Demand for professionals skilled in data, analytics, and machine learning is exploding. A recent report by IBM and Burning Glass states that there will be 364K new job openings in data-driven professions by 2020 in the US. Data scientists bring value to organizations across industries because they are able to solve complex challenges with data and drive important decision-making processes. 39% of the most rigorous data science positions require a degree higher than a bachelor’s.
This MicroMasters program in Statistics and Data Science is comprised of four online courses and a virtually proctored exam that will provide you with the foundational knowledge essential to understanding the methods and tools used in data science, and hands-on training in data analysis and machine learning. You will dive into the fundamentals of probability and statistics, as well as learn, implement, and experiment with data analysis techniques and machine learning algorithms. This program will prepare you to become an informed and effective practitioner of data science who adds value to an organization. The program certificate can be applied, for admitted students, towards a PhD in Social and Engineering Systems (SES) through the MIT Institute for Data, Systems, and Society (IDSS) or may accelerate your path towards a Master’s degree at other universities around the world.
Anyone can enroll in this MicroMasters program. It is designed for learners that want to acquire sophisticated and rigorous training in data science without leaving their day job but without compromising quality. There is no application process but college-level calculus and comfort with mathematical reasoning and Python programming are highly recommended if you want to excel. All the courses are taught by MIT faculty at a similar pace and level of rigor as an on-campus course at MIT. This program brings MIT’s rigorous, high-quality curricula and hands-on learning approach to learners around the world – at scale.
For more detail on this program and credit pathways, please visit https://micromasters.mit.edu/ds/



            Read more
          



Courses under this program:Course 1: Probability - The Science of Uncertainty and Data
Build foundational knowledge of data science with this introduction to probabilistic models, including random processes and the basic elements of statistical inference -- Course 1 of 4 in the MITx MicroMasters program in Statistics and Data Science.
Course 2: Data Analysis in Social Science—Assessing Your KnowledgeLearn the methods for harnessing and analyzing data to answer questions of cultural, social, economic, and policy interest, and then assess that knowledge— Course 2 of 4 in the MITx MicroMasters program in Statistics and Data Science.Course 3: Fundamentals of Statistics
Develop a deep understanding of the principles that underpin statistical inference: estimation, hypothesis testing and prediction. -- Course 3 of 4 in the MITx MicroMasters program in Statistics and Data Science.
Course 4: Machine Learning with Python: from Linear Models to Deep Learning
An in-depth introduction to the field of machine learning, from linear models to deep learning and reinforcement learning, through hands-on Python projects. -- Course 4 of 4 in the MITx MicroMasters program in Statistics and Data Science.
Course 5: Capstone Exam in Statistics and Data Science
Solidify and demonstrate your knowledge and abilities in probability, data analysis, statistics, and machine learning in this culminating assessment. -- Final Requirement of the MITx MicroMasters Program in Statistics and Data Science."
https://www.classcentral.com/course/edx-data-analysis-in-social-science-assessing-your-knowledge-11481,"To learn more about this MicroMasters program, please visit https://micromasters.mit.edu/ds/.This course consists of an assessment that tests your knowledge on the course content from 14.310x - Data Analysis for Social Scientists, a statistics and data analysis course that will introduce you to the essential notions of probability and statistics. It will cover techniques in modern data analysis: estimation, regression and econometrics, prediction, experimental design, randomized control trials (and A/B testing), machine learning, and data visualization. It will illustrate these concepts with applications drawn from real world examples and frontier research. Finally, it will provide instruction for how to use the statistical package R and opportunities for students to perform self-directed empirical analyses.This assessment course should only be taken by learners who have completed and passed 14.310x - Data Analysis for Social Scientists and intend to pursue the MicroMasters credential in Statistics and Data Science.  To get credit in this MicroMasters program:

Enroll in both this assessment course and the content course 14.310x - Data Analysis for Social Scientists (Note: There is no additional fee to enroll in the content course),
Complete the content course 14.310x with a passing grade,
Come back to this course and take the exam to earn your verified certificate that will count toward the MicroMasters credential in Statistics and Data Science.

The timed exam will open between November 26, 2019 - December 9, 2019, but enrollment is open until November 12.This assessment course, along with the content course 14.310x - Data Analysis for Social Scientists,  is part of the MITx MicroMasters Program in Statistics and Data Science. Master the skills needed to be an informed and effective practitioner of data science. You will complete this course and three others from MITx, at a similar pace and level of rigor as an on-campus course at MIT, and then take a virtually-proctored exam to earn your MicroMasters, an academic credential that will demonstrate your proficiency in data science or accelerate your path towards an MIT PhD or a Master's at other universities. To learn more about this program, please visit https://micromasters.mit.edu/ds/.



            Read more"
https://www.classcentral.com/course/business-statistics-analysis-18816,"The Business Statistics and Analysis Specialization is designed to equip you with a basic understanding of business data analysis tools and techniques. You’ll master essential spreadsheet functions, build descriptive business data measures, and develop your aptitude for data modeling. You’ll also explore basic probability concepts, including measuring and modeling uncertainty, and you’ll use various data distributions, along with the Linear Regression Model, to analyze and inform business decisions. The Specialization culminates with a Capstone Project in which you’ll apply the skills and knowledge you’ve gained to an actual business problem. To successfully complete all course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. To see an overview video for this Specialization, click here!



          Course 1: Introduction to Data Analysis Using Excel- The use of Excel is widespread in the industry. It is a very powerful data analysis tool and almost all big and small businesses use Excel in their day to day functioning. This is an introductory course in the use of Excel and is designed to give you a working knowledge of Excel with the aim of getting to use it for more advance topics in Business Statistics later. The course is designed keeping in mind two kinds of learners - those who have very little functional knowledge of Excel and those who use Excel regularly but at a peripheral level and wish to enhance their skills. The course takes you from basic operations such as reading data into excel using various data formats, organizing and manipulating data, to some of the more advanced functionality of Excel. All along, Excel functionality is introduced using easy to understand examples which are demonstrated in a way that learners can become comfortable in understanding and applying them. To successfully complete course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. ________________________________________ WEEK 1 Module 1: Introduction to Spreadsheets In this module, you will be introduced to the use of Excel spreadsheets and various basic data functions of Excel. Topics covered include: • Reading data into Excel using various formats • Basic functions in Excel, arithmetic as well as various logical functions • Formatting rows and columns • Using formulas in Excel and their copy and paste using absolute and relative referencing ________________________________________ WEEK 2 Module 2: Spreadsheet Functions to Organize Data This module introduces various Excel functions to organize and query data. Learners are introduced to the IF, nested IF, VLOOKUP and the HLOOKUP functions of Excel. Topics covered include: • IF and the nested IF functions • VLOOKUP and HLOOKUP • The RANDBETWEEN function ________________________________________ WEEK 3 Module 3: Introduction to Filtering, Pivot Tables, and Charts This module introduces various data filtering capabilities of Excel. You’ll learn how to set filters in data to selectively access data. A very powerful data summarizing tool, the Pivot Table, is also explained and we begin to introduce the charting feature of Excel. Topics covered include: • VLOOKUP across worksheets • Data filtering in Excel • Use of Pivot tables with categorical as well as numerical data • Introduction to the charting capability of Excel ________________________________________ WEEK 4 Module 4: Advanced Graphing and Charting This module explores various advanced graphing and charting techniques available in Excel. Starting with various line, bar and pie charts we introduce pivot charts, scatter plots and histograms. You will get to understand these various charts and get to build them on your own. Topics covered include • Line, Bar and Pie charts • Pivot charts • Scatter plots • HistogramsCourse 2: Basic Data Descriptors, Statistical Distributions, and Application to Business Decisions- The ability to understand and apply Business Statistics is becoming increasingly important in the industry. A good understanding of Business Statistics is a requirement to make correct and relevant interpretations of data. Lack of knowledge could lead to erroneous decisions which could potentially have negative consequences for a firm. This course is designed to introduce you to Business Statistics. We begin with the notion of descriptive statistics, which is summarizing data using a few numbers. Different categories of descriptive measures are introduced and discussed along with the Excel functions to calculate them. The notion of probability or uncertainty is introduced along with the concept of a sample and population data using relevant business examples. This leads us to various statistical distributions along with their Excel functions which are then used to model or approximate business processes. You get to apply these descriptive measures of data and various statistical distributions using easy-to-follow Excel based examples which are demonstrated throughout the course. To successfully complete course assignments, students must have access to Microsoft Excel. ________________________________________ WEEK 1 Module 1: Basic Data Descriptors In this module you will get to understand, calculate and interpret various descriptive or summary measures of data. These descriptive measures summarize and present data using a few numbers. Appropriate Excel functions to do these calculations are introduced and demonstrated. Topics covered include: • Categories of descriptive data • Measures of central tendency, the mean, median, mode, and their interpretations and calculations • Measures of spread-in-data, the range, interquartile-range, standard deviation and variance • Box plots • Interpreting the standard deviation measure using the rule-of-thumb and Chebyshev’s theorem ________________________________________ WEEK 2 Module 2: Descriptive Measures of Association, Probability, and Statistical Distributions This module presents the covariance and correlation measures and their respective Excel functions. You get to understand the notion of causation versus correlation. The module then introduces the notion of probability and random variables and starts introducing statistical distributions. Topics covered include: • Measures of association, the covariance and correlation measures; causation versus correlation • Probability and random variables; discrete versus continuous data • Introduction to statistical distributions ________________________________________ WEEK 3 Module 3: The Normal Distribution This module introduces the Normal distribution and the Excel function to calculate probabilities and various outcomes from the distribution. Topics covered include: • Probability density function and area under the curve as a measure of probability • The Normal distribution (bell curve), NORM.DIST, NORM.INV functions in Excel ________________________________________ WEEK 4 Module 4: Working with Distributions, Normal, Binomial, Poisson In this module, you'll see various applications of the Normal distribution. You will also get introduced to the Binomial and Poisson distributions. The Central Limit Theorem is introduced and explained in the context of understanding sample data versus population data and the link between the two. Topics covered include: • Various applications of the Normal distribution • The Binomial and Poisson distributions • Sample versus population data; the Central Limit TheoremCourse 3: Business Applications of Hypothesis Testing and Confidence Interval Estimation - Confidence intervals and Hypothesis tests are very important tools in the Business Statistics toolbox. A mastery over these topics will help enhance your business decision making and allow you to understand and measure the extent of ‘risk’ or ‘uncertainty’ in various business processes. This is the third course in the specialization ""Business Statistics and Analysis"" and the course advances your knowledge about Business Statistics by introducing you to Confidence Intervals and Hypothesis Testing. We first conceptually understand these tools and their business application. We then introduce various calculations to constructing confidence intervals and to conduct different kinds of Hypothesis Tests. These are done by easy to understand applications. To successfully complete course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. Please note that earlier versions of Microsoft Excel (2007 and earlier) will not be compatible to some Excel functions covered in this course. WEEK 1 Module 1: Confidence Interval - Introduction In this module you will get to conceptually understand what a confidence interval is and how is its constructed. We will introduce the various building blocks for the confidence interval such as the t-distribution, the t-statistic, the z-statistic and their various excel formulas. We will then use these building blocks to construct confidence intervals. Topics covered include: • Introducing the t-distribution, the T.DIST and T.INV excel functions • Conceptual understanding of a Confidence Interval • The z-statistic and the t-statistic • Constructing a Confidence Interval using z-statistic and t-statistic WEEK 2 Module 2: Confidence Interval - Applications This module presents various business applications of the confidence interval including an application where we use the confidence interval to calculate an appropriate sample size. We also introduce with an application, the confidence interval for a population proportion. Towards the close of module we start introducing the concept of Hypothesis Testing. Topics covered include: • Applications of Confidence Interval • Confidence Interval for a Population Proportion • Sample Size Calculation • Hypothesis Testing, An Introduction WEEK 3 Module 3: Hypothesis Testing This module introduces Hypothesis Testing. You get to understand the logic behind hypothesis tests. The four steps for conducting a hypothesis test are introduced and you get to apply them for hypothesis tests for a population mean as well as population proportion. You will understand the difference between single tail hypothesis tests and two tail hypothesis tests and also the Type I and Type II errors associated with hypothesis tests and ways to reduce such errors. Topics covered include: • The Logic of Hypothesis Testing • The Four Steps for conducting a Hypothesis Test • Single Tail and Two Tail Hypothesis Tests • Guidelines, Formulas and an Application of Hypothesis Test • Hypothesis Test for a Population Proportion • Type I and Type II Errors in a Hypothesis WEEK 4 Module 4: Hypothesis Test - Differences in Mean In this module, you'll apply Hypothesis Tests to test the difference between two different data, such hypothesis tests are called difference in means tests. We will introduce the three kinds of difference in means test and apply them to various business applications. We will also introduce the Excel dialog box to conduct such hypothesis tests. Topics covered include: • Introducing the Difference-In-Means Hypothesis Test • Applications of the Difference-In-Means Hypothesis Test • The Equal & Unequal Variance Assumption and the Paired t-test for difference in means. • Some more applicationsCourse 4: Linear Regression for Business Statistics- Regression Analysis is perhaps the single most important Business Statistics tool used in the industry. Regression is the engine behind a multitude of data analytics applications used for many forms of forecasting and prediction. This is the fourth course in the specialization, ""Business Statistics and Analysis"". The course introduces you to the very important tool known as Linear Regression. You will learn to apply various procedures such as dummy variable regressions, transforming variables, and interaction effects. All these are introduced and explained using easy to understand examples in Microsoft Excel. The focus of the course is on understanding and application, rather than detailed mathematical derivations. Note: This course uses the ‘Data Analysis’ tool box which is standard with the Windows version of Microsoft Excel. It is also standard with the 2016 or later Mac version of Excel. However, it is not standard with earlier versions of Excel for Mac. WEEK 1 Module 1: Regression Analysis: An Introduction In this module you will get introduced to the Linear Regression Model. We will build a regression model and estimate it using Excel. We will use the estimated model to infer relationships between various variables and use the model to make predictions. The module also introduces the notion of errors, residuals and R-square in a regression model. Topics covered include: • Introducing the Linear Regression • Building a Regression Model and estimating it using Excel • Making inferences using the estimated model • Using the Regression model to make predictions • Errors, Residuals and R-square WEEK 2 Module 2: Regression Analysis: Hypothesis Testing and Goodness of Fit This module presents different hypothesis tests you could do using the Regression output. These tests are an important part of inference and the module introduces them using Excel based examples. The p-values are introduced along with goodness of fit measures R-square and the adjusted R-square. Towards the end of module we introduce the ‘Dummy variable regression’ which is used to incorporate categorical variables in a regression. Topics covered include: • Hypothesis testing in a Linear Regression • ‘Goodness of Fit’ measures (R-square, adjusted R-square) • Dummy variable Regression (using Categorical variables in a Regression) WEEK 3 Module 3: Regression Analysis: Dummy Variables, Multicollinearity This module continues with the application of Dummy variable Regression. You get to understand the interpretation of Regression output in the presence of categorical variables. Examples are worked out to re-inforce various concepts introduced. The module also explains what is Multicollinearity and how to deal with it. Topics covered include: • Dummy variable Regression (using Categorical variables in a Regression) • Interpretation of coefficients and p-values in the presence of Dummy variables • Multicollinearity in Regression Models WEEK 4 Module 4: Regression Analysis: Various Extensions The module extends your understanding of the Linear Regression, introducing techniques such as mean-centering of variables and building confidence bounds for predictions using the Regression model. A powerful regression extension known as ‘Interaction variables’ is introduced and explained using examples. We also study the transformation of variables in a regression and in that context introduce the log-log and the semi-log regression models. Topics covered include: • Mean centering of variables in a Regression model • Building confidence bounds for predictions using a Regression model • Interaction effects in a Regression • Transformation of variables • The log-log and semi-log regression modelsCourse 5: Business Statistics and Analysis Capstone- The Business Statistics and Analysis Capstone is an opportunity to apply various skills developed across the four courses in the specialization to a real life data. The Capstone, in collaboration with an industry partner uses publicly available ‘Housing Data’ to pose various questions typically a client would pose to a data analyst. Your job is to do the relevant statistical analysis and report your findings in response to the questions in a way that anyone can understand. Please remember that this is a Capstone, and has a degree of difficulty/ambiguity higher than the previous four courses. The aim being to mimic a real life application as close as possible."
https://www.classcentral.com/course/intro-accounting-data-analytics-visual-14477,"Accounting has always been about analytical thinking. From the earliest days of the profession, Luca Pacioli emphasized the importance of math and order for analyzing business transactions. The skillset that accountants have needed to perform math and to keep order has evolved from pencil and paper, to typewriters and calculators, then to spreadsheets and accounting software. A new skillset that is becoming more important for nearly every aspect of business is that of big data analytics: analyzing large amounts of data to find actionable insights. This course is designed to help accounting students develop an analytical mindset and prepare them to use data analytic programming languages like Python and R.

We’ve divided the course into three main sections. In the first section, we bridge accountancy to analytics. We identify how tasks in the five major subdomains of accounting (i.e., financial, managerial, audit, tax, and systems) have historically required an analytical mindset, and we then explore how those tasks can be  completed more effectively and efficiently by using big data analytics. We then present a FACT framework for guiding big data analytics: Frame a question, Assemble data, Calculate the data, and Tell others about the results.

In the second section of the course, we emphasize the importance of assembling data. Using financial statement data, we explain desirable characteristics of both data and datasets that will lead to effective calculations and visualizations.

In the third, and largest section of the course, we demonstrate and explore how Excel and Tableau can be used to analyze big data. We describe visual perception principles and then apply those principles to create effective visualizations. We then examine fundamental data analytic tools, such as regression, linear programming (using Excel Solver), and clustering in the context of point of sale data and loan data. We conclude by demonstrating the power of data analytic programming languages to assemble, visualize, and analyze data. We introduce Visual Basic for Applications  as an example of a programming language, and the Visual Basic Editor as an example of an integrated development environment (IDE).
      


            Read more
          



          INTRODUCTION TO THE COURSE
    -In this module, you will become familiar with the course, your instructor and your classmates, and our learning environment. This orientation module will also help you obtain the technical skills required to navigate and be successful in this course.

MODULE 1: INTRODUCTION TO ACCOUNTANCY ANALYTICS
    -In this module, you will learn how the accounting profession has evolved. You will recognize how data analytics has influenced the accounting profession and how accountants have the ability to impact how data analytics is used in the profession, as well as in an organization. Finally, you will learn how data analytics is influencing the different subdomains within accounting.

MODULE 2: ACCOUNTING ANALYSIS AND AN ANALYTICS MINDSET
    -In this module, you will learn to recognize the importance of making room for empirical enquiry in decision making. You will explore characteristics of an analytical mindset in business and accounting contexts, and link those to your core courses. You will then evaluate a framework for making data-driven decisions using big data.

MODULE 3: DATA AND ITS PROPERTIES
    -This module looks at specific characteristics of data that make it useful for decision making.

MODULE 4: DATA VISUALIZATION 1
    -In this module, you will learn fundamental principles that underlie data visualizations. Using those principles, you will identify use cases for different charts and learn how to build those charts in Excel. You will then use your knowledge of different charts to identify alternative charts that are better suited for directing attention.

MODULE 5: DATA VISUALIZATION 2
    -In this module, you’ll learn how to use Tableau to do with data what spies do when observing their surroundings: get an overview of the data, narrow in on certain aspects of the data that seem abnormal, and then analyze the data. Tableau is a great tool for facilitating the overview, zoom, then filter details-on-demand approach. Tableau is a lot like a more powerful version of Excel's pivot table and pivot chart functionality.

MODULE 6: ANALYTIC TOOLS IN EXCEL 1
    -In this module, you'll be guided through a mini-case study that will illustrate the first three parts of the FACT model, with a focus on the C, or calculations part of the FACT model. First, you will perform a correlation analysis to identify two-way relationships, and analyze correlations using a correlation matrix and scatter plots. You will then build on your knowledge of correlations and learn how to perform regression analysis in Excel. Finally, you will learn how to interpret and evaluate the diagnostic metrics and plots of a regression analysis.

MODULE 7: ANALYTIC TOOLS IN EXCEL 2
    -In this module, you’ll learn how the regression algorithm can be applied to fit a wide variety of relationships among data. Specifically, you’ll learn how to set up the data and run a regression to estimate the parameters of nonlinear relationships, categorical independent variables. You’ll also investigate if the effect of an independent variable depends on the level of another independent variable by including interaction terms in the multiple regression model. Another aspect of this module is learning how to evaluate models, regression or otherwise, to find the most favorable levels of the independent variables. For models that explain revenue, the most favorable levels of the independent variables will maximize revenue. In contrast, if you have a model that describes costs, like a budget, then the most favorable levels of the independent variables will minimize costs. Optimizing models can be difficult because there are so many inputs and constraints that need to be managed. In this module, you’ll learn how to use the Solver Add-In to find the optimal level of inputs. For some models, the dependent variable is a binary variable that has only two values, such as true/false, win/lose, or invest/not invest. In these situations, a special type of regression, called logistic regression, is used to predict how each observation should be classified. You’ll learn about the logit transformation that’s used to convert a binary outcome to a linear relationship with the independent variables. Excel doesn’t have a built-in logistic regression tool, so you’ll learn how to manually design a logistic regression model, and then optimize the parameters using the Solver Add-In tool.

MODULE 8: AUTOMATION IN EXCEL
    -The lessons in this module are organized around several useful tasks, including stacking multiple dataframes together into one dataframe, creating multiple histograms to accompany the descriptive statistics, and learning how to perform k-means clustering. After going through this module, you’ll not only gain a foundation to help you understand coding, but you’ll also learn more about analyzing financial data. Along the way, I hope that you’ll also pick up on a few other useful Excel functions."
https://www.classcentral.com/course/edx-data-models-and-decisions-in-business-analytics-8218,"In today’s world, managerial decisions are increasingly based on data-driven models and analysis using statistical and optimization methods that have dramatically changed the way businesses operate in most domains including service operations, marketing, transportation, and finance.
The main objectives of this course are the following:

Introduce fundamental techniques towards a principled approach for data-driven decision-making.
Quantitative modeling of dynamic nature of decision problems using historical data, and
Learn various approaches for decision-making in the face of uncertainty

Topics covered include probability, statistics, regression, stochastic modeling, and linear, nonlinear and discrete optimization.
Most of the topics will be presented in the context of practical business applications to illustrate its usefulness in practice.




Introduction to Probability: Random variables; Normal, Binomial, Exponential distributions; applications
Estimation: sampling; confidence intervals; hypothesis testing
Regression: linear regression; dummy variables; applications
Linear Optimization; Non-linear optimization; Discrete Optimization; applications
Dynamic Optimization; decision trees"
https://www.classcentral.com/course/data-science-statistics-machine-learning-18814,"Build models, make inferences, and deliver interactive data products.

This specialization continues and develops on the material from the Data Science: Foundations using R specialization. It covers statistical inference, regression models, machine learning, and the development of data products. In the Capstone Project, you’ll apply the skills learned by building a data product using real-world data. At completion, learners will have a portfolio demonstrating their mastery of the material.

The five courses in this specialization are the very same courses that make up the second half of the Data Science Specialization. This specialization is presented for learners who have already mastered the fundamentals and want to skip right to the more advanced courses.
      


          Course 1: Statistical Inference- Statistical inference is the process of drawing conclusions about populations or scientific truths from data. There are many modes of performing inference including statistical modeling, data oriented strategies and explicit use of designs and randomization in analyses. Furthermore, there are broad theories (frequentists, Bayesian, likelihood, design based, …) and numerous complexities (missing data, observed and unobserved confounding, biases) for performing inference. A practitioner can often be left in a debilitating maze of techniques, philosophies and nuance. This course presents the fundamentals of inference in a practical approach for getting things done. After taking this course, students will understand the broad directions of statistical inference and use this information for making informed choices in analyzing data.Course 2: Regression Models- Linear models, as their name implies, relates an outcome to a set of predictors of interest using linear assumptions. Regression models, a subset of linear models, are the most important statistical analysis tool in a data scientist’s toolkit. This course covers regression analysis, least squares and inference using regression models. Special cases of the regression model, ANOVA and ANCOVA will be covered as well. Analysis of residuals and variability will be investigated. The course will cover modern thinking on model selection and novel uses of regression models including scatterplot smoothing.Course 3: Practical Machine Learning- One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.Course 4: Developing Data Products- A data product is the production output from a statistical analysis. Data products automate complex analysis tasks or use technology to expand the utility of a data informed model, algorithm or inference. This course covers the basics of creating data products using Shiny, R packages, and interactive graphics. The course will focus on the statistical fundamentals of creating a data product that can be used to tell a story about data to a mass audience.Course 5: Data Science Capstone- The capstone project class will allow students to create a usable/public data product that can be used to show your skills to potential employers. Projects will be drawn from real-world problems and will be conducted with industry, government, and academic partners."
https://www.classcentral.com/course/edx-essential-math-for-data-analysis-using-excel-online-12164,"Starting with the absolute basics of math and data, this course builds up your analyst skills while removing the mathematical hurdles and barriers that often come with beginning to learn how to analyze data. This course is intended for anyone with the desire to do data analysis and who would like to learn the math behind it all in a simplified way, as well as for anyone who would like a thorough refresher on the essentials. Learn how to handle different data types, understand mathematical notation, become proficient in handling data sets and summary statistics, and even get a sneak peek into how to be effective at making stctistical inferences and predictions, all told in a way to maximize your understanding—so that afterward, you can be well on your way toward analyzing data in any field or discipline.
      


          Module 1: Introduction to Data and VariablesModule 2: Summarizing Data with Distributions and GraphsModule 3: Summary StatisticsModule 4: Business Statistics Module 5: Introduction to Inferential Math and Forecasting"
https://www.classcentral.com/course/six-sigma-analyze-8872,"This course will cover the Measure phase and portions of the Analyze phase of the Six Sigma DMAIC (Define, Measure, Analyze, Improve, and Control) process. You will learn about lean tools for process analysis, failure mode and effects analysis (FMEA), measurement system analysis (MSA) and gauge repeatability and reproducibility (GR&R), and you will be introduced to basic statistics. This course will outline useful measure and analysis phase tools and will give you an overview of statistics as they are related to the Six Sigma process. 

The statistics module will provide you with an overview of the concepts and you will be given multiple example problems to see how to apply these concepts. Every module will include readings, discussions, lecture videos, and quizzes to help make sure you understand the material and concepts that are studied.

Our applied curriculum is built around the latest handbook The Certified Six Sigma Handbook (2nd edition) and students will develop /learn the fundamentals of Six Sigma. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Handbook (2nd edition). The companion text is not required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).
      


            Read more
          



          Measurement System Analysis
    -Welcome to Six Sigma Tools for Analyze! This is the third course in the Six Sigma Yellow Belt Specialization. Your team of instructors, Dr. Bill Bailey, Dr. David Cook, Dr. Christine Scherrer, and Dr. Gregory Wiles, currently work in the College of Engineering and Engineering Technology at Kennesaw State University. This module will introduce you to Measurement System Analysis (MSA) which is a key component of the Measure phase of the DMAIC process. You will also learn about Gauge Repeatability & Reproducibility (GR&R) and why it is used in the measurement phase. 

Process Analysis Tools
    -This module will introduce you to the Analysis phase of the DMAIC process. Process analysis helps you to better understand current processes and how they can be improved. You will be introduced to many of the different process analysis tools that are commonly used by Six Sigma experts. Failure Mode and Effects Analysis (FMEA) will also be introduced to help you better understand how to identify process failures. 

Root Cause Analysis
    -Root cause analysis is a common problem solving step. Determining the root cause of something is an important aspect of uncovering the causes of a problem. In this module you will review the different tools used in determining root cause including 5-whys, process mapping, force-field analysis, and matrix charts. 

Data Analysis
    -In this module you will be diving into the statistical side of Six Sigma. You will begin with learning about the basic distribution types which include normal and binomial. You will then proceed to variation and will learn the difference between common and special cause variation."
https://www.classcentral.com/course/teaching-data-science-19139,"Learn practical ways to teach data science
Understanding how to use and interpret data will be essential for the next generation, but many schools and teachers aren’t equipped to teach basic data science to students. This course will help you introduce data science in the classroom so that your students are prepared for the future.
You will get an introduction to useful tools for exploring data, learn the basics of statistics and explore how you can embed data activities into your teaching plans. You will get hands on experience interpreting real data so that you feel comfortable helping students get started with data science.
This course is primarily for school teachers, but it might also be of interest to parents looking to teach their children about data science."
https://www.classcentral.com/course/six-sigma-tools-define-measure-8871,"This course is for you if you are looking to learn more about Six Sigma or refresh your knowledge of the basic components of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will cover the Define phase and introduce you to the Measure phase of the DMAIC (Define, Measure, Analyze, Improve, and Control) process. You will learn about Six Sigma project development and implementation, you will become familiar with project management tools, you will be introduced to statistics and understand its significance to Six Sigma, and finally you will learn about data collection and its importance to an organization.

Every module will include readings, videos, and a quiz to help make sure you understand the material and concepts that are studied. You will also have the opportunity to participate in peer review exercises to give you the opportunity to apply the material to your daily life.

Our applied curriculum is built around the latest handbook The Certified Six Sigma Handbook (2nd edition) and students will develop /learn the fundamentals of Six Sigma. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Handbook (2nd edition). The companion text is not required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).
      


            Read more
          



          Project Identification
    -Welcome to Six Sigma Tools for Define and Measure! This is the second course in the Six Sigma Yellow Belt Specialization. Your team of instructors, Dr. Bill Bailey, Dr. David Cook, Dr. Christine Scherrer, and Dr. Gregory Wiles, currently work in the College of Engineering and Engineering Technology at Kennesaw State University. This module will introduce you to the process of project development and selection. The first phase of the DMAIC process is the Define phase which begins with project identification and proceeds onto project management. When identifying Six Sigma projects it is important to ensure that resources and time are used in the most effective and productive way. In this module you will learn about the importance of customer and stakeholder needs in relation to project development. 

Project Management Basics
    -Now that we have identified the Six Sigma project how to we begin to develop it? This module will cover the basics of project development and management. You will be introduced to many common aspects of a project plan as well as tools for managing your project. These tools include activity network diagrams, affinity diagrams, tree diagrams, relations charts, and matrix charts. Project Management is the second aspect of the Define phase in the DMAIC process of Six Sigma. 

Basic Statistics
    -The next phase of the DIMAC process is the Measure phase. We will begin by exploring basic statistics and its significance to Six Sigma. This module will introduce you to the measures of variance, range, standard deviation, center, mean, median, and mode. You will also learn how to interpret these measures using given data sets.

Data Collection
    -The second aspect of the Measure phase is Data Collection. In the previous module we learned about statistics and how to use data for different needs. In this module we will learn different techniques on how to collect data, the reasons for collecting data, and ensuring data accuracy and integrity."
https://www.classcentral.com/course/business-analytics-decision-making-7040,"In this course you will learn how to create models for decision making. We will start with cluster analysis, a technique for data reduction that is very useful in market segmentation. You will then learn the basics of Monte Carlo simulation that will help you model the uncertainty that is prevalent in many business decisions. A key element of decision making is to identify the best course of action. Since businesses problems often have too many alternative solutions, you will learn how optimization can help you identify the best option. What is really exciting about this course is that you won’t need to know a computer language or advanced statistics to learn about these predictive and prescriptive analytic models. The Analytic Solver Platform and basic knowledge of Excel is all you’ll need. Learners participating in assignments will be able to get free access to the Analytic Solver Platform.
      


          Data Exploration and Reduction — Cluster Analysis

Dealing with Uncertainty and Analyzing Risk

Identifying the Best Options — Optimization
    -At the end of this module students should be able to: 1. Develop a spreadsheet model for an optimization problem 2. Use Excel to solve optimization models 3. Interpret solutions and conduct what-if analysis

Decision Analytics
    -At the end of this module students should be able to: 1. Given a business situation, apply an appropriate technique to identify the best solution alternatives 2. Formulate and solve models for business problems that requires yes/no decisions and logical constraints 3. Create models that mix techniques and tools such as simulation and optimizationAnalyze and interpret results to make informed decisions"
https://www.classcentral.com/course/data-analytics-accountancy-1-9051,"Welcome to Data Analytics Foundations for Accountancy I! You’re joining thousands of learners currently enrolled in the course. I'm excited to have you in the class and look forward to your contributions to the learning community.

To begin, I recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class.

If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.

Good luck as you get started, and I hope you enjoy the course!
      


          Course Orientation
    -You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.

Module 1: Foundations
    -This module serves as the introduction to the course content and the course Jupyter server, where you will run your analytics scripts. First, you will read about specific examples of how analytics is being employed by Accounting firms. Next, you will learn about the capabilities of the course Jupyter server, and how to create, edit, and run notebooks on the course server. After this, you will learn how to write Markdown formatted documents, which is an easy way to quickly write formatted text, including descriptive text inside a course notebook. Finally, you will begin learning about Python, the programming language used in this course for data analytics.

Module 2: Introduction to Python
    -This module focuses on the basic features in the Python programming language that underlie most data analytics scripts. First, you will read about why accounting students should learn to write computer programs. Second, you will learn about basic data structures commonly used in Python programs. Third, you will learn how to write functions, which can be repeatedly called, in Python, and how to use them effectively in your own programs. Finally, you will learn how to control the execution process of your Python program by using conditional statements and looping constructs. At the conclusion of this module, you will be able to write Python scripts to perform basic data analytic tasks.

Module 3: Introduction to Data Analysis
    -This module introduces fundamental concepts in data analysis. First, you will read a report from the Association of Accountants and Financial Professionals in Business that explores Big Data in Accountancy. Next, you will learn about the Unix file system, which is the operating system used for most big data processing (as well as Linux and Mac OSX desktops and many mobile phones). Second, you will learn how to read and write data to a file from within a Python program. Finally, you will learn about the Pandas Python module that can simplify many challenging data analysis tasks, and includes the DataFrame, which programmatically mimics many of the features of a traditional spreadsheet.

Module 4: Statistical Data Analysis
    -This module introduces fundamental concepts in data analysis. First, you will read about how to perform many basic tasks in Excel by using the Pandas module in Python. Second, you will learn about the Numpy module, which provides support for fast numerical operations within Python. This module will focus on using Numpy with one-dimensional data (i.e., vectors or 1-D arrays), but a later module will explore using Numpy for higher-dimensional data. Third, you will learn about descriptive statistics, which can be used to characterize a data set by using a few specific measurements. Finally, you will learn about advanced functionality within the Pandas module including masking, grouping, stacking, and pivot tables.

Module 5: Introduction to Visualization
    -This module introduces visualization as an important tool for exploring and understanding data. First, the basic components of visualizations are introduced with an emphasis on how they can be used to convey information. Also, you will learn how to identify and avoid ways that a visualization can mislead or confuse a viewer. Next, you will learn more about conveying information to a user visually, including the use of form, color, and location. Third, you will learn how to actually create a simple visualization (basic line plot) in Python, which will introduce creating and displaying a visualization within a notebook, how to annotate a plot, and how to improve the visual aesthetics of a plot by using the Seaborn module. Finally, you will learn how to explore a one-dimensional data set by using rug plots, box plots, and histograms.

Module 6: Introduction to Probability
    -In this Module, you will learn the basics of probability, and how it relates to statistical data analysis. First, you will learn about the basic concepts of probability, including random variables, the calculation of simple probabilities, and several theoretical distributions that commonly occur in discussions of probability. Next, you will learn about conditional probability and Bayes theorem. Third, you will learn to calculate probabilities and to apply Bayes theorem directly by using Python. Finally, you will learn to work with both empirical and theoretical distributions in Python, and how to model an empirical data set by using a theoretical distribution.

Module 7: Exploring Two-Dimensional Data
    -This modules extends what you have learned in previous modules to the visual and analytic exploration of two-dimensional data. First, you will learn how to make two-dimensional scatter plots in Python and how they can be used to graphically identify a correlation and outlier points. Second, you will learn how to work with two-dimensional data by using the Numpy module, including a discussion on analytically quantifying correlations in data. Third, you will read about statistical issues that can impact understanding multi-dimensional data, which will allow you to avoid them in the future. Finally, you will learn about ordinary linear regression and how this technique can be used to model the relationship between two variables.

Module 8: Introduction to Density Estimation
    -Often, as part of exploratory data analysis, a histogram is used to understand how data are distributed, and in fact this technique can be used to compute a probability mass function (or PMF) from a data set as was shown in an earlier module. However, the binning approach has issues, including a dependance on the number and width of the bins used to compute the histogram. One approach to overcome these issues is to fit a function to the binned data, which is known as parametric estimation. Alternatively, we can construct an approximation to the data by employing a non-parametric density estimation. The most commonly used non-parametric technique is kernel density estimation (or KDE). In this module, you will learn about density estimation and specifically how to employ KDE. One often overlooked aspect of density estimation is the model representation that is generated for the data, which can be used to emulate new data. This concept is demonstrated by applying density estimation to images of handwritten digits, and sampling from the resulting model."
https://www.classcentral.com/course/healthcare-administration-management-18858,"This Specialization was designed to help you keep pace with the rapidly changing world of health and health care. Through partnership with Wharton and Penn Medicine, you’ll learn from experts in business acumen, health care management, and health care policy to develop the skills you’ll need to successfully navigate the quickly evolving landscape of this fast-growing field.* By the end of this specialization, you will have a strong foundational understanding of the American health care system, along with the basic financial and management skills to make better, well-informed decisions that improve the cost and quality of the health care you or your enterprise provides. *The employment of medical and health services managers is projected to grow 17% from 2014 to 2024, significantly faster than the average for all occupations. Source: Bureau of Labor Statistics, US Department of Labor. “Occupational Outlook handbook 2016-17 Edition. Medical and Health Services Managers.”



          Course 1: Financial Acumen for Non-Financial Managers- In this course, you’ll explore how financial statement data and non-financial metrics can be linked to financial performance. Professors Rick Lambert and Chris Ittner of the Wharton School have designed this course to help you gain a practical understanding of how data is used to assess what drives financial performance and forecast future financial scenarios. You’ll learn more about the frameworks of financial reporting, income statements, and cash reporting, and apply different approaches to analyzing financial performance using real-life examples to see the concepts in action. By the end of this course, you’ll have honed your skills in understanding how financial data and non-financial data interact to forecast events and be able to determine the best financial strategy for your organization.Course 2: The Economics of Health Care Delivery- In this course, you’ll learn about the key components of health care, and the economics behind their principles and pricing strategies. Professors Ezekiel Emanuel of Penn Medicine and Guy David of the Wharton School have designed this course to help you understand the complex structure of the health care system and health insurance. Through study and analysis of providers and insurance through an economic lens, you’ll learn how basic economic principles apply to both principles and payment methods. By the end of this course, you’ll be able to identify different types of health care providers and understand the dynamic between the providers and patient so you can employ best practices and maximize profit for your health care organization.Course 3: Management Fundamentals- People are the most valuable asset of any business, but they are also the most unpredictable, and the most difficult asset to manage. And although managing people well is critical to the health of any organization, most managers don't get the training they need to make good management decisions. Now, award-winning authors and renowned management Professors Mike Useem and Peter Cappelli of the Wharton School have designed this course to introduce you to the key elements of managing people. Based on their popular course at Wharton, this course will teach you how to motivate individual performance and design reward systems, how to design jobs and organize work for high performance, how to make good and timely management decisions, and how to design and change your organization’s architecture. By the end of this course, you'll have developed the skills you need to start motivating, organizing, and rewarding people in your organization so that you can thrive as a business and as a social organization.Course 4: Health Care Innovation- In this course, you’ll learn the foundational economic theories behind health care innovation and how to optimize your own health care practice or organization. Designed to help you gain a practical understanding of the theoretical frameworks of behavioral economics and operations management in the health care setting, this course will help you apply these frameworks to assess health care practices and apply innovation while managing risk. You’ll also explore the best practices for evaluating one’s innovative practices, using real-life examples of success to see the concepts in action. By the end of this course, you’ll have honed your skills in optimizing health care operations, and be able to develop the right set of evaluations and questions to achieve best innovative practices within your organization."
https://www.classcentral.com/course/six-sigma-define-measure-advanced-9073,"This course is for you if you are looking to dive deeper into Six Sigma or strengthen and expand your knowledge of the basic components of green belt level of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will take you deeper into the principles and tools associated with the ""Design"" and ""Measure"" phases of the DMAIC structure of Six Sigma.

It is highly recommended that you complete the ""Yellow Belt Specialization""  and the course ""Six Sigma and the Organization (Advanced)"" before beginning this course.

In this course, your instructors will introduce you to, and have you apply, some of the tools and metrics that are critical components of Six Sigma. This course will provide you with the advanced knowledge of team dynamics and performance, process analysis, probability, statistics,  statistical distributions, collecting and summarizing data, measurement systems analysis, process and performance capability, and exploratory data analysis associated with Six Sigma and Lean. 

Every module will include readings, videos, and quizzes to help make sure you understand the material and concepts that are studied. 

Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Green Belt Handbook (2nd edition). The companion text is NOT required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly-recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).
      


            Read more
          



          Yellow Belt Refresher - Team Basics
    -This module is a Yellow Belt Refresher Area, since some learners may have entered this course without having taken the previous yellow belt material. This refresher area does not serve as a ""replacement"" for taking the yellow belt specialization. It is only a ""patch"" for those who already have some knowledge of six sigma, but find the yellow belt specialization to be too introductory for their current situation. If you find yourself struggling with the green belt material in this course, it is highly recommended that you travel through the yellow belt specialization before attempting the green belt content.  In this module you will learn about team development, different types of teams, tools for decision making, and tools for communication. 

Process Analysis and Documentation
    -In this module you will learn the application of process maps and flow charts swim lane. You will also about the application of: written procedures and works instructions, CEDAC, the Pareto diagram, and the relationship matrix.

Probability and Statistics - pt1
    -In this module, you will learn how mutually-exclusive events and independent events relate to probability. You will also learn how to solve basic probability problems, including those that require the addition, multiplicative, and conditional rules of probability.

Probability and Statistics - pt2
    -In this module, you will learn how to solve probability problems related to combinations and permutations. You will also learn about central limit theorem and how to apply it to the concept of confidence intervals. This module also touches on descriptive and inferential statistics.

Statistical Distribution
    -In this module, you will learn about the basic characteristics of probability distributions. You will also learn about binomial, normal, Poisson, t-, chi-square, and F-distributions.

COLLECTING AND SUMMARIZING DATA & MEASURING SYSTEM ANALYSIS
    -In this module, you will learn the different types and sources of data. You will also learn: how to classify groups of data for descriptive statistics analysis, different ways to graphically display data, how to express the importance of measurement systems analysis, and the difference between precision and accuracy. This module also begins with a Yellow Belt Refresher Area, since some learners may have entered this course without having taken the previous yellow belt material. This refresher area does not serve as a ""replacement"" for taking the yellow belt material. It is only a ""patch"" for those who already have some knowledge of six sigma, but find the yellow belt specialization to be too introductory for their current situation. If you find yourself struggling with the green belt material in this course, it is highly recommended that you travel through the yellow belt specialization before attempting the green belt content. 

PROCESS AND PERFORMANCE CAPABILITY & EXPLORATORY DATA ANALYSIS
    -In this module, you will learn how to use statistical tools and analyze data, including skills like: how to distinguish between natural process limits and specification limits, how to calculate the value of Cp and Ck, how to deal with process capability studies. This module also addresses multi-vari studies, the the correlation coefficient, and linear regression."
https://www.classcentral.com/course/intro-to-data-exploration-11220,"This course answers the questions, What is data visualization and What is the power of visualization? It also introduces core concepts such as dataset elements, data warehouses and exploratory querying, and combinations of visual variables for graphic usefulness, as well as the types of statistical graphs, tools that are essential to exploratory data analysis.
      


          Getting Started

Introduction to Data Exploration Components

Exploratory Querying and Visual Variables Used in Data Exploration and Visualization

Statistical Graphics: Design Principles for the Most Widely Used Data Visualization Charts

 STATISTICAL GRAPHICS: DESIGN PRINCIPLES FOR Box Charts and QQ Plots"
https://www.classcentral.com/course/edx-analytics-for-decision-making-7517,"Want to know how to avoid bad decisions with data?
Making good decisions with data can give you a distinct competitive advantage in business. This statistics and data analysis course will help you understand the fundamental concepts of sound statistical thinking that can be applied in surprisingly wide contexts, sometimes even before there is any data! Key concepts like understanding variation, perceiving relative risk of alternative decisions, and pinpointing sources of variation will be highlighted.
These big picture ideas have motivated the development of quantitative models, but in most traditional statistics courses, these concepts get lost behind a wall of little techniques and computations. In this course we keep the focus on the ideas that really matter, and we illustrate them with lively, practical, accessible examples.
We will explore questions like: How are traditional statistical methods still relevant in modern analytics applications? How can we avoid common fallacies and misconceptions when approaching quantitative problems? How do we apply statistical methods in predictive applications? How do we gain a better understanding of customer engagement through analytics?
This course will be is relevant for anyone eager to have a framework for good decision-making. It will be good preparation for students with a bachelor's degree contemplating graduate study in a business field.
Opportunities in analytics are abundant at the moment. Specific techniques or software packages may be helpful in landing first jobs, but those techniques and packages may soon be replaced by something newer and trendier. Understanding the ways in which quantitative models really work, however, is a management level skill that is unlikely to go out of style.
This course is part of the Business Principles and Entrepreneurial Thought XSeries.



            Read more"
https://www.classcentral.com/course/edx-market-segmentation-analysis-11387,"Conducting market segmentation analysis and committing to a long-term market segmentation strategy is a complex and challenging journey for any organisation. This course guides you through the entire process of market segmentation analysis and offers a ten-step process that makes customer segmentation efficient and organised. 
This course begins with the decision to conduct market segmentation analysis and continues through to the final stages of evaluating the success of the strategy and monitoring the market for possible changes. We also cover segmentation variables such as geographic segmentation, psychographic segmentation, behavioural segmentation, and demographic segmentation. 
In this course, we will explore how to leverage statistical concepts into the organisation's segmentation strategy, such as the hierarchical clustering and partitioning methods, exploratory data analysis, biclustering, mixture models, and regression models. 
The concepts and skills you will gain in this course are relevant in a wide range of contexts in both the for- and not-for-profit sectors. 
This course enables you to conduct customer segmentation analysis. You can replicate the calculations and visualisations demonstrated in the customer segmentation modelsby downloading the data and the R code. R is a free open-source statistical computing environment, and is widely acknowledged as the universal language of computational statistics. 
This course is based on and taught by the authors of the book Market Segmentation Analysis: Understanding It, Doing It, and Making It Useful. You will have full access to this valuable resource when you enrol in this course.



            Read more
          



Module 1: Introduction and Steps 1 and 2
We define market segmentation analysis, explain why it is the basis of marketing planning, and why it informs both strategic and tactical marketing decisions. We provide an overview of the ten-step process in market segmentation analysis. In Step 1, we explain the requirements for market segmentation, helping learners decide whether their organisation is ready to segment. In Step 2 we specify the ideal target segment, including segment evaluation criteria. 
Module 2: Steps 3 and 4
In Step 3,we describe collecting data and define the segmentation variables and criteria, and discuss different sources of data. In Step 4 we explore the data and discuss data cleaning and how to pre-process categorical and numeric variables. We also demonstrate how to use R, to assist you with exploring the data. 
Module 3: Step 5
In Step 5,we extract market segments and group consumers using distance-based, hierarchical, and partitioning methods; hybrid approaches; and model-based methods. We also explain data structure analysis. 
Module 4: Steps 6 and 7
In Steps 6 and 7, we profile and describe segments. We use traditional approaches as well as visualisation techniques to identify key characteristics of market segments. In Step 7 we develop and visualise a complete picture of market segments; this includes testing and predicting segment differences in descriptor variables. 
Module 5: Steps 8, 9 and 10
In Step 8, we select the target segments including the tasks targeting decision and market segment evaluation. In Step 9 we customise the marketing mix. We also discuss the implications of market segmentation for marketing mix decisions concerning product, price, place,and promotion. Finally, in Step 10, we evaluate the success of the segmentation strategy, and stability of segment membership. We conclude with a discussion of segment hopping and segment evolution."
https://www.classcentral.com/course/serverless-machine-learning-gcp-8696,"This one-week accelerated on-demand course provides participants a a hands-on introduction to designing and building machine learning models on Google Cloud Platform. Through a combination of presentations, demos, and hand-on labs, participants will learn machine learning (ML) and TensorFlow concepts, and develop hands-on skills in developing, evaluating, and productionizing ML models.

OBJECTIVES

This course teaches participants the following skills:

  ● Identify use cases for machine learning

  ● Build an ML model using TensorFlow

  ● Build scalable, deployable ML models using Cloud ML

  ● Know the importance of preprocessing and combining features

  ● Incorporate advanced ML concepts into their models

  ● Productionize trained ML models


PREREQUISITES

To get the most of out of this course, participants should have:

  ● Completed Google Cloud Fundamentals- Big Data and Machine Learning course OR have equivalent experience

  ● Basic proficiency with common query language such as SQL

  ● Experience with data modeling, extract, transform, load activities

  ● Developing applications using a common programming language such Python

  ● Familiarity with Machine Learning and/or statistics

Google Account Notes:
• Google services are currently unavailable in China.
      


          Welcome to Serverless Machine Learning on Google Cloud Platform

Module 1: Getting Started with Machine Learning

Module 2: Building ML models with Tensorflow

Module 3: Scaling ML models with Cloud ML Engine

Module 4: Feature Engineering"
https://www.classcentral.com/course/swayam-project-management-7912,"With the concept of managing Big Projects under costs and time constraints, it is imperative, that people working in manufacturing/process/service industry have a very good understanding of the general and advanced concepts of Project Management. It is with this motivation that this course is designed, to meet the demand in the market from, UG to PG students coming from a variety of fields, be it Engineering or Management.INTENDED AUDIENCE:Bachelors in Engineering, Masters in Business Administration, Masters in Economics, Masters in Industrial Engineering, Masters in Operations Research/Operations Management, PhD in related fieldsPREREQUISITE:Basic Probability & Statistics,Basic Operations ResearchINDUSTRY SUPPORT:Manufacturing Industry, Chemical Industry, Steel Industry, Cement Industry, Service Industry, Industry with well developed SCM, etc.



COURSE LAYOUT Risk associated with ProjectsDecisionTree ModelingCost Evaluation Techniques in Project ManagementGANNT Chart and Precedence DiagramsPER, CPMProject Life CyclesConcepts of SchedulingGERTQ-GERTCritical Chain and Theory of ConstraintsActivity Network DiagramResource requirement, Resource constraints, Crashing of JobsProject Control TechniquesEarned Value Project"
https://www.classcentral.com/course/edx-interpreting-and-communicating-data-insights-in-business-6525,"Data flows everywhere today, but consuming and communicating the essence of these signals is no easy task. Business executives want to hear findings as efficiently as possible. Business managers would like an engaging dialogue with data, but turn data aside if the burden of knowledge is too great. Results really matter. A proven framework and method for interpreting and communicating data insights is necessary when the going gets tough.
In this data analysis and statistics course, you’ll discover effective strategies and tools to master the process of interpreting and then communicating your data analysis and visualization work to business audiences. This course will empower you with proven business analytics methods and boost your confidence for effective communication of insights.



Week 1: Translate Business Needs into Measurements
In data work, it can be hard to know where to start. And in business, this challenge is made more pressing by the fact that data insights aren’t free. Companies invest money in technology, skills and infrastructure to enable access to data so that it drives value for the organization. Here in Module 1, we’ll learn methods that will enable you to find data insights for business with confidence. We begin by tying our data question back to business goals and decisions through the process of translation. We'll then introduce our data sources and describe the three data sets you'll use for your homework assignments
Week 2: Business Analytic Methods
In this module you will learn how to draw insights out of data using both statistics and data visualization methods. We then introduce some common cognitive biases as it relates to analytics, and how create the right artifacts for the right types of insight.
Week 3: Build Compelling Data Presentations
In this module we learn how to take the artifacts and insight techniques and combine them together with strong narrative and methods to allow for optimal communication of insights. We begin by setting up our artifacts in the proper format, learn a new presentation tool, and how to structure storytelling as efficiently as possible.
Week 4: Communicate Insights to the Business
In our final module, we bring together our artifacts and story from the previous modules into the final hurtle of our analytics flow: communicating the results to your stakeholder, client, or boss. We discuss techniques to consider when delivering insights and information, as well as ways to mitigate concerns or criticisms through ownership."
https://www.classcentral.com/course/edx-business-analytics-fundamentals-10333,"Data analysis has become critical for success in business. Most major employers are looking for candidates with the ability to analyze, understand, and articulate data analytics. 
This foundational business analytics course will provide you with the hands-on skills and knowledge to be able to analyze, present findings, and make meaningful conclusions about data in a business setting. By the end of this course, you will be able tooffer valuable insights by recognizing, interpreting, and summarizing your company's data. 
The course covers key subjects related to businessanalytics including Data Collection, Data Visualizations, Descriptive Statistics, Basic Probability, Statistical Inference, and Creating Linear Models. 
This course will use real-life datasetsfrom publicly available sources."
https://www.classcentral.com/course/columbiax-data-science-for-executives-18406,"Data science is making us smarter and more innovative in so many ways. How does it all work? In this Data Science and Analytics Professional Certificate program you will gain insight into the latest data science tools and their application in finance, health care, product development, sales and more. With real world examples, we will demonstrate how data science can improve corporate decision-making and performance, personalize medicine and advance your career goals.
Taught by a distinguished team of professors at Columbia University’s Data Science Institute, this program is perfect for anyone who wants to understand basic concepts in data science without getting into the weeds of programming. Aimed at organization leaders, business managers, health care professionals and anyone considering a career in data science, this program will steep learners in the fundamentals of statistics, machine learning and algorithms. It will also introduce emerging technologies such as the Internet of Things (IoT) , or wirelessly connected products, and techniques that allow computers to summarize mountains of text, audio and video. Concrete examples provided throughout the program will ensure that learners fully grasp and master key concepts.



Courses under this program:Course 1: Statistical Thinking for Data Science and Analytics
Learn how statistics plays a central role in the data science approach.
Course 2: Machine Learning for Data Science and Analytics
Learn the principles of machine learning and the importance of algorithms.
Course 3: Enabling Technologies for Data Science and Analytics: The Internet of Things
Discover the relationship between Big Data and the Internet of Things (IoT)."
https://www.classcentral.com/course/swayam-six-sigma-7967,"The course on Six-Sigma will focus on detailed strategic and operational issues of process improvement and variation reduction. Six-sigma is a measure of quality that strives for near perfection. It is a disciplined, data-driven approach for eliminating defects (driving towards six standard deviations between the mean and the nearest specification limit) in any process-from manufacturing to transactional and from product to service.A Six-sigma defect is anything outside of customer specifications. To be tagged Six Sigma, a process must not produce more than 3.4 defects per million opportunities.Six-sigma employs a systematic approach of DMAIC (Define, Measure, Analyze, Improve and Control) for the process improvement. This course will provide a detailed understanding on various issues specific to each phase of DMAIC.The course is designed with a practical orientation and includes cases, industry examples and MINITAB software applications.The course is designed to satisfy the need of both industry professionals and University students.The content is beneficial to both manufacturing and service industry.INTENDED AUDIENCE : Mechanical Engineering, MBA, Industrial EngineeringPREREQUISITES : StatisticsINDUSTRY SUPPORT : Manufacturing and Service Industry 
      


COURSE LAYOUT Week 1:QUALITY: FUNDAMENTALS AND KEY CONCEPTSLecture 1: Brief overview of the courseLecture 2: Quality concepts and definitionLecture 3: History of continuous improvementLecture 4: Six Sigma Principles and Focus Areas (Part 1)Lecture 5: Six Sigma Principles and Focus Areas (Part 2)Lecture 6: Six Sigma ApplicationsWeek 2:QUALITY: FUNDAMENTALS AND KEY CONCEPTSLecture 7: Quality Management: Basics and Key ConceptsLecture 8: Fundamentals of Total Quality ManagementLecture 9: Cost of qualityLecture 10: Voice of customerLecture 11: Quality Function Deployment (QFD)Lecture 12: Management and Planning Tools (Part 1)Lecture 13: Management and Planning Tools (Part 2)Week 3:DEFINELecture 14: Six Sigma Project Identification, Selection and DefinitionLecture 15: Project Charter and MonitoringLecture 16: Process characteristics and analysisLecture 17: Process Mapping: SIPOCWeek 4:MEASURELecture 18: Data Collection and Summarization (Part 1)Lecture 19: Data Collection and Summarization (Part 2)Lecture 20: Measurement systems: FundamentalsLecture 21: Measurement systems analysis: Gage R&R studyLecture 22: Fundamentals of statisticsLecture 23: Probability theoryWeek 5:MEASURELecture 24: Process capability analysis: Key ConceptsLecture 25: Process capability analysis: Measures and IndicesLecture 26: Process capability analysis: Minitab ApplicationLecture 27: Non-normal process capability analysisWeek 6:ANALYZELecture 28: Hypothesis testing: FundamentalsLecture 29: Hypothesis Testing: Single Population TestLecture 30: Hypothesis Testing: Two Population TestLecture 31: Hypothesis Testing: Two Population: Minitab ApplicationLecture 32: Correlation and Regression AnalysisLecture 33: Regression Analysis: Model ValidationWeek 7: ANALYZELecture 34: One-Way ANOVALecture 35: Two-Way ANOVALecture 36: Multi-vari AnalysisLecture 37: Failure Mode Effect Analysis (FMEA)Week 8:IMPROVELecture 38: Introduction to Design of ExperimentLecture 39: Randomized Block DesignLecture 40: Randomized Block Design: Minitab ApplicationLecture 41: Factorial DesignLecture 42: Factorial Design: Minitab ApplicationWeek 9:IMPROVELecture 43: Fractional Factorial DesignLecture 44: Fractional Factorial Design: Minitab ApplicationLecture 45: Taguchi Method: Key ConceptsLecture 46: Taguchi Method: Illustrative ApplicationWeek 10:CONTROLLecture 47: Seven QC ToolsLecture 48: Statistical Process Control: Key ConceptsLecture 49: Statistical Process Control: Control Charts for VariablesLecture 50: Operating Characteristic (OC) Curve for Variable Control chartsLecture 51: Statistical Process Control: Control Charts for AttributesLecture 52: Operating Characteristic (OC) Curve for Attribute Control chartsLecture 53: Statistical Process Control: Minitab ApplicationWeek 11:CONTROLLecture 54: Acceptance Sampling: Key ConceptsLecture 55: Design of Acceptance Sampling Plans for Attributes (Part 1)Lecture 56: Design of Acceptance Sampling Plans for Attributes (Part 2)Lecture 57: Design of Acceptance Sampling Plans for VariablesLecture 58: Acceptance Sampling: Minitab ApplicationWeek 12:SIX SIGMA IMPLEMENTATION CHALLENGESLecture 59: Design for Six Sigma (DFSS): DMADV, DMADOVLecture 60: Design for Six Sigma (DFSS): DFXLecture 61: Team ManagementLecture 62: Six Sigma: Case studyLecture 63: Six Sigma: Summary of key concepts"
https://www.classcentral.com/course/iversity-business-analysis-101-2843,"Over the last 10 to 15 years, the inception of new products and services has steered corporations to manage this development in a much more efficient manner in order to meet customer demand. This introspection within the business community was comprised of two elements:
1.) Can one identify the deficiencies in previous workings in order to overcome the shortcomings?2.) What is the employee skillset required in order to ensure proper deployment of these new customer needs?
The inception of the Business Analysis profession was a result of this changed situation as well as an overall classification of those competencies that were already being practiced in the enterprise.This course takes a look at the competencies required of a Business Analysis. In addition, you will be exposed to the following elements tied to the subject matter:

The typical business analysis path, starting with a problem statement, the requirements tied to the need to be addressed and a definition of the most robust solution possible.
The tie between the enterprise strategy and business analysis activities.
The relationship between needs and current processes
Document analysis and the importance of historical data
The means by which elicitation is performed
Solutions, business cases and decisions
Business Analysis and quality
System thinking and estimations

This course is intended to give you a general overview of what this discipline entails and can act as a springboard into more detailed study at a future date.
What do I learn?
According to the U.S. Bureau of Labor Statistics, a 19% job growth rate for Business Analysts is expected till 2022, and the outlook for the rest of the world should be higher still.

What are the underlying competencies of a Business Analyst?
The importance of stakeholder management within the business analysis domain.
Where within the enterprise can a business analyst be found?
In which sectors are business analyst competencies being practiced?
What is the importance of processes within the business analysis domain?
Why business transformation starts with an understanding of the current processes deployed within an organization.
What are some of the tools and techniques used to create, evaluate, modify and analyze processes?
What are organizational process assets and enterprise environmental factors?
What tools and techniques are used to create and analyze enterprise artifacts?
What is elicitation, and what techniques does a BA have at her disposal?
The importance of traceability within an organization.
What is the relationship between solutions and risks?
What elements should one consider when evaluating numerous solution possibilities?
What is the cost of quality?
Why is quality management important to a Business Analyst?
What quality management tools and techniques can a Business Analyst use?
What is the Deming Cycle?
How are the 7 Basic Quality Tools deployed?
What means of estimation does a BA have at her disposal?
Business Analysis, data and forecasting.
What is the theory of constraints?
Game theory, crowdsourcing and the Business Analyst.




            Read more
          



The first week, we look at what makes a Business Analyst; what are the underlying competencies, how a BA fits in the organization, the concept of stakeholders and organizational readiness, and lastly in which sectors those BA competencies can be found.
In week #2, we look at process analysis; the importance of process documentation, the relationship between processes and business transformation, process diagramming and process modeling.
Week 3 takes us to document analysis. We look at Organizational Process Assets and Enterprise Environmental Factors, Impact Analysis, business rules, traceability and other elements that prepare you to move on in your BA path.
Our fourth week takes us to the world of requirements gathering; what elicitation is, the different ways of performing the exercise and what steps need to be followed within requirements analysis. So that’s week 4, requirements gathering.
Week # 5 concerns all things tied to solutions; the relationship between a problem statement and a solution, the evaluation of multiple solutions, and how we pick the best one.
Our sixth week views the subject of quality management. What cost does quality have? What are the seven basic quality tools and of course what this all has to do with Business Analysis?
Our last week takes us into the realm of system thinking and estimation. How important is probability within our BA activities? What is the theory of constraints? We will perform a walkthrough on subject such as crowdsourcing and game theory, and that pretty much covers our last week."
https://www.classcentral.com/course/udacity-problem-solving-with-advanced-analytics-6458,"The Problem Solving with Analytics course provides students with the foundational knowledge to use data analytics to create business insights. You will learn:  To apply a useful framework to solve a business problem To determine which analytical method to apply given the nature of the problem and available data To use linear regression to generate business insights Throughout this course you’ll also learn the techniques to apply your knowledge in a data analytics program called Alteryx. At the end of the course, you’ll complete a project based on the principles in the course.This course is part of the Business Analyst Nanodegree.Why Take This Course?Using advanced analytics is less about being a statistics wiz and more about understanding how to approach problems and knowing what tools to use. In this course you will be introduced to predictive analytics, a powerful tool to help businesses analyze data and predict future outcomes and trends. You’ll learn a scientific approach to solving problems with data, a foundational skill for anyone interested in making data driven decisions in a business context. You'll be introduced Alteryx, a data analytics tool that enables you prepare, blend, and analyze data quickly. This course is ideal for anyone who is interested in pursuing a career in business analysis, but lacks programming experience.



Lesson 1 - The Analytical Problem Solving FrameworkIn this lesson you’ll learn a Problem Solving Framework that is useful to approaching any data analytics problem. The framework includes six key steps: business problem understanding, data understanding, data preparation, analysis and modeling, validation, and visualization. Lesson 2 - Selecting an Analytical MethodologyIn this lesson you’ll learn how to determine which analytical approach is most appropriate given the context of the problem and the nature of the available data. Lesson 3 - Linear RegressionIn this course you’ll learn apply the Problem Solving Framework to a business problem using linear regression, a predictive model used to predict numeric outcomes. You’ll first be introduced to regression using just one predictor variable, then how to add more variables to improve the accuracy of the model, and finally how to interpret and use the results."
https://www.classcentral.com/course/data-science-environmental-modelling-11806,"Discover how data science can help us understand environmental change
Environmental and climate change impact our lives, but what role does data play in informing us about such changes to our world? On this online course, we examine and explore the use of statistics and data science in better understanding the environment we live in.
You will develop data science skills learning from experts and completing hands-on modelling activities using real world environmental data and the powerful programming language R.
You will also consider how data can help plan the use of renewable energy resources such as wind power.
This course is for people with an interest in environment and/or renewable energy and who wish to gain new skills in data science. It will also be suitable for those with an interest in data science and who wish to learn more about applications in environment and renewable energy. You don’t need to be an expert in R to take this course."
https://www.classcentral.com/course/edx-mathematical-methods-for-quantitative-finance-18041,"Due to challenges arising from the COVID-19 pandemic, start dates for the MITx MicroMasters® Program in Finance courses have been postponed until Fall 2020.
Please note: the dates listed for each course are tentative and subject to change. If we make any further changes to the program schedule, we will share this information with you via email, and make it available on our website.
Modern finance is the science of decision making in an uncertain world, and its language is mathematics. As part of the MicroMasters® Program in Finance, this course develops the tools needed to describe financial markets, make predictions in the face of uncertainty, and find optimal solutions to business and investment decisions.
This course will help anyone seeking to confidently model risky or uncertain outcomes. Its topics are essential knowledge for applying the theory of modern finance to real-world settings. Quants, traders, risk managers, investment managers, investment advisors, developers, and engineers will all be able to apply these tools and techniques.
The course is excellent preparation for anyone planning to take the CFA exams.



Learning modules:


Probability: review of laws probability; common distributions of financial mathematics; CLT, LLN, characteristic functions, asymptotics. 


Statistics: statistical inference and hypothesis tests; time series tests and econometric analysis; regression methods 


Time-series models: random walks and Bernoulli trials; recursive calculations for Markov processes; basic properties of linear time series models (AR(p), MA(q), GARCH(1,1)); first-passage properties; applications to forecasting and trading strategies. 


Continuous time stochastic processes: continuous time limits of discrete processes; properties of Brownian motion; introduction to Itô calculus; solving differential equations of finance; applications to derivative pricing and risk management. 


Linear algebra: review of axioms and operations on linear spaces; covariance and correlation matrices; applications to asset pricing. 


Optimization: Lagrange multipliers and multivariate optimization; inequality constraints and quadratic programming; Markov decision processes and dynamic programming; variational methods; applications to portfolio construction, algorithmic trading, and best execution.| 


Numerical methods: Monte Carlo techniques; quadratic programming"
https://www.classcentral.com/course/swayam-business-analytics-for-management-decision-10050,"Students can exposure on data analysis, modeling and spreadsheet use with BUSINESS ANALYTICS for DECISION MAKING. This course will be exclusively quantitative and an application to business/ management related problems. It is connected with problem sets and real life cases to know the relevance of a particular problem and the decision making thereofINTENDED AUDIENCE : ManagementPREREQUISITES : Basic Statistics,Basic Mathematics,Basic ManagementINDUSTRY SUPPORT : Not applicable 
      


COURSE LAYOUT Week 1:Introduction to Business AnalyticsWeek 2:Exploring Data and Analytics on SpreadsheetsWeek 3:Descriptive AnalyticsWeek 4:Inferential Analytics 1Week 5:Inferential Analytics 2Week 6:Predictive Analytics 1Week 7:Predictive Analytics 2Week 8:Predictive Analytics 3Week 9:Prescriptive Analytics 1Week 10:Prescriptive Analytics 2Week 11:Prescriptive Analytics 3Week 12:Decision Analytics"
https://www.classcentral.com/course/big-data-r-hadoop-8056,"You will experience how to use RHadoop tool to manage and analyse big data.
This course will give you access to a virtual environment with installations of Hadoop, R and Rstudio to get hands-on experience with big data management. Several unique examples from statistical learning and related R code for map-reduce operations will be available for testing and learning.
Those with basic knowledge in statistical learning and R will better understand the methods behind and how to run them in parallel using map-reduce functions and Hadoop data storage. At the end of the course you will get access to RHadoop on a supercomputer at University of Ljubljana.
This course is designed for people interested in data science, computational statistics and machine learning and have basic experiences with them. It will be also useful for advanced undergraduate students and first year PhD students in data analysis, statistics or bioinformatics, who wish to understand how to manage big data with Hadoop using R programming language.
We expect that the learners will also have basic experiences with linux and bash and working experiences with R and matrix operations. They should be also capable to download and run virtual machine.
All software needed to actively participate the course is provided within the virtual machine that the followers are supposed to download and run on the local machine. No extra software is needed.
You will need a modest local machine with 15GB free disk space and 2GB RAM.



            Read more"
https://www.classcentral.com/course/big-data-decisions-5420,"##
Many datasets can provide solutions to important problems and inform decisions. However, the size, complexity, quality and diversity of these datasets make them difficult to process and analyse. Join us and we’ll share new technological or methodological solutions you can use to meet the demand for analytics in your field.
We will introduce big data and some of the statistical and mathematical approaches for analysing it. Then we explore the power of big data and the process of getting from data to decisions before you use some of the tools available for storing and managing large datasets.
Our course is open to anyone with an interest in big data and is essential if you’re looking to add big data analytics to your skill set. A basic knowledge of software engineering, statistics and mathematics will help you gain the most from this learning experience.
The course includes optional, practical exercises designed to help you become familiar with some of the current tools used in big data analytics. If you would like to try the exercises you will need to install Cloudera, an open source platform for data management and analytics. We will email you detailed instructions explaining how to access a trial version and set up a virtual machine before the course starts."
https://www.classcentral.com/course/swayam-business-analytics-and-data-mining-modeling-using-r-10073,"Objective of this course is to impart knowledge on use of data mining techniques for deriving business intelligence to achieve organizational goals. Use of R (statistical computingCSS - MOOCs Proposal software) to build, assess, and compare models based on real datasets and cases with an easy-to-follow learning curve.INTENDED AUDIENCE : NILLPREREQUISITES :Basic Statistics KnowledgeINDUSTRY SUPPORT : Big Data companies, Analytics & Consultancy companies, Companies with Analytics Division 
      


COURSE LAYOUT Week1:General Overview of Data Mining and its Components Introduction and Data Mining Process Introduction to R Basic Statistical TechniquesWeek2:Data Preparation and Exploration Visualization TechniquesWeek3:Data Preparation and Exploration Visualization Techniques Dimension Reduction Techniques Principal Component AnalysisWeek4:Performance Metrics and Assessment Performance Metrics for Prediction and ClassificationWeek5:Supervised Learning Methods Multiple Linear RegressionWeek6:Supervised Learning Methods Multiple Linear RegressionWeek7:Supervised Learning Methods NaÃ ̄ve BayesWeek8:Supervised Learning Methods Classification & Regression TreesWeek9:Supervised Learning Methods Classification & Regression TreesWeek10:Supervised Learning Methods Logistic RegressionWeek11:Supervised Learning Methods Logistic Regression Artificial Neural NetworksWeek12:Supervised Learning Methods and Wrap Up Artificial Neural Networks Discriminant Analysis Conclusion"
https://www.classcentral.com/course/swayam-quantitative-finance-3968,"The course covers the latest tools used in Quantitative Finance and covers the whole gamut of relevant topics like boot strapping, time series analysis, stochastic programming, analysis of panel data, etc. It will equip the students with necessary tools needed to understand how these different concepts may be used in the field of quantitative finance



          1.Microeconomics
2.Macroeconomics3.Statistics
4.Distribution 5.Sampling Techniques
 
6.Optimization
7.Advanced topics in Optimization
8.Time Series Analysis
9.Options & Dervatives"
https://www.classcentral.com/course/social-media-research-plan-13069,"Prepare to analyse social media
Analysing social media can be a complicated process. How do you plan your research? What methods can you use?
Get answers to these questions and more with this course. You will learn the basics of quantitive research to analyse social media. You will learn how to turn word-based questions into useful data. You will also explore how to turn hunches into testable propositions, concepts and variables.
This course is for professionals looking to advance their careers and learn more about social media and digital media analytics.
There is no special prior experience or knowledge required to participate. 
To make the most of this course, you will need to: ensure you have access to Excel and that you have a Twitter account; install or access TAGS- (https://tags.hawksey.info/get-tags/); use a laptop or desktop computer. You will also need to know some technology and statistics."
https://www.classcentral.com/course/data-analyst-nanodegree--nd002-18208,"Successful Data Analysts have a unique set of skills, and represent important value to organizations eager to make data-powered business decisions. In this program, you’ll learn to use Python, SQL, and statistics to uncover insights, communicate critical findings, and create data-driven solutions. Demand for qualified Data Analysts continues to rise, and as a graduate of this program, you will be prepared to take on these roles.
Use Python, SQL, and statistics to uncover insights, communicate critical findings, and create data-driven solutions.
      


           Prerequisite Knowledge You should have experience working with Python (specifically Numpy and Pandas) and SQL.See detailed requirements.Introduction to Data AnalysisLearn the data analysis process of wrangling, exploring, analyzing, and communicating data. Work with data in Python, using libraries like NumPy and Pandas.Explore Weather TrendsInvestigate a DatasetPractical StatisticsLearn how to apply inferential statistics and probability to real-world scenarios, such as analyzing A/B tests and building supervised learning models.Analyze Experiment ResultsData WranglingLearn the data wrangling process of gathering, assessing, and cleaning data. Learn to use Python to wrangle data programmatically and prepare it for analysis.Wrangle and Analyze DataData Visualization with PythonLearn to apply visualization principles to the data analysis process. Explore data visually at multiple levels to find insights and create a compelling story.Communicate Data Findings"
https://www.classcentral.com/course/edx-iot-programming-and-big-data-9753,"The Internet of Things is creating massive quantities of data, and managing and analysing it requires a unique approach to programming and statistics for distributed data sources.
This course will teach introductory programming concepts that allow connection to, and implementation of some functionality on, IoT devices, using the Python programming language. In addition, students will learn how to use Python to process text log files, such as those generated automatically by IoT sensors and other network-connected systems.
Learners do not need prior programming experience to undertake this course, and will not learn a specific programming language - however Python will be used for demonstrations. This course will focus on learning by working through realistic examples."
https://www.classcentral.com/course/edx-computational-thinking-and-big-data-8161,"Computational thinking is an invaluable skill that can be used across every industry, as it allows you to formulate a problem and express a solution in such a way that a computer can effectively carry it out.
In this course, part of the Big Data MicroMasters program, you will learn how to apply computational thinking in data science. You will learn core computational thinking concepts including decomposition, pattern recognition, abstraction, and algorithmic thinking.
You will also learn about data representation and analysis and the processes of cleaning, presenting, and visualizing data. You will develop skills in data-driven problem design and algorithms for big data.
The course will also explain mathematical representations, probabilistic and statistical models, dimension reduction and Bayesian models.
You will use tools such as R and Java data processing libraries in associated language environments.



Section 1: Data in R 
Identify the components of RStudio; Identify the subjects and types of variables in R; Summarise and visualise univariate data, including histograms and box plots. 
Section 2: Visualising relationships 
Produce plots in ggplot2 in R to illustrate the relationship between pairs of variables; Understand which type of plot to use for different variables; Identify methods to deal with large datasets. 
Section 3: Manipulating and joining data 
Organise different data types, including strings, dates and times; Filter subjects in a data frame, select individual variables, group data by variables and calculate summary statistics; Join separate dataframes into a single dataframe; Learn how to implement these methods in mapReduce. 
Section 4: Transforming data and dimension reduction 
Transform data so that it is more appropriate for modelling; Use various methods to transform variables, including q-q plots and Box-Cox transformation, so that they are distributed normally Reduce the number of variables using PCA; Learn how to implement these techniques into modelling data with linear models. 
Section 5: Summarising data 
Estimate model parameters, both point and interval estimates; Differentiate between the statistical concepts or parameters and statistics; Use statistical summaries to infer population characteristics; Utilise strings; Learn about k-mers in genomics and their relationship to perfect hash functions as an example of text manipulation. 
Section 6: Introduction to Java 
Use complex data structures; Implement your own data structures to organise data; Explain the differences between classes and objects; Motivate object-orientation. 
Section 7: Graphs 
Encode directed and undirected graphs in different data structures, such as matrices and adjacency lists; Execute basic algorithms, such as depth-first search and breadth-first search. 
Section 8: Probability 
Determine the probability of events occurring when the probability distribution is discrete; How to approximate. 
Section 9: Hashing 
Apply hash functions on basic data structures in Java; Implement your own hash functions and execute, these as well as built-in ones; Differentiate good from bad hash functions based on the concept of collisions. 
Section 10: Bringing it all together 
Understand the context of big data in programming."
https://www.classcentral.com/course/data-science-foundations-r-18756,"Ask the right questions, manipulate data sets, and create visualizations to communicate results. This Specialization covers foundational data science tools and techniques, including getting, cleaning, and exploring data, programming in R, and conducting reproducible research. Learners who complete this specialization will be prepared to take the Data Science: Statistics and Machine Learning specialization, in which they build a data product using real-world data. The five courses in this specialization are the very same courses that make up the first half of the Data Science Specialization. This specialization is presented for learners who want to start and complete the foundational part of the curriculum first, before moving onto the more advanced topics in Data Science: Statistics and Machine Learning.



          Course 1: The Data Scientist’s Toolbox- In this course you will get an introduction to the main tools and ideas in the data scientist's toolbox. The course gives an overview of the data, questions, and tools that data analysts and data scientists work with. There are two components to this course. The first is a conceptual introduction to the ideas behind turning data into actionable knowledge. The second is a practical introduction to the tools that will be used in the program like version control, markdown, git, GitHub, R, and RStudio.Course 2: R Programming- In this course you will learn how to program in R and how to use R for effective data analysis. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language. The course covers practical issues in statistical computing which includes programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting R code. Topics in statistical data analysis will provide working examples.Course 3: Getting and Cleaning Data- Before you can work with data you have to get some. This course will cover the basic ways that data can be obtained. The course will cover obtaining data from the web, from APIs, from databases and from colleagues in various formats. It will also cover the basics of data cleaning and how to make data “tidy”. Tidy data dramatically speed downstream data analysis tasks. The course will also cover the components of a complete data set including raw data, processing instructions, codebooks, and processed data. The course will cover the basics needed for collecting, cleaning, and sharing data.Course 4: Exploratory Data Analysis- This course covers the essential exploratory techniques for summarizing data. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data.Course 5: Reproducible Research- This course focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner. Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. This course will focus on literate statistical analysis tools which allow one to publish data analyses in a single document that allows others to easily execute the same analysis to obtain the same results."
https://www.classcentral.com/course/edx-statistical-predictive-modelling-and-applications-14422,"In this course, you will learn three predictive modelling techniques - linear and logistic regression, and naive Bayes - and their applications in real-world scenarios.
The first half of the course focuses on linear regression. This technique allows you to model a continuous outcome variable using both continuous and categorical predictors. This technique enables you to predict product sales based on several customer variables.
In the second half of the course, you will learn about logistic regression, which is the counterpart of linear regression, when the response variable is categorical. You will also be introduced to naive Bayes; a very intuitive, probabilistic modeling technique.



Week 1: Simple Linear Regression
Week 2: Multiple Linear Regression
Week 3: Extensions and Applications
Week 4: Introduction to Naive Bayes
Week 5: Logistic Regression
Week 6: Estimation and Comparison"
https://www.classcentral.com/course/swayam-data-analysis-decision-making-iii-13981,"This is the third part of the three part course (DADM-I, DADM-II, DADM-III) which covers ""Operations Research and its tools with applications"". In general Decision Analysis and Decision Making (DADM) covers three main areas which are: Multivariate Statistical Analysis with its applications, Other Decision Making Models like DEA, AHP, ANP, TOPSIS, etc., and Operations Research and its tools with applications. These three part DADM course will be more practical and application oriented rather than theoretical in nature.INTENDED AUDIENCE: Masters in Business Administration, Masters in Economics, Masters in Statistics/Mathematics, Masters in Industrial Engineering, Masters in Operations Research/Operations Management, PhD in related fields as mentioned abovePREREQUISITES: Probability & Statistics Operations ResearchINDUSTRY SUPPORT: Manufacturing industry, chemical industry, steel industry, cement industry, etc.



COURSE LAYOUT Week 1: Introduction, Ideas of Optimization and ModelingWeek 2: Linear Programming (LP) and related topicsWeek 3: Simplex Method, Interior point Method and related conceptsWeek 4: Non-Linear Programming (NLP)Week 5: Goal ProgrammingWeek 6: Stochastic Programming   Week 7: 0-1 Programming and other related methodsWeek 8: Polynomial OptimizationWeek 9: Reliability Based ProgrammingWeek 10: Robust OptimizationWeek 11: Other topics like Parametric programming, etcWeek 12: Multi-objective Programming"
https://www.classcentral.com/course/wharton-business-financial-modeling-18841,"Wharton's Business and Financial Modeling Specialization is designed to help you make informed business and financial decisions. These foundational courses will introduce you to spreadsheet models, modeling techniques, and common applications for investment analysis, company valuation, forecasting, and more. When you complete the Specialization, you'll be ready to use your own data to describe realities, build scenarios, and predict performance.



          Course 1: Fundamentals of Quantitative Modeling- How can you put data to work for you? Specifically, how can numbers in a spreadsheet tell us about present and past business activities, and how can we use them to forecast the future? The answer is in building quantitative models, and this course is designed to help you understand the fundamentals of this critical, foundational, business skill. Through a series of short lectures, demonstrations, and assignments, you’ll learn the key ideas and process of quantitative modeling so that you can begin to create your own models for your own business or enterprise. By the end of this course, you will have seen a variety of practical commonly used quantitative models as well as the building blocks that will allow you to start structuring your own models. These building blocks will be put to use in the other courses in this Specialization.Course 2: Introduction to Spreadsheets and Models- The simple spreadsheet is one of the most powerful data analysis tools that exists, and it’s available to almost anyone. Major corporations and small businesses alike use spreadsheet models to determine where key measures of their success are now, and where they are likely to be in the future. But in order to get the most out of a spreadsheet, you have the know-how to use it. This course is designed to give you an introduction to basic spreadsheet tools and formulas so that you can begin harness the power of spreadsheets to map the data you have now and to predict the data you may have in the future. Through short, easy-to-follow demonstrations, you’ll learn how to use Excel or Sheets so that you can begin to build models and decision trees in future courses in this Specialization. Basic familiarity with, and access to, Excel or Sheets is required.Course 3: Modeling Risk and Realities- Useful quantitative models help you to make informed decisions both in situations in which the factors affecting your decision are clear, as well as in situations in which some important factors are not clear at all. In this course, you can learn how to create quantitative models to reflect complex realities, and how to include in your model elements of risk and uncertainty. You’ll also learn the methods for creating predictive models for identifying optimal choices; and how those choices change in response to changes in the model’s assumptions. You’ll also learn the basics of the measurement and management of risk. By the end of this course, you’ll be able to build your own models with your own data, so that you can begin making data-informed decisions. You’ll also be prepared for the next course in the Specialization.Course 4: Decision-Making and Scenarios- This course is designed to show you how use quantitative models to transform data into better business decisions. You’ll learn both how to use models to facilitate decision-making and also how to structure decision-making for optimum results. Two of Wharton’s most acclaimed professors will show you the step-by-step processes of modeling common business and financial scenarios, so you can significantly improve your ability to structure complex problems and derive useful insights about alternatives. Once you’ve created models of existing realities, possible risks, and alternative scenarios, you can determine the best solution for your business or enterprise, using the decision-making tools and techniques you’ve learned in this course.Course 5: Wharton Business and Financial Modeling Capstone- In this Capstone you will recommend a business strategy based on a data model you’ve constructed. Using a data set designed by Wharton Research Data Services (WRDS), you will implement quantitative models in spreadsheets to identify the best opportunities for success and minimizing risk. Using your newly acquired decision-making skills, you will structure a decision and present this course of action in a professional quality PowerPoint presentation which includes both data and data analysis from your quantitative models. Wharton Research Data Services (WRDS) is the leading data research platform and business intelligence tool for over 30,000 corporate, academic, government and nonprofit clients in 33 countries. WRDS provides the user with one location to access over 200 terabytes of data across multiple disciplines including Accounting, Banking, Economics, ESG, Finance, Insurance, Marketing, and Statistics."
https://www.classcentral.com/course/business-analytics-forecasting-6739,"##
Companies, governments and other organizations now collect and analyze huge amounts of data about suppliers, clients, employees, citizens, transactions, and much more. There are a number of ways organizations can use this data. Business analytics uses this data to make better decisions and forecasting is an arm of this predictive analytics. Forecasting especially can provide a powerful toolkit for analyzing time series data.
Learn about forecasting in a wider context
Quantitative forecasting uses statistical and data mining methods to generating numerical forecasts, an important component of decision making across many business functions, including economic forecasting, workload projections, sales forecasts, and power and transportation demand. Today’s big data forecasting can include forecasting many series on a frequent basis, such as daily demand of thousands of products at retail chains, hourly statistics of wind turbines, minute-by-minute web traffic, and call volume to call centers.
Forecasting can also be combined with statistical monitoring methods for purposes of anomaly detection – for example, public health organizations collect and monitor clinical and other data for detecting disease outbreaks. Forecasting is also often combined with simulation for purposes of scenario building. On this course we’ll have a look at some of these uses in more depth as well as examining the processes that these different industries use.
Understand the forecasting process
This course focuses on forecasting time series, where past and present values are used to forecast future values of a series of interest. The course covers issues relating to different steps of the forecasting process, from goal definition, through data visualization, modeling, and performance evaluation to model deployment.
In this course you will:
Learn how to define a forecasting task and workflow
Understand how to evaluate forecasting performance
Apply and be familiar with popular forecasting methods
Explore, identify and model different types of patterns in time series
Be able to implement a forecasting process in practice
Familiarity with basic statistical methods including linear regression.
Basic knowledge of Excel and R software.



            Read more"
https://www.classcentral.com/course/mitx-finance-18293,"The skills and expertise required for a career in finance are in high demand across countless industries. From asset management, to corporations, to official institutions, the career opportunities for qualified finance professionals continue to grow and evolve. For example, demand for financial analysts is predicted to grow at a faster than average rate of 11% through 2026 (Source). And according to Glassdoor, the median salary of a quantitative financial analyst was $106,575. (Source)
The MITx MicroMasters® Program in Finance offers recent graduates, early to mid-stage professionals, and other individuals interested in pursuing a career in finance, an opportunity to advance in the finance field or fast-track an MIT Sloan Master of Finance through a rigorous, comprehensive online curriculum, delivered by the world-renowned MIT Sloan School of Management.
Drawn from the STEM-based curriculum taught on campus, all five online courses in this program mirror on-campus graduate-level MIT coursework and cover the following topics: modern finance, financial accounting, mathematical methods for quantitative finance, and derivatives markets. Learners who complete and pass each course in this online program may earn a MicroMasters program certificate in finance, and are considered affiliate members of the MIT Alumni Association. Those learners are eligible to apply to the MIT Sloan Master of Finance and upon acceptance, earn credit for the work performed online.



            Read more
          



Courses under this program:Course 1: Foundations of Modern Finance I
A mathematically rigorous framework to understand financial markets delivered with data-driven insights from MIT professors.
Course 2: Foundations of Modern Finance II
Learn fundamental principles of modern finance, including valuation models, methods for risk analysis, derivative instruments and investment management.
Course 3: Financial Accounting
How do investors, creditors, and other users analyze financial statements to assess corporate performance. Learn financial accounting, how to read financial statements, and input valuation models for better corporate finance decision-making.
Course 4: Mathematical Methods for Quantitative Finance
Learn the mathematical foundations essential for financial engineering and quantitative finance: linear algebra, optimization, probability, stochastic processes, statistics, and applied computational techniques in R.
Course 5: Derivatives Markets: Advanced Modeling and Strategies
Financial derivatives are ubiquitous in global capital markets. Students will obtain a sophisticated understanding of valuation methods; tools for quantifying, hedging, and speculating on risk; and a basic familiarity with major markets and instruments."
https://www.classcentral.com/course/genomic-data-science-18700,"With genomics sparks a revolution in medical discoveries, it becomes imperative to be able to better understand the genome, and be able to leverage the data and information from genomic datasets. Genomic Data Science is the field that applies statistics and data science to the genome. This Specialization covers the concepts and tools to understand, analyze, and interpret data from next generation sequencing experiments. It teaches the most common tools used in genomic data science including how to use the command line, along with a variety of software implementation tools like Python, R, Bioconductor, and Galaxy. This Specialization is designed to serve as both a standalone introduction to genomic data science or as a perfect compliment to a primary degree or postdoc in biology, molecular biology, or genetics, for scientists in these fields seeking to gain familiarity in data science and statistical tools to better interact with the data in their everyday work. To audit Genomic Data Science courses for free, visit https://www.coursera.org/jhu, click the course, click Enroll, and select Audit. Please note that you will not receive a Certificate of Completion if you choose to Audit.



          Course 1: Introduction to Genomic Technologies- This course introduces you to the basic biology of modern genomics and the experimental tools that we use to measure it. We'll introduce the Central Dogma of Molecular Biology and cover how next-generation sequencing can be used to measure DNA, RNA, and epigenetic patterns. You'll also get an introduction to the key concepts in computing and data science that you'll need to understand how data from next-generation sequencing experiments are generated and analyzed. This is the first course in the Genomic Data Science Specialization.Course 2: Genomic Data Science with Galaxy- Learn to use the tools that are available from the Galaxy Project. This is the second course in the Genomic Big Data Science Specialization.Course 3: Python for Genomic Data Science- This class provides an introduction to the Python programming language and the iPython notebook. This is the third course in the Genomic Big Data Science Specialization from Johns Hopkins University.Course 4: Algorithms for DNA Sequencing- We will learn computational methods -- algorithms and data structures -- for analyzing DNA sequencing data. We will learn a little about DNA, genomics, and how DNA sequencing is used. We will use Python to implement key algorithms and data structures and to analyze real genomes and DNA sequencing datasets.Course 5: Command Line Tools for Genomic Data Science- Introduces to the commands that you need to manage and analyze directories, files, and large sets of genomic data. This is the fourth course in the Genomic Big Data Science Specialization from Johns Hopkins University.Course 6: Bioconductor for Genomic Data Science- Learn to use tools from the Bioconductor project to perform analysis of genomic data. This is the fifth course in the Genomic Big Data Specialization from Johns Hopkins University.Course 7: Statistics for Genomic Data Science- An introduction to the statistics behind the most popular genomic data science projects. This is the sixth course in the Genomic Big Data Science Specialization from Johns Hopkins University.Course 8: Genomic Data Science Capstone- In this culminating project, you will deploy the tools and techniques that you've mastered over the course of the specialization. You'll work with a real data set to perform analyses and prepare a report of your findings."
https://www.classcentral.com/course/trendy-klassifikatsii-9446,"В этом курсе мы поговорим о трендах и классификаторах. Анализ трендов помогает ответить на вопросы вроде: растут ли продажи, увеличивается ли количество пользователей сервиса? Если есть рост, то случайность это или закономерность? Есть ли в данных сезонные колебания? Как выделить тренд и как объяснить его? 
Также мы поговорим о факторном анализе, который позволяет найти скрытую переменную (или переменные), направляющие проявление множества видимых признаков. Как найти такие скрытые переменные и понять, что за ними стоит?
В заключительной части курса поговорим о классификаторах, применение которых решает задачи отнесения объектов к тому или иному классу с определенной вероятностью, а также позволяет прогнозировать попадание нового объекта в определенный класс. Как предсказать исход события, зная основные характеристики действующего лица? Закончит ли слушатель курс, отдаст ли заемщик кредит? Как оценить точность прогноза и минимизировать ошибки? 
Мы разберемся с устройством обозначенных методов анализа данных и попрактикуемся в их применении.
      


          Анализ временных рядов
    -В этом модуле мы начнем разговор о временных рядах. 
Сначала разберемся с понятием временного ряда, затем поговорим об анализе временных рядов. Рассмотрим такие компоненты временного ряда, как тренд, сезонность и остатки. 
После этого рассмотрим методы разложения временного ряда на составляющие и поймём, как и зачем выделять описанные компоненты во временных рядах. В заключении поговорим о том, как выявлять выбросы в данных, а также посмотрим на практике, как разложить временной ряд на трендовую составляющую, сезонную компоненту и остатки, используя R. 

Прогноз временных рядов
    -В этом модуле мы продолжим разговор о временных рядах и научимся не только анализировать, но и прогнозировать их. Сначала рассмотрим авторегрессионную модель (AR) и сезонную авторегрессионную модель (SAR), которые подходят для решения задач прогнозирования, а также модели скользящего среднего (MA-модели), позволяющие сглаживать выбросы и описывать данные. Дальше поговорим о комбинации этих моделей (ARMA и ARIMA). 
Во второй части модуля мы поговорим об адаптивных моделях, обсудим их основные виды, а также поговорим о следящем контроле как инструменте их мониторинга. В заключении модуля попрактикуемся: построим прогноз временного ряда в R. 

Факторный анализ
    -В этом модуле поговорим о факторном анализе. Сначала поймем общий принцип: что это за модель, и для решения каких задач она применяется. Дальше разберем методы факторного анализа и научимся строить факторы одним из самых распространенных способов: методом главных компонент. В заключительных лекциях модуля мы поговорим о том, как оценить качество факторной модели, как можно использовать построенные переменные для дальнейшего анализа, а также пошагово разберем построение факторной модели в SPSS.

Классификация
    -В завершающем модуле курса мы поговорим о методах классификации. Для начала поставим задачу классификации: для чего применяются классификаторы, какие задачи из реальной жизни они помогают решать. Затем разберем некоторые методы классификации: линейный и Байесовский классификаторы, дерево решений, модель бинарной логистической регрессии и способы оценки её качества.
Вы научитесь прогнозировать класс, в который попадёт объект с заданной вероятностью (к примеру, отдаст ли заёмщик кредит, или закончит ли студент курс), а также познакомитесь с тем, как применять методы классификации в R и SPSS на реальных данных.

Итоговое задание"
https://www.classcentral.com/course/advanced-machine-learning-10341,"Discover and apply advanced statistical machine learning techniques
This online course explores advanced statistical machine learning.
You will discover where machine learning techniques are used in the data science project workflow. You will then look in detail at supervised learning statistical modeling algorithms for classification and regression problems, examining how these algorithms are related, and how models generated by them can be tuned and evaluated.
You will also look at feature engineering and how to analyse sufficiency of data.
This is an advanced course and some experience with machine learning, data science or statistical modeling is expected. Links will be provided to basic resources about assumed knowledge.
Sections of the course make use of advanced mathematics, including statistics, linear algebra, calculus and information theory. If you have prior knowledge of these areas, particularly the first two, you will obtain additional insights into the methods used. If you do not have this prior knowledge, you will still be able to achieve the learning outcomes of the course.
The course uses R. If you have not programmed with R before, you should consider taking a quick introductory course, such as Try R."
https://www.classcentral.com/course/more-data-mining-with-weka-8389,"Become an experienced data miner
This course introduces advanced data mining skills, following on from Data Mining with Weka. You’ll process a dataset with 10 million instances. You’ll mine a 250,000-word text dataset. You’ll analyze a supermarket dataset representing 5000 shopping baskets. You’ll learn about filters for preprocessing data, selecting attributes, classification, clustering, association rules, cost-sensitive evaluation. You’ll meet learning curves and automatically optimize learning parameters.  Weka originated at the University of Waikato in NZ, and Ian Witten has authored a leading book on data mining.
This course is aimed at anyone who deals in data. It follows on from Data Mining with Weka, and you should have completed that first (or have otherwise acquired a rudimentary knowledge of Weka). As with the previous course, it involves no computer programming, although you need some experience with using computers for everyday tasks. High-school maths is more than enough; some elementary statistics concepts (means and variances) are assumed.
Before the course starts, download the free Weka software. It runs on any computer, under Windows, Linux, or Mac. It has been downloaded millions of times and is being used all around the world.
(Note: Depending on your computer and system version, you may need admin access to install Weka.)"
https://www.classcentral.com/course/udacity-data-science-interview-prep-11458,"Data science job interviews  can be daunting. Technical interviewers often ask you to design an experiment or model. You may need to solve problems using Python and SQL. You will likely need to show how you connect data skills to business decisions and strategy.In this course, you'll review the common questions asked in data science, data analyst, and machine learning interviews. You'll learn how to answer machine learning questions about predictions, underfitting and overfitting. You'll walk through typical data analyst questions about statistics and probability. Then, you'll dive deeper into the data structures and algorithms you need to know. You'll also learn tips for answering questions like, ""Tell me about one of your recent projects."" At the end of the course, you'll have a chance to practice what you've learned. You'll receive a link for unlimited mock interviews on Pramp. Practice the skills you need to show up for your data science interview with confidence!Why Take This Course?Knowing what to expect and practicing are keys to technical interview success. But, knowing the right answer is only part of the process. You also need to show how you tackle problems and communicate your thinking. In this course, you'll learn how to approach an interview not as a test, but a showcase. You'll learn to prepare not through memorization, but through process. You'll learn tips for remaining calm so you can answer with confidence to show your expertise.This course will help you prepare and practice for your data science interview. Experienced data scientists will walk you through clear steps for answering tough questions. They'll share their tips for how to respond when you are nervous or don't know the answer. Then, you'll have an opportunity to practice what you've learned in mock interviews.Udacity partners with tech industry leaders to bring you the most comprehensive resources for your job search. Join this course if you want to be in the driver’s seat of your job search where you decide which roles to interview for and land those interviews!



            Read more"
https://www.classcentral.com/course/quantitative-investing-8006,"##
Financial markets have become increasingly complex - it’s not easy to know where to start when it comes to investing. In today’s world the demand of accurate data-driven quantitative analysis across the world is steadily rising: it’s become crucial to understand and be able to use statistical and mathematical information accurately and promptly.
This course will teach you the essentials of modern investment theory and help you learn to apply them in real life using financial data and programming.
Understand modern investment theory
On the course you’ll start by learning about key concepts in modern investment theory and quantitative investing like return, risk and portfolio optimisation.
Get used to statistical techniques and practice using programming language R
Once you’ve learnt basic investment concepts we’ll look at applying them using statistics and programming using open source language R will be introduced. You’ll practice by doing assignments every week using actual securities data from Yahoo Finance.
Discover how to construct an investment portfolio
Using your new knowledge of quantitative investing and your ability to analyse investment characteristics using programming, you’ll be able to build your own diversified investment portfolio based on return analysis.
Learn from an expert in the field
Through the course you’ll be taught by an educator who has worked in quantitative finance for more than 15 years in Wall Street global investment banks, before she joined SKKU in 2015. She was Chief Investment Officer at a hedge fund and head of systematic trading groups at global banks such as Citi and J.P. Morgan.
You’ll need to have studied maths to a high school level, but other than that there are no special requirements for this course - you don’t need any experience in programming or investing.



            Read more"
https://www.classcentral.com/course/six-sigma-fundamentals-18775,"This specialization is for you if you are looking to learn more about Six Sigma or refresh your knowledge of the basic components of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes, performance, and quality assurance. In this specialization, you will learn proven principles and tools specific to six sigma and lean. This is a sequential, linear designed specialization that covers the introductory level content (at the ""yellow belt"" level) of Six Sigma and Lean. Yellow Belt knowledge is needed before advancing to Green Belt (which is a second specialization offered here on Coursera by the USG). Green Belt knowledge is needed before moving to a Black Belt. The proper sequence of this specialization is: Course #1 - Six Sigma Fundamentals Course #2 - Six Sigma Tools for Define and Measure Course #3 - Six Sigma Tools for Analyze Course #4 - Six Sigma Tools for Improve and Control At the end of Course #4 (Six Sigma Tools for Improve and Control), there is a peer-reviewed, capstone project. Successful completion of this project is necessary for full completion of this specialization. It should be noted that completing either the Yellow Belt or Green Belt Specializations does not give the learner ""professional accreditation"" in Six Sigma. However, successful completion will assist in better preparation for such professional accreditation testing.



          Course 1: Six Sigma Principles - This course is for you if you are looking to learn more about Six Sigma or refresh your knowledge of the basic components of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will introduce you to the purpose of Six Sigma and its value to an organization. You will learn about the basic principles of Six Sigma and Lean. Your instructors will introduce you to, and have you apply, some of the tools and metrics that are critical components of Six Sigma. This course will provide you with the basic knowledge of the principles, roles, and responsibilities of Six Sigma and Lean. Every module will include readings, videos, and a quiz to help make sure you understand the material and concepts that are studied. You will also have the opportunity to participate in discussions and peer review exercises to give you the opportunity to apply the material to your daily life. Our applied curriculum is built around the latest handbook The Certified Six Sigma Handbook (2nd edition) and students will develop /learn the fundamentals of Six Sigma. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Handbook (2nd edition). The companion text is not required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).Course 2: Six Sigma Tools for Define and Measure- This course is for you if you are looking to learn more about Six Sigma or refresh your knowledge of the basic components of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will cover the Define phase and introduce you to the Measure phase of the DMAIC (Define, Measure, Analyze, Improve, and Control) process. You will learn about Six Sigma project development and implementation, you will become familiar with project management tools, you will be introduced to statistics and understand its significance to Six Sigma, and finally you will learn about data collection and its importance to an organization. Every module will include readings, videos, and a quiz to help make sure you understand the material and concepts that are studied. You will also have the opportunity to participate in peer review exercises to give you the opportunity to apply the material to your daily life. Our applied curriculum is built around the latest handbook The Certified Six Sigma Handbook (2nd edition) and students will develop /learn the fundamentals of Six Sigma. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Handbook (2nd edition). The companion text is not required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).Course 3: Six Sigma Tools for Analyze- This course will cover the Measure phase and portions of the Analyze phase of the Six Sigma DMAIC (Define, Measure, Analyze, Improve, and Control) process. You will learn about lean tools for process analysis, failure mode and effects analysis (FMEA), measurement system analysis (MSA) and gauge repeatability and reproducibility (GR&R), and you will be introduced to basic statistics. This course will outline useful measure and analysis phase tools and will give you an overview of statistics as they are related to the Six Sigma process. The statistics module will provide you with an overview of the concepts and you will be given multiple example problems to see how to apply these concepts. Every module will include readings, discussions, lecture videos, and quizzes to help make sure you understand the material and concepts that are studied. Our applied curriculum is built around the latest handbook The Certified Six Sigma Handbook (2nd edition) and students will develop /learn the fundamentals of Six Sigma. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Handbook (2nd edition). The companion text is not required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).Course 4: Six Sigma Tools for Improve and Control- This course will provide you will the tools necessary to complete the final components of the analyze phase as well as the improve and control phases of the Six Sigma DMAIC (Define, Measure, Analyze, Improve, and Control) process. This course is the final course in the Six Sigma Yellow Belt Specialization. You will learn about relationships from data using correlation and regression as well as the different hypothesis terms in hypothesis testing. This course will provide you with tools and techniques for improvement. You will also understand the importance of a control plan, as well as its key characteristics, for maintaining process improvements. Every module will include readings, discussions, lecture videos, and quizzes to help make sure you understand the material and concepts that are studied. Our applied curriculum is built around the latest handbook The Certified Six Sigma Handbook (2nd edition) and students will develop /learn the fundamentals of Six Sigma. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Handbook (2nd edition). The companion text is not required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ)."
https://www.classcentral.com/course/strategic-analytics-18835,"This specialization is designed for students, business analysts, and data scientists who want to apply statistical knowledge and techniques to business contexts. We recommend that you have some background in statistics, R or another programming language, and familiarity with databases and data analysis techniques such as regression, classification, and clustering.We’ll cover a wide variety of analytics approaches in different industry domains. You’ll engage in hands-on case studies in real business contexts: examples include predicting and forecasting events, statistical customer segmentation, and calculating customer scores and lifetime value. We’ll also teach you how to take these analyses and effectively present them to stakeholders so your business can take action. The third course and the Capstone Project are designed in partnership with Accenture, one of the world’s best-known consulting, technology services, and outsourcing companies. You’ll learn about applications in a wide variety of sectors, including media, communications, public service,etc. By the end of this specialization, you’ll be able to use statistical techniques in R to develop business intelligence insights, and present them in a compelling way to enable smart and sustainable business decisions. You’ll earn a Specialization Certificate from one of the world’s leading business schools and learn from two of Europe’s leading professors in business analytics and marketing.



            Read more
          



          Course 1: Foundations of strategic business analytics- Who is this course for? This course is designed for students, business analysts, and data scientists who want to apply statistical knowledge and techniques to business contexts. For example, it may be suited to experienced statisticians, analysts, engineers who want to move more into a business role. You will find this course exciting and rewarding if you already have a background in statistics, can use R or another programming language and are familiar with databases and data analysis techniques such as regression, classification, and clustering. However, it contains a number of recitals and R Studio tutorials which will consolidate your competences, enable you to play more freely with data and explore new features and statistical functions in R. With this course, you’ll have a first overview on Strategic Business Analytics topics. We’ll discuss a wide variety of applications of Business Analytics. From Marketing to Supply Chain or Credit Scoring and HR Analytics, etc. We’ll cover many different data analytics techniques, each time explaining how to be relevant for your business. We’ll pay special attention to how you can produce convincing, actionable, and efficient insights. We'll also present you with different data analytics tools to be applied to different types of issues. By doing so, we’ll help you develop four sets of skills needed to leverage value from data: Analytics, IT, Business and Communication. By the end of this MOOC, you should be able to approach a business issue using Analytics by (1) qualifying the issue at hand in quantitative terms, (2) conducting relevant data analyses, and (3) presenting your conclusions and recommendations in a business-oriented, actionable and efficient way. Prerequisites : 1/ Be able to use R or to program 2/ To know the fundamentals of databases, data analysis (regression, classification, clustering) We give credit to Pauline Glikman, Albane Gaubert, Elias Abou Khalil-Lanvin (Students at ESSEC BUSINESS SCHOOL) for their contribution to this course design.Course 2: Foundations of marketing analytics- Who is this course for? This course is designed for students, business analysts, and data scientists who want to apply statistical knowledge and techniques to business contexts. For example, it may be suited to experienced statisticians, analysts, engineers who want to move more into a business role, in particular in marketing. You will find this course exciting and rewarding if you already have a background in statistics, can use R or another programming language and are familiar with databases and data analysis techniques such as regression, classification, and clustering. However, it contains a number of recitals and R Studio tutorials which will consolidate your competences, enable you to play more freely with data and explore new features and statistical functions in R. Business Analytics, Big Data and Data Science are very hot topics today, and for good reasons. Companies are sitting on a treasure trove of data, but usually lack the skills and people to analyze and exploit that data efficiently. Those companies who develop the skills and hire the right people to analyze and exploit that data will have a clear competitive advantage. It's especially true in one domain: marketing. About 90% of the data collected by companies today are related to customer actions and marketing activities.The domain of Marketing Analytics is absolutely huge, and may cover fancy topics such as text mining, social network analysis, sentiment analysis, real-time bidding, online campaign optimization, and so on. But at the heart of marketing lie a few basic questions that often remain unanswered: (1) who are my customers, (2) which customers should I target and spend most of my marketing budget on, and (3) what's the future value of my customers so I can concentrate on those who will be worth the most to the company in the future. That's exactly what this course will cover: segmentation is all about understanding your customers, scorings models are about targeting the right ones, and customer lifetime value is about anticipating their future value. These are the foundations of Marketing Analytics. And that's what you'll learn to do in this course.Course 3: Case studies in business analytics with ACCENTURE- Who is this course for ? This course is RESTRICTED TO LEARNERS ENROLLED IN Strategic Business Analytics SPECIALIZATION as a preparation to the capstone project. During the first two MOOCs, we focused on specific techniques for specific applications. Instead, with this third MOOC, we provide you with different examples to open your mind to different applications from different industries and sectors. The objective is to give you an helicopter overview on what's happening in this field. You will see how the tools presented in the two previous courses of the Specialization are used in real life projects. We want to ignite your reflection process. Hence, you will best make use of the Accenture cases by watching first the MOOC and then investigate by yourself on the different concepts, industries, or challenges that are introduced during the videos. At the end of this course learners will be able to: - identify the possible applications of business analytics, - hence, reflect on the possible solutions and added-value applications that could be proposed for their capstone project. The cases will be presented by senior practitioners from Accenture with different backgrounds in term of industry, function, and country. Special attention will be paid to the ""value case"" of the issue raised to prepare you for the capstone project of the specialization. About Accenture Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions—underpinned by the world’s largest delivery network—Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With more than 358,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.Course 4: Capstone: Create Value from Open Data- The Capstone project is an individual assignment. Participants decide the theme they want to explore and define the issue they want to solve. Their “playing field” should provide data from various sectors (such as farming and nutrition, culture, economy and employment, Education & Research, International & Europe, Housing, Sustainable, Development & Energies, Health & Social, Society, Territories & Transport). Participants are encouraged to mix the different fields and leverage the existing information with other (properly sourced) open data sets. Deliverable 1 is the preliminary preparation and problem qualification step. The objectives is to define the what, why & how. What issue do we want to solve? Why does it promise value for public authorities, companies, citizens? How do we want to explore the provided data? For Deliverable 2, the participant needs to present the intermediary outputs and adjustments to the analysis framework. The objectives is to confirm the how and the relevancy of the first results. Finally, with Deliverable 3, the participant needs to present the final outputs and the value case. The objective is to confirm the why. Why will it create value for public authorities, companies, and citizens. Assessment and grading: the participants will present their results to their peers on a regular basis. An evaluation framework will be provided for the participants to assess the quality of each other’s deliverables."
https://www.classcentral.com/course/mitx-supply-chain-management-18308,"Gain expertise in the growing field of Supply Chain Management through an innovative online program consisting of five courses and a final capstone exam. The MicroMasters Program in Supply Chain from MITx is an advanced, professional, graduate-level foundation in Supply Chain Management. It represents the equivalent of one semester's worth of coursework at MIT.
The MicroMasters program certificate will showcase your understanding of supply chain analytics, design, technology, dynamics and end-to-end supply chain management. Build on the program certificate and take advantage of a great opportunity to be accepted into the #1 ranked supply chain management Masters Degree program for a fraction of the cost.



Courses under this program:Course 1: Supply Chain Comprehensive Exam
Take the Comprehensive Exam in Supply Chain Management to earn the MITx MicroMasters credential.
Course 2: Supply Chain Analytics
Master and apply the core methodologies used in supply chain analysis and modeling, including statistics, regression, optimization and probability ​- part of the MITx Supply Chain Management MicroMasters Credential.
Course 3: Supply Chain Fundamentals
Learn fundamental concepts for logistics and supply chain management from both analytical and practical perspectives – part of the MITx MicroMasters Credential in Supply Chain Management.
Course 4: Supply Chain Design
Learn how to design and optimize the physical, financial, and information flows of a supply chain to enhance business performance ​– part of the MITx MicroMasters Credential in Supply Chain Management​.
Course 5: Supply Chain Dynamics
Learn how to manage and harness the dynamics and interactions between firms and entities within a supply chain - ​part of the MITx Supply Chain Management MicroMasters Credential.
Course 6: Supply Chain Technology and Systems
Learn how technology is used in supply chain systems from fundamental concepts to innovative applications - ​part of the MITx Supply Chain Management MicroMasters Credential."
https://www.classcentral.com/course/data-visualization-nanodegree--nd197-18198,"Communicating effectively is one of the most important skills needed today, and every business is collecting data to make informed decisions. Build on your data or business background to drive data-driven recommendations. Whether you are a data analyst looking to communicate more effectively, or a business leader looking to build data literacy, you will finish this program able to use data effectively in visual stories and presentations.
Combine data, visuals, and narrative to tell impactful stories and make data-driven decisions.
      


           Prerequisite Knowledge To be successful in this program, you should have basic statistics and data analysis skills.See detailed requirements.Intro to Data VisualizationLearn how to select the most appropriate data visualization for an analysis, evaluate the effectiveness of a data visualization, and build interactive and engaging Tableau dashboards.Build Data DashboardsDashboard DesignsDesign and create a dashboard in an enterprise environment. Discover user needs, identify key metrics, and tailor your dashboard to a particular audience. Design a Data DashboardData StorytellingLearn the end to end process for telling a story and providing a recommendation based on data. You'll define an effective problem statement, structure a data presentation, scope analyses, identify biases and limitations within your dataset, and pull together an end-to-end analysis.Build a Data StoryAdvanced Data StorytellingLearn advanced data visualization and storytelling techniques. Learn to use Tableau Storypoint to add interactivity and other visual elements to a story, and add animation and narration with Tableau Pages and Flourish. Animate a Data Story"
https://www.classcentral.com/course/marketing-analytics-nanodegree--nd028-18206,"This program is for anyone who wishes to gain foundational data skills that apply to marketing. You’ll learn to collect, organize, and analyze data with Excel; build Excel models to analyze possible business outcomes, and visualize and communicate your findings using Tableau and Data Studios. You’ll also gain valuable skills in Google Analytics. With analysts in high demand, this program is an ideal choice whether you’re just getting started with marketing or data, are interested in applying data skills to your current role, or plan to pursue further studies or career goals.
Gain foundational data skills applicable to marketing. Collect and analyze data, model marketing scenarios, and communicate your findings with Excel, Tableau, Google Analytics, and Data Studios.
      


           Prerequisite Knowledge No prerequisites.Introduction to Data AnalysisLearn how to use statistics and visuals to find and communicate insights. Develop Excel skills to manipulate, analyze, and visualize data in a spreadsheet. Build Excel models to analyze possible business outcomes.Interpret a Data VisualizationAnalyze Survey DataData VisualizationLearn to apply design and visualization principles to create impactful data visualizations, build data dashboards, and tell stories with data.Storytelling With DataBuild Data DashboardsGoogle AnalyticsIn this course you’ll acquire in-depth knowledge of Google Analytics, as you learn to use advanced reporting techniques, analyze and optimize results, build fluency with Data Studio, and produce actionable insights that power significant business growth.Advanced Displays, Segments & ViewsNavigating, Reports, & DashboardsMarketing AnalyticsIn this course you will learn about a wide range of marketing and business metrics, and how to evaluate the growth and health of your marketing efforts.Crafting an Analytic BriefCreate a Campaign Report"
https://www.classcentral.com/course/practical-data-science-matlab-18864,"Do you find yourself in an industry or field that increasingly uses data to answer questions? Are you working with an overwhelming amount of data and need to make sense of it? Do you want to avoid becoming a full-time software developer or statistician to do meaningful tasks with your data?

Completing this specialization will give you the skills and confidence you need to achieve practical results in Data Science quickly. Being able to visualize, analyze, and model data are some of the most in-demand career skills from fields ranging from healthcare, to the auto industry, to tech startups.

This specialization assumes you have domain expertise in a technical field and some exposure to computational tools, such as spreadsheets. To be successful in completing the courses, you should have some background in basic statistics (histograms, averages, standard deviation, curve fitting, interpolation).

Throughout this specialization, you will be using MATLAB. MATLAB is the go-to choice for millions of people working in engineering and science, and provides the capabilities you need to accomplish your data science tasks. You will be provided with free access to MATLAB for the duration of the specialization to complete your work.
      


          Course 1: Exploratory Data Analysis with MATLAB- In this course, you will learn to think like a data scientist and ask questions of your data. You will use interactive features in MATLAB to extract subsets of data and to compute statistics on groups of related data. You will learn to use MATLAB to automatically generate code so you can learn syntax as you explore. You will also use interactive documents, called live scripts, to capture the steps of your analysis, communicate the results, and provide interactive controls allowing others to experiment by selecting groups of data. These skills are valuable for those who have domain knowledge and some exposure to computational tools, but no programming background is required. To be successful in this course, you should have some knowledge of basic statistics (e.g., histograms, averages, standard deviation, curve fitting, interpolation). By the end of this course, you will be able to load data into MATLAB, prepare it for analysis, visualize it, perform basic computations, and communicate your results to others. In your last assignment, you will combine these skills to assess damages following a severe weather event and communicate a polished recommendation based on your analysis of the data. You will be able to visualize the location of these events on a geographic map and create sliding controls allowing you to quickly visualize how a phenomenon changes over time.Course 2: Data Processing and Feature Engineering with MATLAB- In this course, you will build on the skills learned in Exploratory Data Analysis with MATLAB to lay the foundation required for predictive modeling. This intermediate-level course is useful to anyone who needs to combine data from multiple sources or times and has an interest in modeling. These skills are valuable for those who have domain knowledge and some exposure to computational tools, but no programming background. To be successful in this course, you should have some background in basic statistics (histograms, averages, standard deviation, curve fitting, interpolation) and have completed Exploratory Data Analysis with MATLAB. Throughout the course, you will merge data from different data sets and handle common scenarios, such as missing data. In the last module of the course, you will explore special techniques for handling textual, audio, and image data, which are common in data science and more advanced modeling. By the end of this course, you will learn how to visualize your data, clean it up and arrange it for analysis, and identify the qualities necessary to answer your questions. You will be able to visualize the distribution of your data and use visual inspection to address artifacts that affect accurate modeling.Course 3: Predictive Modeling and Machine Learning with MATLAB- In this course, you will build on the skills learned in Exploratory Data Analysis with MATLAB and Data Processing and Feature Engineering with MATLAB to increase your ability to harness the power of MATLAB to analyze data relevant to the work you do. These skills are valuable for those who have domain knowledge and some exposure to computational tools, but no programming background. To be successful in this course, you should have some background in basic statistics (histograms, averages, standard deviation, curve fitting, interpolation) and have completed courses 1 through 2 of this specialization. By the end of this course, you will use MATLAB to identify the best machine learning model for obtaining answers from your data. You will prepare your data, train a predictive model, evaluate and improve your model, and understand how to get the most out of your models.Course 4: Data Science Project: MATLAB for the Real World- Like most subjects, practice makes perfect in Data Science. In the capstone project, you will apply the skills learned across courses in the Practical Data Science with MATLAB specialization to explore, process, analyze, and model data. You will choose your own pathway to answer key questions with the provided data. To complete the project, you must have mastery of the skills covered in other courses in the specialization. The project will test your ability to import and explore your data, prepare the data for analysis, train a predictive model, evaluate and improve your model, and communicate your results."
https://www.classcentral.com/course/swayam-behavioral-and-personal-finance-17529,"This course will cover the behavioral aspects of financial decision making and personal finance planning. The students shall be introduced to the theoretical, mathematical, and empirical underpinnings of anomalies and biases that investors face in financial markets. The course also focuses the behavioral approach of investment and personal financial planning.
Through this course, we do not hope to make you ‘financially literate’ or advise you what to do with your money. Instead, the course will help you explore some of the most common biases and mistakes that we, as individuals, make while dealing with money (or something of that sort). With the help of discussions on related theories, mathematical illustrations, and experimental exercises, participants should be able to become familiar with terminology, techniques and approaches used in behavioralized financial services industry. Most of techniques can also be relevant to traditional financial advisory services, the products and services in the fintech domain, and public policy.
 
 
INTENDED AUDIENCE : • BTech/BE student with a basic knowledge of Economics and Statistics
• Students of MBA/MCom/MA (Eco)/BBA/BMS
PREREQUISITES : Nil
INDUSTRY SUPPORT : Companies from BFSI (Banking, Financial services & Insurance) sector:Edelweiss Fin. Services Ltd.,
Barclays India, Yes Bank, CFA Institute, Capital First Ltd., Nomura, and other financial advisory firms,
including investment banking companies such as JPMC, DB, etc.

      


            Read more
          



COURSE LAYOUT
Week 1 : Introduction to behavioral economics and finance: the concept of expected utility, the vonNeumann Morgenstern framework.
Week 2 : Non-expected utility preferences and its applications in finance.
Week 3 :Beliefs, biases and heuristics in financial markets.
Week 4 :Basics of personal finance, financial planning, and budgeting.
Week 5 :Investment decision making and behavioral finance
Week 6 :Investment strategies for individual investors.
Week 7 : Purchasing decisions, consumer credit and related issues.
Week 8 :Alternative investment and structured finance."
https://www.classcentral.com/course/uc-san-diegox-data-science-18326,"Excel in Data Science, one of the hottest fields in tech today. Learn how to gain new insights from big data by asking the right questions, manipulating data sets and visualizing your findings in compelling ways.
In this MicroMasters program, you will develop a well-rounded understanding of the mathematical and computational tools that form the basis of data science and how to use those tools to make data-driven business recommendations.
This MicroMasters program encompasses two sides of data science learning: the mathematical and the applied.
Mathematical courses cover probability, statistics, and machine learning. The applied courses cover the use of specific toolkit and languages such as Python, Numpy, Matplotlib, pandas and Scipy, the Jupyter notebook environment and Apache Spark to delve into real world data.
You will learn how to collect, clean and analyse big data using popular open source software will allow you to perform large-scale data analysis and present your findings in a convincing, visual way. When combined with expertise in a particular type of business, it will make you a highly desirable employee.



Courses under this program:Course 1: Machine Learning Fundamentals
Understand machine learning's role in data-driven modeling, prediction, and decision-making.
Course 2: Python for Data Science
Learn to use powerful, open-source, Python tools, including Pandas, Git and Matplotlib, to manipulate, analyze, and visualize complex datasets.
Course 3: Probability and Statistics in Data Science using Python
Using Python, learn statistical and probabilistic approaches to understand and gain insights from data.
Course 4: Big Data Analytics Using Spark
Learn how to analyze large datasets using Jupyter notebooks, MapReduce and Spark as a platform."
https://www.classcentral.com/course/swayam-six-sigma-12995,"The course on Six-Sigma will focus on detailed strategic and operational issues of process improvement and variation reduction. Six-sigma is a measure of quality that strives for near perfection. It is a disciplined, data-driven approach for eliminating defects (driving towards six standard deviations between the mean and the nearest specification limit) in any process-from manufacturing to transactional and from product to service. 
A Six-sigma defect is anything outside of customer specifications. To be tagged Six Sigma, a process must not produce more than 3.4 defects per million opportunities. 
Six-sigma employs a systematic approach of DMAIC (Define, Measure, Analyze, Improve and Control) for the process improvement. This course will provide a detailed understanding on various issues specific to each phase of DMAIC. 
The course is designed with a practical orientation and includes cases, industry examples and MINITAB software applications.  
The course is designed to satisfy the need of both industry professionals and University students.
The content is beneficial to both manufacturing and service industry.
      


Week 1  : QUALITY: FUNDAMENTALS AND KEY CONCEPTSLecture 1: Brief overview of the course

Lecture 2: Quality concepts and definition
Lecture 3: History of continuous improvement
Lecture 4: Six Sigma Principles and Focus Areas (Part 1)
Lecture 5: Six Sigma Principles and Focus Areas (Part 2)
Lecture 6: Six Sigma Applications
Week 2 :  QUALITY: FUNDAMENTALS AND KEY CONCEPTSLecture 7: Quality Management: Basics and Key Concepts 
Lecture 8: Fundamentals of Total Quality Management
Lecture 9: Cost of quality
Lecture 10: Voice of customer 
Lecture 11: Quality Function Deployment (QFD)
Lecture 12: Management and Planning Tools (Part 1)
Lecture 13: Management and Planning Tools (Part 2)
Week 3  : DEFINELecture 14: Six Sigma Project Identification, Selection and Definition
Lecture 15: Project Charter and Monitoring
Lecture 16: Process characteristics and analysis
Lecture 17: Process Mapping: SIPOC
Week 4 : MEASURE Lecture 18: Data Collection and Summarization (Part 1)
Lecture 19: Data Collection and Summarization (Part 2)
Lecture 20: Measurement systems: Fundamentals
Lecture 21: Measurement systems analysis: Gage R&R study
Lecture 22: Fundamentals of statistics
Lecture 23: Probability theory
Week 5  : MEASURE Lecture 24: Process capability analysis: Key Concepts
Lecture 25: Process capability analysis: Measures and Indices 
Lecture 26: Process capability analysis: Minitab Application
Lecture 27: Non-normal process capability analysis
Week 6  :  ANALYZE Lecture 28: Hypothesis testing: Fundamentals
Lecture 29: Hypothesis Testing: Single Population Test
Lecture 30: Hypothesis Testing: Two Population Test
Lecture 31: Hypothesis Testing: Two Population: Minitab Application
Lecture 32: Correlation and Regression Analysis
Lecture 33: Regression Analysis: Model Validation
Week 7  :   ANALYZE Lecture 34: One-Way ANOVA

Lecture 35: Two-Way ANOVA
Lecture 36: Multi-vari Analysis
Lecture 37: Failure Mode Effect Analysis (FMEA)
Week 8  :  IMPROVELecture 38: Introduction to Design of Experiment
Lecture 39: Randomized Block Design
Lecture 40: Randomized Block Design: Minitab Application
Lecture 41: Factorial Design
Lecture 42: Factorial Design: Minitab Application
Week 9  :  IMPROVELecture 43: Fractional Factorial Design
Lecture 44: Fractional Factorial Design: Minitab Application
Lecture 45: Taguchi Method: Key Concepts
Lecture 46: Taguchi Method: Illustrative Application
Week 10  :  CONTROL Lecture 47: Seven QC Tools
Lecture 48: Statistical Process Control: Key Concepts
Lecture 49: Statistical Process Control: Control Charts for Variables
Lecture 50: Operating Characteristic (OC) Curve for Variable Control charts
Lecture 51: Statistical Process Control: Control Charts for Attributes
Lecture 52: Operating Characteristic (OC) Curve for Attribute Control charts
Lecture 53: Statistical Process Control: Minitab Application
Week 11  :  CONTROL Lecture 54: Acceptance Sampling: Key Concepts
Lecture 55: Design of Acceptance Sampling Plans for Attributes (Part 1)
Lecture 56: Design of Acceptance Sampling Plans for Attributes (Part 2)
Lecture 57: Design of Acceptance Sampling Plans for Variables 
Lecture 58: Acceptance Sampling: Minitab Application
Week 12  : SIX SIGMA IMPLEMENTATION CHALLENGESLecture 59: Design for Six Sigma (DFSS): DMADV, DMADOV
Lecture 60: Design for Six Sigma (DFSS): DFX
Lecture 61: Team Management
Lecture 62: Six Sigma: Case study
Lecture 63: Six Sigma: Summary of key concepts"
https://www.classcentral.com/course/swayam-financial-derivatives-risk-management-14056,"Regulatory reforms across the world are gradually being introduced to reduce trade impediments between nations and usher in free market based pricing. Cross border investments through direct/portfolio routes are also being enticed as a medium for funding of growth and developmental activities. In addition, the governments of developing nations continue to pursue their strategy of partial privatization of the frontier sectors in an attempt to raise revenues for the exchequer as well as reduce operational losses with increased efficiency. Under these stimuli, scientific risk management by the investor fraternity becomes of cardinal necessity for generating competitive returns and surviving in the marketplace. Derivatives have proven to be immensely useful in the management of financial risk. Their vitality can be gauged from the exponential growth in trading volumes as well as the advent of new structured products literally on a day to day basis. Derivatives in petroleum and natural gas industries in the United States are, now, well entrenched, and they are being extensively used in the electricity industry as well.Traditional courses on derivatives can be classified almost exclusively into those: (i) that provide a comprehensive coverage of the underlying mathematical models using stochastic calculus and develop the subject as an extension of probabilistic mathematics e.g. mathematical finance and (ii) that cover the theme purely at a superficial level focusing on the operating aspects like exchange trading methodologies, marking and margining aspects etc. They consciously avoid entering the mathematical/stochastic structure that forms the very basis of the pricing and applications of these instruments.The first set is aimed at students who have had a sound grounding in mathematics and statistics and are inclined to study derivatives as an application of the theory of stochastic processes as part of their graduate degree in statistics. The other set caters to the needs of undergraduates/graduates in commerce or management who know very little about the mathematics of derivatives but study derivatives to work on the front office interfaces or trading terminals in broker houses or other market players.The fallout of this mutually exclusive segmentation is that both segments do not cover derivatives as a cogent wholesome. They deliver the content in the asymptote rather than as a mainstream course. This course fights that trend by covering in detail the topics that are thrown by the wayside in the traditional coverage. It provides valuable insights into the underlying financial The fallout of this mutually exclusive segmentation is that both segments do not cover derivatives as a cogent wholesome. They deliver the content in the asymptote rather than as a mainstream course. This course fights that trend by covering in detail the topics that are thrown by the wayside in the traditional coverage. It provides valuable insights into the underlying financial.INTENDED AUDIENCE: The audience would comprise of those desirous of get acquainted with the intricacies of derivativespricing, their strategizing and their applications as hedging instruments and also, appreciating the nuances that have led to the origin and extensive development of this field of knowledgePREREQUISITES:i) Basics of finance,(ii) Senior school mathematics (algebra, calculus & probability). INDUSTRY SUPPORT: This course will attract immense recognition in the entire financial services industry including banks,stock & commodity exchanges, stock & commodity brokers, portfolio managers, investment bankers,market regulators etc. Those employed in corporate finance shall also find it valuable as it would add to their versatility. Academicians will find it a gateway to further work in related areas. 
      


            Read more
          



COURSE LAYOUT Week 1: Overview of Derivatives; Forwards: Introduction & Pricing, Arbitrage, Forwards Pricing on Consumption Assets; Futures: Introduction & Salient Features.Week 2:Futures: Margining & MTM, Forwards & Futures Prices, Exposure & Risk, Basics of Futures Hedging, Nuances in Futures Hedging.Week 3:Further Aspects of Futures Hedging; Basics of Mean-Variance Portfolio Theory & CAPM; Systematic & Unsystematic Risk. Week 4:Index Futures: Features, Hedging & Arbitrage; Basics of Interest Rates, YTM & Other Yield Measures.Week 5:Interest Rate Risk & Its Measurement; Interest Rate Futures: Features of IRFs, Hedging of Interest Rate Risk.Week 6:T-Bill & Eurodollar Futures, T-Bond Futures; Tailing the Hedge; Basic Theory of Options.  Week 7:Options: Price Bounds, Put-Call Parity; American Options; Trading Strategies.Week 8:Option Spread Strategies; Stochastic Processes: Basic Theory, Brownian Motion, Diffusion Equation, Central Limit Theorem.Week 9:Ito’s Equation; Stock Price Distribution, Fokker Planck Equation; Option Pricing: Binomial Model.Week 10:Girsanov Theorem; Black Scholes Model; Option Greeks.Week 11:Option Greeks: Further Properties, Role in Trading; FRAs & Swaps.Week 12:Valuation of Swaps; Value at Risk."
https://www.classcentral.com/course/tumx-six-sigma-and-lean-18396,"Learn how to drive quality and productivity programs by mastering the powerful analytical and management tools of the Six Sigma methodology and the revolutionary concepts of Lean Management.
Six Sigma and Lean enable organisations to measure and analyse production processes, to eliminate waste and to evolve their management structures in order to motivate employees and improve quality and productivity.
The series of courses is provided in collaboration with the TUM School of Management Executive Education Center and the Chair of Production and Supply Chain Management.
In this Professional Certificate program, you will learn the fundamentals of the Six Sigma methodology and Lean Manufacturing. You will learn the DMAIC (Define, Measure, Analyse, Improve, Control) process improvement cycle and examine how the principles of Lean production improve quality and productivity and enable organisational transformation.
You will be challenged to learn both the quantitative and qualitative methods associated with Six Sigma and Lean, including:

problem definition;
baseline performance measurement and process capability;
measurement system analysis;
root cause analysis;
regression and correlation;
design of experiments;
control charts implementation.

Both descriptive and inferential statistics will be applied and all concepts are exercised using online problems and interactive exercises and case studies.
Upon successful completion of this program, learners will earn the TUM Lean and Six Sigma Yellow Belt certification, confirming mastery of Lean Six Sigma fundamentals to a Yellow Belt level. The material is based on the American Society of Quality (www.asq.org) Body of Knowledge up to a Green Belt Level. The Professional Certificate is designed as preparation for a Lean Six Sigma Green Belt exam.
A TUM Lean Six Sigma Yellow Belt Certification will help you advance your career, increase your salary earnings and improve your organisation through your mastery of quality skills.
NOTE: In order to achieve the TUM Six Sigma and Lean Yellow Belt Certification it is MANDATORY to complete all 3 courses mentioned below, pursuing all 3 Verified Certificates. Then, automatically, you will earn the Professional Certificate. Courses can be taken in any order, but since Six Sigma: Analyse, Improve, Control builds on the material learned in Six Sigma: Define and Measure, we recommend you to take this course before Six Sigma: Analyse, Improve, Control.



            Read more
          



Courses under this program:Course 1: Six Sigma: Define and MeasureAn introduction to the Six Sigma methodology and DMAIC cycle for process improvement with a focus on the Define and Measure phases, including basic statistics for understanding sampling plans and calculating process capability.Course 2: Six Sigma: Analyze, Improve, ControlLearn how to statistically analyse process data to determine the root cause for process problems, to propose solutions, and to implement quality management tools, such as 8D and the 5 Whys, as well as the concept of Design for Six Sigma (DFSS).Course 3: Lean ProductionLearn how to apply key elements of Lean Production, from minimizing inventory and reducing setup times, to using 5S and Kaizen, in order to improve quality and productivity in your workplace."
https://www.classcentral.com/course/swayam-dealing-with-materials-data-collection-analysis-and-interpretation-17565,"This course is an introductory course with hands on sessions in R on some basic aspects of materials data. The course will cover all aspects, namely, data collection, analysis and interpretation. All the concepts will be covered with materials data and the hands-on sessions will be conducted using R programming language.
 
 
INTENDED AUDIENCE : Students of materials science, materials engineering, metallurgy, ceramics engineering, polymers and any other student (such as physics, chemistry, mechanical engineering etc) who is interested in materials data
PREREQUISITES : An exposure to R is preferable; a basic engineering mathematics or mathematical methods course is preferable
INDUSTRY SUPPORT : Nil

      


COURSE LAYOUT
Week 1 : Introduction: basic probability and statistics
Week 2 : Introduction: basic R
Week 3 : Presenting data: inaccurcacies and error and its propagation
Week 4 : R for descriptive data analysis
Week 5 : Probability distributions
Week 6 : Probability distributions using R
Week 7 : Processing of experimental data using R
Week 8 : Fitting functions to data: regression, testing significance of fit
Week 9 : R for graphical handling of data and fitting
Week 10 : Basics of design of experiments
Week 11 : Bayesian inference and its uses
Week 12 : Case studies using R"
https://www.classcentral.com/course/microsoft-data-science-fundamentals-18461,"Please note on June 30, 2020, this program will be retiring and no longer available on edX. If you are interested in earning the Professional Certificate you must be complete the program by June 30, 2020, in order to earn the certificate. 
Accelerate your career in one of the hottest fields – data science. Learn data science fundamentals, including data queries, data analysis, data visualization and how statistics informs data science practices.
Built in collaboration with leading universities and employers, this Professional Certificate program will develop the analytical skills you need to take advantage of the 1.5 million career opportunities available now in data science.



Courses under this program:Course 1: Analyzing and Visualizing Data with Power BI
Learn Power BI, a powerful cloud-based service that helps data scientists visualize and share insights from their data.
Course 2: Introduction to Data Science
Get started on your Data Science journey.
Course 3: Analytics Storytelling for Impact
Learn the art and science of data storytelling and achieve greater analytics impact.
Course 4: Ethics and Law in Data and Analytics
Analytics and AI are powerful tools that have real-word outcomes. Learn how to apply practical, ethical, and legal constructs and scenarios so that you can be an effective analytics professional."
https://www.classcentral.com/course/data-science-python-18916,"The 5 courses in this University of Michigan specialization introduce learners to data science through the python programming language. This skills-based specialization is intended for learners who have a basic python or programming background, and want to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular python toolkits such as pandas, matplotlib, scikit-learn, nltk, and networkx to gain insight into their data.

Introduction to Data Science in Python (course 1), Applied Plotting, Charting & Data Representation in Python (course 2), and Applied Machine Learning in Python (course 3) should be taken in order and prior to any other course in the specialization. After completing those, courses 4 and 5 can be taken in any order. All 5 are required to earn a certificate.
      


          Course 1: Introduction to Data Science in Python- This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses. This course should be taken before any of the other Applied Data Science with Python courses: Applied Plotting, Charting & Data Representation in Python, Applied Machine Learning in Python, Applied Text Mining in Python, Applied Social Network Analysis in Python.Course 2: Applied Plotting, Charting & Data Representation in Python- This course will introduce the learner to information visualization basics, with a focus on reporting and charting using the matplotlib library. The course will start with a design and information literacy perspective, touching on what makes a good and bad visualization, and what statistical measures translate into in terms of visualizations. The second week will focus on the technology used to make visualizations in python, matplotlib, and introduce users to best practices when creating basic charts and how to realize design decisions in the framework. The third week will be a tutorial of functionality available in matplotlib, and demonstrate a variety of basic statistical charts helping learners to identify when a particular method is good for a particular problem. The course will end with a discussion of other forms of structuring and visualizing data. This course should be taken after Introduction to Data Science in Python and before the remainder of the Applied Data Science with Python courses: Applied Machine Learning in Python, Applied Text Mining in Python, and Applied Social Network Analysis in Python.Course 3: Applied Machine Learning in Python- This course will introduce the learner to applied machine learning, focusing more on the techniques and methods than on the statistics behind these methods. The course will start with a discussion of how machine learning is different than descriptive statistics, and introduce the scikit learn toolkit through a tutorial. The issue of dimensionality of data will be discussed, and the task of clustering data, as well as evaluating those clusters, will be tackled. Supervised approaches for creating predictive models will be described, and learners will be able to apply the scikit learn predictive modelling methods while understanding process issues related to data generalizability (e.g. cross validation, overfitting). The course will end with a look at more advanced techniques, such as building ensembles, and practical limitations of predictive models. By the end of this course, students will be able to identify the difference between a supervised (classification) and unsupervised (clustering) technique, identify which technique they need to apply for a particular dataset and need, engineer features to meet that need, and write python code to carry out an analysis. This course should be taken after Introduction to Data Science in Python and Applied Plotting, Charting & Data Representation in Python and before Applied Text Mining in Python and Applied Social Analysis in Python.Course 4: Applied Text Mining in Python- This course will introduce the learner to text mining and text manipulation basics. The course begins with an understanding of how text is handled by python, the structure of text both to the machine and to humans, and an overview of the nltk framework for manipulating text. The second week focuses on common manipulation needs, including regular expressions (searching for text), cleaning text, and preparing text for use by machine learning processes. The third week will apply basic natural language processing methods to text, and demonstrate how text classification is accomplished. The final week will explore more advanced methods for detecting the topics in documents and grouping them by similarity (topic modelling). This course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python.Course 5: Applied Social Network Analysis in Python- This course will introduce the learner to network analysis through tutorials using the NetworkX library. The course begins with an understanding of what network analysis is and motivations for why we might model phenomena as networks. The second week introduces the concept of connectivity and network robustness. The third week will explore ways of measuring the importance or centrality of a node in a network. The final week will explore the evolution of networks over time and cover models of network generation and the link prediction problem. This course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python."
https://www.classcentral.com/course/machine-learning-reinforcement-finance-18807,"The main goal of this specialization is to provide the knowledge and practical skills necessary to develop a strong foundation on core paradigms and algorithms of machine learning (ML), with a particular focus on applications of ML to various practical problems in Finance. The specialization aims at helping students to be able to solve practical ML-amenable problems that they may encounter in real life that include: (1) mapping the problem on a general landscape of available ML methods, (2) choosing particular ML approach(es) that would be most appropriate for resolving the problem, and (3) successfully implementing a solution, and assessing its performance. The specialization is designed for three categories of students: · Practitioners working at financial institutions such as banks, asset management firms or hedge funds · Individuals interested in applications of ML for personal day trading · Current full-time students pursuing a degree in Finance, Statistics, Computer Science, Mathematics, Physics, Engineering or other related disciplines who want to learn about practical applications of ML in Finance. The modules can also be taken individually to improve relevant skills in a particular area of applications of ML to finance.



          Course 1: Guided Tour of Machine Learning in Finance- This course aims at providing an introductory and broad overview of the field of ML with the focus on applications on Finance. Supervised Machine Learning methods are used in the capstone project to predict bank closures. Simultaneously, while this course can be taken as a separate course, it serves as a preview of topics that are covered in more details in subsequent modules of the specialization Machine Learning and Reinforcement Learning in Finance. The goal of Guided Tour of Machine Learning in Finance is to get a sense of what Machine Learning is, what it is for and in how many different financial problems it can be applied to. The course is designed for three categories of students: Practitioners working at financial institutions such as banks, asset management firms or hedge funds Individuals interested in applications of ML for personal day trading Current full-time students pursuing a degree in Finance, Statistics, Computer Science, Mathematics, Physics, Engineering or other related disciplines who want to learn about practical applications of ML in Finance Experience with Python (including numpy, pandas, and IPython/Jupyter notebooks), linear algebra, basic probability theory and basic calculus is necessary to complete assignments in this course.Course 2: Fundamentals of Machine Learning in Finance- The course aims at helping students to be able to solve practical ML-amenable problems that they may encounter in real life that include: (1) understanding where the problem one faces lands on a general landscape of available ML methods, (2) understanding which particular ML approach(es) would be most appropriate for resolving the problem, and (3) ability to successfully implement a solution, and assess its performance. A learner with some or no previous knowledge of Machine Learning (ML) will get to know main algorithms of Supervised and Unsupervised Learning, and Reinforcement Learning, and will be able to use ML open source Python packages to design, test, and implement ML algorithms in Finance. Fundamentals of Machine Learning in Finance will provide more at-depth view of supervised, unsupervised, and reinforcement learning, and end up in a project on using unsupervised learning for implementing a simple portfolio trading strategy. The course is designed for three categories of students: Practitioners working at financial institutions such as banks, asset management firms or hedge funds Individuals interested in applications of ML for personal day trading Current full-time students pursuing a degree in Finance, Statistics, Computer Science, Mathematics, Physics, Engineering or other related disciplines who want to learn about practical applications of ML in Finance Experience with Python (including numpy, pandas, and IPython/Jupyter notebooks), linear algebra, basic probability theory and basic calculus is necessary to complete assignments in this course.Course 3: Reinforcement Learning in Finance- This course aims at introducing the fundamental concepts of Reinforcement Learning (RL), and develop use cases for applications of RL for option valuation, trading, and asset management. By the end of this course, students will be able to - Use reinforcement learning to solve classical problems of Finance such as portfolio optimization, optimal trading, and option pricing and risk management. - Practice on valuable examples such as famous Q-learning using financial problems. - Apply their knowledge acquired in the course to a simple model for market dynamics that is obtained using reinforcement learning as the course project. Prerequisites are the courses ""Guided Tour of Machine Learning in Finance"" and ""Fundamentals of Machine Learning in Finance"". Students are expected to know the lognormal process and how it can be simulated. Knowledge of option pricing is not assumed but desirable.Course 4: Overview of Advanced Methods of Reinforcement Learning in Finance- In the last course of our specialization, Overview of Advanced Methods of Reinforcement Learning in Finance, we will take a deeper look into topics discussed in our third course, Reinforcement Learning in Finance. In particular, we will talk about links between Reinforcement Learning, option pricing and physics, implications of Inverse Reinforcement Learning for modeling market impact and price dynamics, and perception-action cycles in Reinforcement Learning. Finally, we will overview trending and potential applications of Reinforcement Learning for high-frequency trading, cryptocurrencies, peer-to-peer lending, and more. After taking this course, students will be able to - explain fundamental concepts of finance such as market equilibrium, no arbitrage, predictability, - discuss market modeling, - Apply the methods of Reinforcement Learning to high-frequency trading, credit risk peer-to-peer lending, and cryptocurrencies trading."
https://www.classcentral.com/course/data-collection-18801,"This specialization covers the fundamentals of surveys as used in market research, evaluation research, social science and political research, official government statistics, and many other topic domains. In six courses, you will learn the basics of questionnaire design, data collection methods, sampling design, dealing with missing values, making estimates, combining data from different sources, and the analysis of survey data. In the final Capstone Project, you’ll apply the skills learned throughout the specialization by analyzing and comparing multiple data sources. Faculty for this specialisation comes from the Michigan Program in Survey Methodology and the Joint Program in Survey Methodology, a collaboration between the University of Maryland, the University of Michigan, and the data collection firm Westat, founded by the National Science Foundation and the Interagency Consortium of Statistical Policy in the U.S. to educate the next generation of survey researchers, survey statisticians, and survey methodologists. In addition to this specialization we offer short courses, a summer school, certificates, master degrees as well as PhD programs.



          Course 1: Framework for Data Collection and Analysis- This course will provide you with an overview over existing data products and a good understanding of the data collection landscape. With the help of various examples you will learn how to identify which data sources likely matches your research question, how to turn your research question into measurable pieces, and how to think about an analysis plan. Furthermore this course will provide you with a general framework that allows you to not only understand each step required for a successful data collection and analysis, but also help you to identify errors associated with different data sources. You will learn some metrics to quantify each potential error, and thus you will have tools at hand to describe the quality of a data source. Finally we will introduce different large scale data collection efforts done by private industry and government agencies, and review the learned concepts through these examples. This course is suitable for beginners as well as those that know about one particular data source, but not others, and are looking for a general framework to evaluate data products.Course 2: Data Collection: Online, Telephone and Face-to-face- This course presents research conducted to increase our understanding of how data collection decisions affect survey errors. This is not a “how–to-do-it” course on data collection, but instead reviews the literature on survey design decisions and data quality in order to sensitize learners to how alternative survey designs might impact the data obtained from those surveys. The course reviews a range of survey data collection methods that are both interview-based (face-to-face and telephone) and self-administered (paper questionnaires that are mailed and those that are implemented online, i.e. as web surveys). Mixed mode designs are also covered as well as several hybrid modes for collecting sensitive information e.g., self-administering the sensitive questions in what is otherwise a face-to-face interview. The course also covers newer methods such as mobile web and SMS (text message) interviews, and examines alternative data sources such as social media. It concentrates on the impact these techniques have on the quality of survey data, including error from measurement, nonresponse, and coverage, and assesses the tradeoffs between these error sources when researchers choose a mode or survey design.Course 3: Questionnaire Design for Social Surveys- This course will cover the basic elements of designing and evaluating questionnaires. We will review the process of responding to questions, challenges and options for asking questions about behavioral frequencies, practical techniques for evaluating questions, mode specific questionnaire characteristics, and review methods of standardized and conversational interviewing.Course 4: Sampling People, Networks and Records- Good data collection is built on good samples. But the samples can be chosen in many ways. Samples can be haphazard or convenient selections of persons, or records, or networks, or other units, but one questions the quality of such samples, especially what these selection methods mean for drawing good conclusions about a population after data collection and analysis is done. Samples can be more carefully selected based on a researcher’s judgment, but one then questions whether that judgment can be biased by personal factors. Samples can also be draw in statistically rigorous and careful ways, using random selection and control methods to provide sound representation and cost control. It is these last kinds of samples that will be discussed in this course. We will examine simple random sampling that can be used for sampling persons or records, cluster sampling that can be used to sample groups of persons or records or networks, stratification which can be applied to simple random and cluster samples, systematic selection, and stratified multistage samples. The course concludes with a brief overview of how to estimate and summarize the uncertainty of randomized sampling.Course 5: Dealing With Missing Data- This course will cover the steps used in weighting sample surveys, including methods for adjusting for nonresponse and using data external to the survey for calibration. Among the techniques discussed are adjustments using estimated response propensities, poststratification, raking, and general regression estimation. Alternative techniques for imputing values for missing items will be discussed. For both weighting and imputation, the capabilities of different statistical software packages will be covered, including R®, Stata®, and SAS®.Course 6: Combining and Analyzing Complex Data- In this course you will learn how to use survey weights to estimate descriptive statistics, like means and totals, and more complicated quantities like model parameters for linear and logistic regressions. Software capabilities will be covered with R® receiving particular emphasis. The course will also cover the basics of record linkage and statistical matching—both of which are becoming more important as ways of combining data from different sources. Combining of datasets raises ethical issues which the course reviews. Informed consent may have to be obtained from persons to allow their data to be linked. You will learn about differences in the legal requirements in different countries.Course 7: Survey Data Collection and Analytics Project (Capstone)- The Capstone Project offers qualified learners to the opportunity to apply their knowledge by analyzing and comparing multiple data sources on the same topic. Students will develop a research question, access and analyze relevant data, and critically examine the quality of each data source. At the completion of this capstone, students will have demonstrated hands-on data analysis capability, evaluated the quality of different data sources using the Total Survey Error approach, involving at least some of the following: comparing weighted non-probability samples to data collected from probability samples, using sampling techniques to correct for coverage errors, and tracking and assess the ease of using an online questionnaire that you implement."
https://www.classcentral.com/course/mathematics-for-data-science-18856,"Behind numerous standard models and constructions in Data Science there is mathematics that makes things work. It is important to understand it to be successful in Data Science. In this specialisation we will cover wide range of mathematical tools and see how they arise in Data Science. We will cover such crucial fields as Discrete Mathematics, Calculus, Linear Algebra and Probability. To make your experience more practical we accompany mathematics with examples and problems arising in Data Science and show how to solve them in Python.
      


          Course 1: Discrete Math and Analyzing Social Graphs- The main goal of this course is to introduce topics in Discrete Mathematics relevant to Data Analysis. We will start with a brief introduction to combinatorics, the branch of mathematics that studies how to count. Basics of this topic are critical for anyone working in Data Analysis or Computer Science. We will illustrate new knowledge, for example, by counting the number of features in data or by estimating the time required for a Python program to run. Next, we will apply our knowledge in combinatorics to study basic Probability Theory. Probability is everywhere in Data Analysis and we will study it in much more details later. Our goals for probability section in this course will be to give initial flavor of this field. Finally, we will study the combinatorial structure that is the most relevant for Data Analysis, namely graphs. Graphs can be found everywhere around us and we will provide you with numerous examples. We will mainly concentrate in this course on the graphs of social networks. We will provide you with relevant notions from the graph theory, illustrate them on the graphs of social networks and will study their basic properties. In the end of the course we will have a project related to social network graphs. As prerequisites we assume only basic math (e.g., we expect you to know what is a square or how to add fractions), basic programming in Python (functions, loops, recursion), common sense and curiosity. Our intended audience are all people that work or plan to work in Data Analysis, starting from motivated high school students.Course 2: Calculus and Optimization for Machine Learning- Hi! Our course aims to provide necessary background in Calculus sufficient for up-following Data Science courses. Course starts with basic introduction to concepts concerning functional mappings. Later students are assumed to study limits (in case of sequences, single- and multivariate functions), differentiability (once again starting from single variable up to multiple cases), integration, thus sequentially building up a base for the basic optimisation. To provide an understanding of the practical skills set being taught, the course introduces the final programming project considering the usage of optimisation routine in machine learning. Additional materials provided during the course include interactive plots in GeoGebra environment used during lectures, bonus reading materials with more general methods and more complicated basis for discussed themes.Course 3: First Steps in Linear Algebra for Machine Learning- The main goal of the course is to explain the main concepts of linear algebra that are used in data analysis and machine learning. Another goal is to improve the student’s practical skills of using linear algebra methods in machine learning and data analysis. You will learn the fundamentals of working with data in vector and matrix form, acquire skills for solving systems of linear algebraic equations and finding the basic matrix decompositions and general understanding of their applicability. This course is suitable for you if you are not an absolute beginner in Matrix Analysis or Linear Algebra (for example, have studied it a long time ago, but now want to take the first steps in the direction of those aspects of Linear Algebra that are used in Machine Learning). Certainly, if you are highly motivated in study of Linear Algebra for Data Sciences this course could be suitable for you as well.Course 4: Probability Theory, Statistics and Exploratory Data Analysis- Exploration of Data Science requires certain background in probability and statistics. This course introduces you to the necessary sections of probability theory and statistics, guiding you from the very basics all way up to the level required for jump starting your ascent in Data Science. The core concept of the course is random variable — i.e. variable whose values are determined by random experiment. Random variables are used as a model for data generation processes we want to study. Properties of the data are deeply linked to the corresponding properties of random variables, such as expected value, variance and correlations. Dependencies between random variables are crucial factor that allows us to predict unknown quantities based on known values, which forms the basis of supervised machine learning. We begin with the notion of independent events and conditional probability, then introduce two main classes of random variables: discrete and continuous and study their properties. Finally, we learn different types of data and their connection with random variables. While introducing you to the theory, we'll pay special attention to practical aspects for working with probabilities, sampling, data analysis, and data visualization in Python. This course requires basic knowledge in Discrete mathematics (combinatorics) and calculus (derivatives, integrals)."
https://www.classcentral.com/course/business-analytics-nanodegree--nd098-18221,"This program is for anyone who wishes to gain foundational data skills that are applicable to virtually all fields. You’ll learn to collect, organize, and analyze data with Excel and SQL; build Excel models to analyze possible business outcomes; and visualize and communicate your findings using Tableau. This program is an ideal choice whether you’re just getting started with data, are interested in applying data skills to your current role, or plan to pursue further studies or career goals.Gain foundational data skills applicable to any industry. Collect and analyze data, model business scenarios, and communicate your findings with SQL, Excel, and Tableau.



Prerequisite Knowledge
This is an introductory program and has no prerequisites. In order to succeed, we recommend having experience using a computer and being able to download and install applications.See detailed requirements.


Welcome to the Program
Meet your instructors and learn how to optimize your classroom. Learn how people use data to answer questions, and find your own insights from a data dashboard.
Interpret a Data Visualization


Introduction to Data
Learn how to use statistics and visuals to find and communicate insights. Develop Excel skills to manipulate, analyze, and visualize data in a spreadsheet. Build Excel models to analyze possible business outcomes.
Analyze NYSE Data


SQL for Data Analysis
Learn to use Structured Query Language (SQL) to extract and analyze data stored in databases.
Query a Digital Music Store Database


Data Visualization
Learn to apply design and visualization principles to create impactful data visualizations, build data dashboards, and tell stories with data.
Build Data Dashboards"
https://www.classcentral.com/course/clinical-data-science-18836,"Are you interested in how to use data generated by doctors, nurses, and the healthcare system to improve the care of future patients? If so, you may be a future clinical data scientist! This specialization provides learners with hands on experience in use of electronic health records and informatics tools to perform clinical data science. This series of six courses is designed to augment learner’s existing skills in statistics and programming to provide examples of specific challenges, tools, and appropriate interpretations of clinical data. By completing this specialization you will know how to: 1) understand electronic health record data types and structures, 2) deploy basic informatics methodologies on clinical data, 3) provide appropriate clinical and scientific interpretation of applied analyses, and 4) anticipate barriers in implementing informatics tools into complex clinical settings. You will demonstrate your mastery of these skills by completing practical application projects using real clinical data. This specialization is supported by our industry partnership with Google Cloud. Thanks to this support, all learners will have access to a fully hosted online data science computational environment for free! Please note that you must have access to a Google account (i.e., gmail account) to access the clinical data and computational environment.



          Course 1: Introduction to Clinical Data Science- This course will prepare you to complete all parts of the Clinical Data Science Specialization. In this course you will learn how clinical data are generated, the format of these data, and the ethical and legal restrictions on these data. You will also learn enough SQL and R programming skills to be able to complete the entire Specialization - even if you are a beginner programmer. While you are taking this course you will have access to an actual clinical data set and a free, online computational environment for data science hosted by our Industry Partner Google Cloud. At the end of this course you will be prepared to embark on your clinical data science education journey, learning how to take data created by the healthcare system and improve the health of tomorrow's patients.Course 2: Clinical Data Models and Data Quality Assessments- This course aims to teach the concepts of clinical data models and common data models. Upon completion of this course, learners will be able to interpret and evaluate data model designs using Entity-Relationship Diagrams (ERDs), differentiate between data models and articulate how each are used to support clinical care and data science, and create SQL statements in Google BigQuery to query the MIMIC3 clinical data model and the OMOP common data model.Course 3: Identifying Patient Populations- This course teaches you the fundamentals of computational phenotyping, a biomedical informatics method for identifying patient populations. In this course you will learn how different clinical data types perform when trying to identify patients with a particular disease or trait. You will also learn how to program different data manipulations and combinations to increase the complexity and improve the performance of your algorithms. Finally, you will have a chance to put your skills to the test with a real-world practical application where you develop a computational phenotyping algorithm to identify patients who have hypertension. You will complete this work using a real clinical data set while using a free, online computational environment for data science hosted by our Industry Partner Google Cloud.Course 4: Clinical Natural Language Processing- This course teaches you the fundamentals of clinical natural language processing (NLP). In this course you will learn the basic linguistic principals underlying NLP, as well as how to write regular expressions and handle text data in R. You will also learn practical techniques for text processing to be able to extract information from clinical notes. Finally, you will have a chance to put your skills to the test with a real-world practical application where you develop text processing algorithms to identify diabetic complications from clinical notes. You will complete this work using a free, online computational environment for data science hosted by our Industry Partner Google Cloud.Course 5: Predictive Modeling and Transforming Clinical Practice- This course teaches you the fundamentals of transforming clinical practice using predictive models. This course examines specific challenges and methods of clinical implementation, that clinical data scientists must be aware of when developing their predictive models.Course 6: Advanced Clinical Data Science- This course prepares you to deal with advanced clinical data science topics and techniques including temporal and research quality analysis."
https://www.classcentral.com/course/swayam-scalable-data-science-14279,"Consider the following example problems: One is interested in computing summary statistics (word count distributions) for a set of words which occur in the same document in entire Wikipedia collection (5 million documents). Naive techniques, will run out of main memory on most computers. One needs to train an SVM classifier for text categorization, with unigram features (typically ~10 million) for hundreds of classes. One would run out of main memory, if they store uncompressed model parameters in main memory. One is interested in learning either a supervised model or find unsupervised patterns, but the data is distributed over multiple machines. Communication being the bottleneck, naïve methods to adapt existing algorithms to such a distributed setting might perform extremely poorly. In all the above situations, a simple data mining / machine learning task has been made more complicated due to large scale of input data, output results or both. In this course, we discuss algorithmic techniques as well as software paradigms which allow one to develop scalable algorithms and systems for the common data science tasks.INTENDED AUDIENCE : Computer Science and EngineeringPREREQUISITES : Algorithms, Machine LearningINDUSTRY SUPPORT : Google, Microsoft, Facebook, Amazon, Flipkart, LinkedIn etc. 
      


COURSE LAYOUT Week 1 : Background: Introduction (30 mins) Probability: Concentration inequalities, (30 mins) Linear algebra: PCA, SVD (30 mins) Optimization: Basics, Convex, GD. (30 mins) Machine Learning: Supervised, generalization, feature learning, clustering. (30 mins)Week 2 : Memory-efficient data structures: Hash functions, universal / perfect hash families (30 min) Bloom filters (30 mins) Sketches for distinct count (1 hr) Misra-Gries sketch. (30 min)Week 3 : Memory-efficient data structures (contd.): Count Sketch, Count-Min Sketch (1 hr) Approximate near neighbors search: Introduction, kd-trees etc (30 mins) LSH families, MinHash for Jaccard, SimHash for L2 (1 hr)Week 4 : Approximate near neighbors search: Extensions e.g. multi-probe, b-bit hashing, Data dependent variants (1.5 hr) Randomized Numerical Linear Algebra Random projection (1 hr)Week 5 : Randomized Numerical Linear Algebra CUR Decomposition (1 hr) Sparse RP, Subspace RP, Kitchen Sink (1.5 hr)Week 6 : Map-reduce and related paradigms Map reduce - Programming examples - (page rank, k-means, matrix multiplication) (1 hr) Big data: computation goes to data. + Hadoop ecosystem (1.5 hrs)Week 7 : Map-reduce and related paradigms (Contd.) Scala + Spark (1 hr) Distributed Machine Learning and Optimization: Introduction (30 mins) SGD + Proof (1 hr)Week 8 : Distributed Machine Learning and Optimization: ADMM + applications (1 hr) Clustering (1 hr) Conclusion (30 mins)"
https://www.classcentral.com/course/swayam-business-analytics-text-mining-modeling-using-python-13932,"Objective of this course is to impart knowledge on use of text mining techniques for deriving business intelligence to achieve organizational goals. Use of Python based software platform to build, assess, and compare models based on real datasets and cases with an easy-to-follow learning curve.INTENDED AUDIENCE: UG & PG engineering students: all branches MBA students Professionals working in or aspiring for Business Analyst, Data Analyst, Data Scientist, and Data Engineer rolesPREREQUISITES: Relevant sessions from the courses Business Analytics & Data Mining Modelling Using R Parts I and IIINDUSTRY SUPPORT: Big Data companies, Analytics & Consultancy companies, Companies with Analytics Division 
      


COURSE LAYOUT Week 1: Introductory overview of Text Mining- Introductory Thoughts- Data Mining vs. Text Mining- Text Mining and Text Characteristics- Predictive Text Analytics- Text Mining Problems- Prediction & Evaluation- Python as a Data Science PlatformPython for Analytics- Introduction to Python Installation- Jupyter Notebook IntroductionWeek 2: Python Basics- Python Programming Features- Commands for common tasks and control- Essential Python programming concepts & language mechanicsBuilt in Capabilities of Python- Data structures: tuples, lists, dicts, and setsWeek 3: Built in Capabilities of Python- Functions, Namespaces, Scope, Local functions, Writing more reusable generic functions  Week 4: Built in Capabilities of Python- Generators- Errors & Exception Handling- Working with filesNumerical Python- N-dimensional array objectsWeek 5: Numerical Python- Vectorized array operations- File management using arrays- Linear algebra operations- Pseudo-random number generation- Random walksPython pandas- Data structures: Series and DataFrameWeek 6: Python pandas- Applying functions and methods- Descriptive Statistics- Correlation and CovarianceWorking with Data in Python- Working with CSV, EXCEL files- Working with Web APIsWeek 7: Working with Data in Python- Filtering out missing data, Filling in the missing data, removing duplicates- Perform transformations based on mappings- Binning continuous variables- Random sampling and random reordering of rows- Dummy variables- String and text processing- Regular expressions- Categorical typeData Visualization using Python- Matplotlib Library- Plots & SubplotsWeek 8: Text mining modeling using NLTK- Text Corpus- Sentence Tokenization- Word Tokenization- Removing special Characters- Expanding contractions- Removing Stopwords- Correcting words: repeated characters- Stemming & lemmatization- Part of Speech Tagging- Feature Extraction- Bag of words model- TF-IDF model- Text classification problem- Building a classifier using support vector machine"
https://www.classcentral.com/course/data-scientist-nanodegree--nd025-18219,"Data science skills are in high demand, and you'll finish this program with practical skills needed to land a data science job. You'll build projects designed with our industry partners using real-world data, and by the end of the program you will be able to build machine learning models includng supervised and unsupervised methods; create and run data pipelines; design experiments; build recommendation systems; deploy solutions to the cloud; and more. This program is an ideal way to move into a data science career, and by the end of the program you'll be ready to apply for data science jobs.Build effective machine learning models, run data pipelines, build recommendation systems, and deploy solutions to the cloud with industry-aligned projects.



Prerequisite Knowledge
We recommend students are familiar with machine learning concepts, like those in the Intro to Machine Learning Nanodegree Program. In addition, students should be familiar with Python programming, probability, and statistics. See detailed requirements.


Solving Data Science Problems
Learn the data science process, including how to build effective data visualizations, and how to communicate with various stakeholders.
Write a Data Science Blog Post


Software Engineering for Data Scientists
Develop software engineering skills that are essential for data scientists, such as creating unit tests and building classes.


Data Engineering for Data Scientists
Learn to work with data through the entire data science process, from running pipelines, transforming data, building models, and deploying solutions to the cloud.
Build Disaster Response Pipelines with Figure Eight


Experiment Design and Recommendations
Learn to design experiments and analyze A/B test results. Explore approaches for building recommendation systems.
Design a Recommendation Engine with IBM


Data Science Projects
Leverage what you’ve learned throughout the program to build your own open-ended Data Science project. This project will serve as a demonstration of your valuable abilities as a Data Scientist.
Data Science Capstone Project"
https://www.classcentral.com/course/berkeleyx-foundations-of-data-science-18422,"This Professional Certificate gives you a new lens to explore the issues and problems that you care about. You’ll learn how to combine data with Python programming skills to ask questions and explore problems that you encounter in any field of study, in a future job, and even in everyday life.
This program will help you become a data scientist by teaching you how to analyze a diverse array of real data sets including economic data, geographic data and social networks. Typically, the information will be incomplete and there will be some uncertainty involved. You will then study inference, which will help you quantify uncertainty and measure the accuracy of your estimates. Finally, you will put all of your knowledge together and learn about prediction using machine learning.
The program focuses on a set of core concepts and techniques that have broad applicability. Unlike “bootcamps” for programmers, it presents data science as a way of thinking, in which interpretation and communication are as important as computation and statistical methods.
We all have to be able to think critically and make decisions based on data. Thus, the program aims to make data science accessible to everyone.
It is designed specifically for students who have not previously taken statistics or computer science courses. No prior programming experience is needed. The program is based on Data 8, Berkeley’s fastest-growing class, taken by 1,000+ students each semester.
You don’t have to download any software – a browser is all you need. Open up a window and prepare to have some fun.



            Read more
          



Courses under this program:Course 1: Foundations of Data Science: Computational Thinking with PythonLearn the basics of computational thinking, an essential skill in today’s data-driven world, using the popular programming language, Python.Course 2: Foundations of Data Science: Inferential Thinking by ResamplingLearn how to use inferential thinking to make conclusions about unknowns based on data in random samples.Course 3: Foundations of Data Science: Prediction and Machine LearningLearn how to use machine learning, with a focus on regression and classification, to automatically identify patterns in your data and make better predictions."
https://www.classcentral.com/course/predictive-analytics-for-business-nanodegree--nd0-18211,"Business Analysts are in high demand. In this program, you’ll learn to apply predictive analytics and business intelligence to solve real-world business problems. You’ll do so by building fluency in two leading software packages: Alteryx, a tool that enables you to prepare and analyze data quickly; and Tableau, a powerful data visualization tool. Upon graduating from the program, you’ll be ready to apply for a wide array of Business Analyst roles.Learn to apply predictive analytics and business intelligence to solve real-world business problems.



Prerequisite Knowledge
Students who enroll should be familiar with algebra and descriptive statistics and have experience working with data in Excel. Working knowledge of SQL and Tableau is a plus, but not required.See detailed requirements.


Problem Solving with Advanced Analytics
In this course, we give you a framework to help you organize and plan your analytical approach. We also introduce both simple Linear Regression and Multiple Linear Regression.
Predict Sales for a Catalog Launch


Data Wrangling
Data Wrangling is at the core of all data activity. In this course you learn how to work with different data types,dirty data, and outliers. You will also learn how to reformat data and join data from different sources together.
Create an Analytical Dataset


Classification Models
Classification models are a powerful tool for business analyst. In this course, you learn more about binary and non-binary classification models and how to use them to drive business insights.
Predict Loan Default Risk


A/B Testing
Helping businesses make the best decisions is an essential part of Business Analysis. Planning and executing the analysis of an AB test allow you to provide confident recommendations. In this course, you learn how to create, execute, and analyze an AB test.
A/B Test a Menu Launch


Time Series Forecasting
Time Series Forecasting is a powerful analytical tool. In this course, you learn how ETS and ARIMA models are used to forecast data and how they deal with trends and seasonality. These skills will be evaluated in the final project.


Segmentation and Clustering
Segmentation and Clustering are effective methods for finding patterns in your data. In this course, you learn how to prepare data to be clustered appropriately and interpret results.
Combine Predictive Techniques"
https://www.classcentral.com/course/mba-18737,"This specialisation from the University of London is designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in further study and in international business. If completed successfully, you may be able to use your certificate from this specialisation as part of the application process for the University of London Global MBA. This Specialisation is endorsed by CMI.



          Course 1: Professional Skills for International Business- This course provides insight into the key professional skills needed by managers at all levels of an organisation. You’ll learn key skills such as how to make a positive first impression; how to become a role model at work; effective time and resource management; and networking. This course forms part of a specialisation from the University of London designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in international business, or in further study. If completed successfully, your certificate from this specialisation can also be used as part of the application process for the University of London Global MBA programme, particularly for early career applicants. If you would like more information about the Global MBA, please visit https://mba.london.ac.uk/. This course is endorsed by CMICourse 2: Management Skills for International Business- This course focuses on a range of management techniques. You’ll discover the main skills and competencies of effective leaders, and how to distinguish between management and leadership. The course will cover team dynamics, how to build effective relationships, key motivation theories, and how to use communication to best effect. This course forms part of a specialisation from the University of London designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in international business, or in further study. If completed successfully, your certificate from this specialisation can also be used as part of the application process for the University of London Global MBA programme, particularly for early career applicants. If you would like more information about the Global MBA, please visit https://mba.london.ac.uk/. This course is endorsed by CMICourse 3: Quantitative Foundations for International Business- This course provides the essential mathematics required to succeed in the finance and economics related modules of the Global MBA, including equations, functions, derivatives, and matrices. You can test your understanding with quizzes and worksheets, while more advanced content will be available if you want to push yourself. This course forms part of a specialisation from the University of London designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in international business, or in further study. If completed successfully, your certificate from this specialisation can also be used as part of the application process for the University of London Global MBA programme, particularly for early career applicants. If you would like more information about the Global MBA, please visit https://mba.london.ac.uk/. This course is endorsed by CMICourse 4: Statistics for International Business- This course introduces core areas of statistics that will be useful in business and for several MBA modules. It covers a variety of ways to present data, probability, and statistical estimation. You can test your understanding as you progress, while more advanced content is available if you want to push yourself. This course forms part of a specialisation from the University of London designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in international business, or in further study. If completed successfully, your certificate from this specialisation can also be used as part of the application process for the University of London Global MBA programme, particularly for early career applicants. If you would like more information about the Global MBA, please visit https://mba.london.ac.uk/. This course is endorsed by CMICourse 5: International Business Environment- This course explores the international business environment in which organisations function. You’ll learn about core analysis methods, including PESTLE, SWOT, and Boston Box Matrices, as well as the applications of Porter’s Five Forces. You’ll have the opportunity to participate in discussion forums and access case studies, as well as testing your understanding in quizzes. This course forms part of a specialization from the University of London designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in international business, or in further study. If completed successfully, your certificate from this specialization can also be used as part of the application process for the University of London Global MBA programme, particularly for early career applicants. If you would like more information about the Global MBA, please visit https://mba.london.ac.uk/. This course is endorsed by CMICourse 6: International Business Capstone- The Capstone project is the culmination of the International Business Essentials specialisation, and brings together knowledge gained in the previous courses and applies this to a real-world scenario. Throughout the Capstone, you’ll be taught new skills, and will use the knowledge you have gained throughout the Specialisation to develop a business plan. This course forms part of a specialisation from the University of London designed to help you develop and build the essential business, academic, and cultural skills necessary to succeed in international business, or in further study. If completed successfully, your certificate from this specialisation can also be used as part of the application process for the University of London Global MBA programme, particularly for early career applicants. If you would like more information about the Global MBA, please visit https://mba.london.ac.uk/. This course is endorsed by CMI"
https://www.classcentral.com/course/managerial-economics-business-analysis-18507,"In order to effectively manage and operate a business, managers and leaders need to understand the market characteristics and economic environment they operate in. In this Specialization, you will build a solid understanding of the operation of markets and the macro-economic environment with real-world examples. You will be able to identify firm and country-level economic factors that impact business decisions, develop an analytical framework using statistical tools, and apply economic theory and data in the analysis of business environment and trends to make effective business decisions. The capstone project involves an in-depth analysis of an actual business situation in which you will examine the global economic environment of a business. The final project will be a business plan that uses statistical tools and economic theory to create a comprehensive analysis of the microeconomic and macroeconomic environment in which the focal company operates. This Specialization is part of the University of Illinois Masters of Business Administration degree program, the iMBA. Learn more about the admission into the program here and how your Coursera work can be leveraged if accepted into the program. You can also start with for-credit courses within the Illinois iMBA degree program.



          Course 1: Firm Level Economics: Consumer and Producer Behavior- All goods and services are subject to scarcity at some level. Scarcity means that society must develop some allocation mechanism – rules to determine who gets what. Over recorded history, these allocation rules were usually command based – the king or the emperor would decide. In contemporary times, most countries have turned to market based allocation systems. In markets, prices act as rationing devices, encouraging or discouraging production and encouraging or discouraging consumption in such a way as to find an equilibrium allocation of resources. We will construct demand curves to capture consumer behavior and supply curves to capture producer behavior. The resulting equilibrium price “rations” the scarce commodity. Markets are frequent targets of government intervention. This intervention can be direct control of prices or it could be indirect price pressure through the imposition of taxes or subsidies. Both forms of intervention are impacted by elasticity of demand. After this course, you will be able to: • Describe consumer behavior as captured by the demand curve. • Describe producer behavior as captured by the supply curve. • Explain equilibrium in a market. • Explain the impact of taxes and price controls on market equilibrium. • Explain elasticity of demand. • Describe cost theory and how firms optimize given the constraints of their own costs and an exogenously given price. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 2: Firm Level Economics: Markets and Allocations- In this class, we will derive equilibrium outcomes across a variety of market structures. We will begin by understanding equilibrium under a market structure called Perfect Competition, a benchmark construction. Economists have tools to measure the efficiency of market outcomes. We next consider the polar extreme of a competitive market: a monopoly market. We will determine the monopoly equilibrium price and quantity and efficiency properties. Much economic activity takes place in markets with just a handful of very large producers. To understand equilibrium in these oligopoly markets requires more careful attention to strategic interdependence. To capture this interdependence, we consider collusive arrangements among a small number of rivals as well as the use of simple game theoretic techniques to model equilibrium. Market Failure describes situations where markets fail to find the efficient outcome. Information asymmetries are one fertile form of market failure. Another form of market failure occurs when externalities are present. We will examine one key externality, pollution, and construct a policy prescription to mitigate the negative efficiency impacts of this externality. Upon successful completion of this course, you will be able to: • Explain how different market structures result in different resource allocations. • Model the impact of external shocks to a particular market structure and demonstrate the new equilibrium price and quantity after the impact of this external shock has played out. • Evaluate the efficiency of an equilibrium. Different market structures produce different levels of efficiency. • Explain when and why the government might intervene with regulatory authority or antitrust litigation to lessen inefficiencies in some markets. • Describe how information problems can cause inefficient outcomes. • Understand externalities and consider optimal government response to these market failures. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 3: Country Level Economics: Macroeconomic Variables and Markets- This course discusses how macroeconomic variables affect individuals’ personal, professional, and public activities and lays the foundation for the analysis of the mechanisms that drive macroeconomic variables. It start in its first module by introducing the key macroeconomic variables and explaining how they are defined and measured in order to enable the students to interpret macroeconomic data properly. In the second module, the course offers a perspective for separating out various parts of the economy driven by different processes and for combining those components to develop a richer view of the whole. In particular, it applies this approach to the analysis of the relationship of the trade deficit with the budget deficit and private savings, offering insights about some key determinants of the balance of payments. The third and fourth modules focus on the analyses of the foreign currency and money markets to provide fundamental models of the interest rate and exchange rate determination. They also discuss how these variables interact with each other and with the macroeconomic conditions, particularly monetary policy and the expectations about the future trends in the economy. These analyses lay the foundation for more comprehensive models of the macroeconomy in the next course of the Managerial Economics and Business Analysis Specialization. At the end of this course, you will be able to: • Systematically assess the national and international economic environment in which you live and work. • Analyze macroeconomic issues using key tools. • Be a more effective professional in your line of activity. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 4: Country Level Economics: Policies, Institutions, and Macroeconomic Performance- This course examines macroeconomic performance in the short run and the long run based on the economy’s institutional and policy environment. The first module develops a model of macroeconomy in the short run when the price level has its own momentum and does not respond much to supply and demand forces. The model enables one to see how GDP, interest rate, and exchange rate are determined in the short run and how they respond to macroeconomic shocks and policies. The second module starts the analysis of long-run equilibrium by examining the foreign exchange market. It then connects the long-run outcome with expectations about the future trends in the exchange rate at each moment, which constitute a key driver of the spot exchange rate in the short run. The methodology developed for this purpose can be applied to expectations concerning other macroeconomic variables as well. Finally, the long-run foreign exchange model is employed to derive a number of important lessons for the long run trends in currency values and competitiveness of producers in various countries. The third module examines the drivers of aggregate output in the long run and the mechanisms of adjustment from the short run to the long run. The model provides insights about why some countries are much richer than others and why some economies grow faster than others over decades. The analysis also sheds light on why inflation varies across countries or over time in the same country. The model is employed to analyze the sources of macroeconomic instability and the roles of fiscal and monetary policies in stabilization or destabilization of the macroeconomy. The final module discusses the characteristics of desirable macroeconomic policies and the reasons why actual policies deviate from them. It connects these deviations to country characteristics that one needs to take into account when assessing a country’s long-term macroeconomic environment. The module ends with a discussion of the institutional conditions that help bring about better fiscal and monetary policies. At the end of this course, you will be able to: • Understand how the market for aggregate goods and services interacts with the money market to shape the macroeconomic equilibrium that determines income, interest rate, and exchange rate in the short run. • Understand the links between the short-run and long-run processes. • Assess the dynamic effects of macroeconomic policies and understand the roles of globalization, government policies, institutions, and expectations in macroeconomic outcomes. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 5: Exploring and Producing Data for Business Decision Making- This course provides an analytical framework to help you evaluate key problems in a structured fashion and will equip you with tools to better manage the uncertainties that pervade and complicate business processes. Specifically, you will be introduced to statistics and how to summarize data and learn concepts of frequency, normal distribution, statistical studies, sampling, and confidence intervals. While you will be introduced to some of the science of what is being taught, the focus will be on applying the methodologies. This will be accomplished through the use of Excel and data sets from many different disciplines, allowing you to see the use of statistics in very diverse settings. The course will focus not only on explaining these concepts, but also understanding the meaning of the results obtained. Upon successful completion of this course, you will be able to: • Summarize large data sets in graphical, tabular, and numerical forms. • Understand the significance of proper sampling and why you can rely on sample information. • Understand why normal distribution can be used in so many settings. • Use sample information to infer about the population with a certain level of confidence about the accuracy of the estimations. • Use Excel for statistical analysis. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 6: Inferential and Predictive Statistics for Business- This course provides an analytical framework to help you evaluate key problems in a structured fashion and will equip you with tools to better manage the uncertainties that pervade and complicate business processes. The course aim to cover statistical ideas that apply to managers. We will consider two basic themes: first, is recognizing and describing variations present in everything around us, and then modeling and making decisions in the presence of these variations. The fundamental concepts studied in this course will reappear in many other classes and business settings. Our focus will be on interpreting the meaning of the results in a business and managerial setting. While you will be introduced to some of the science of what is being taught, the focus will be on applying the methodologies. This will be accomplished through use of Excel and using data sets from many different disciplines, allowing you to see the use of statistics in very diverse settings. The course will focus not only on explaining these concepts but also understanding the meaning of the results obtained. Upon successful completion of this course, you will be able to: • Test for beliefs about a population.. • Compare differences between populations. • Use linear regression model for prediction. • Learn how to use Excel for statistical analysis. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 7: Managerial Economics and Business Analysis Capstone- The capstone project involves an in-depth analysis of an actual business situation in which you will examine the global economic environment of a business. The final project will be a business plan that uses statistical tools and economic theory to create a comprehensive analysis of the microeconomic and macroeconomic environment in which the focal company operates. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu."
https://www.classcentral.com/course/data-analysis-18648,"Learn SAS or Python programming, expand your knowledge of analytical methods and applications, and conduct original research to inform complex decisions. The Data Analysis and Interpretation Specialization takes you from data novice to data expert in just four project-based courses. You will apply basic data science tools, including data management and visualization, modeling, and machine learning using your choice of either SAS or Python, including pandas and Scikit-learn. Throughout the Specialization, you will analyze a research question of your choice and summarize your insights. In the Capstone Project, you will use real data to address an important issue in society, and report your findings in a professional-quality report. You will have the opportunity to work with our industry partners, DRIVENDATA and The Connection. Help DRIVENDATA solve some of the world's biggest social challenges by joining one of their competitions, or help The Connection better understand recidivism risk for people on parole in substance use treatment. Regular feedback from peers will provide you a chance to reshape your question. This Specialization is designed to help you whether you are considering a career in data, work in a context where supervisors are looking to you for data insights, or you just have some burning questions you want to explore. No prior experience is required. By the end you will have mastered statistical methods to conduct original research to inform complex decisions.



            Read more
          



          Course 1: Data Management and Visualization- Whether being used to customize advertising to millions of website visitors or streamline inventory ordering at a small restaurant, data is becoming more integral to success. Too often, we’re not sure how use data to find answers to the questions that will make us more successful in what we do. In this course, you will discover what data is and think about what questions you have that can be answered by the data – even if you’ve never thought about data before. Based on existing data, you will learn to develop a research question, describe the variables and their relationships, calculate basic statistics, and present your results clearly. By the end of the course, you will be able to use powerful data analysis tools – either SAS or Python – to manage and visualize your data, including how to deal with missing data, variable groups, and graphs. Throughout the course, you will share your progress with others to gain valuable feedback, while also learning how your peers use data to answer their own questions.Course 2: Data Analysis Tools- In this course, you will develop and test hypotheses about your data. You will learn a variety of statistical tests, as well as strategies to know how to apply the appropriate one to your specific data and question. Using your choice of two powerful statistical software packages (SAS or Python), you will explore ANOVA, Chi-Square, and Pearson correlation analysis. This course will guide you through basic statistical principles to give you the tools to answer questions you have developed. Throughout the course, you will share your progress with others to gain valuable feedback and provide insight to other learners about their work.Course 3: Regression Modeling in Practice- This course focuses on one of the most important tools in your data analysis arsenal: regression analysis. Using either SAS or Python, you will begin with linear regression and then learn how to adapt when two variables do not present a clear linear relationship. You will examine multiple predictors of your outcome and be able to identify confounding variables, which can tell a more compelling story about your results. You will learn the assumptions underlying regression analysis, how to interpret regression coefficients, and how to use regression diagnostic plots and other tools to evaluate the quality of your regression model. Throughout the course, you will share with others the regression models you have developed and the stories they tell you.Course 4: Machine Learning for Data Analysis- Are you interested in predicting future outcomes using your data? This course helps you do just that! Machine learning is the process of developing, testing, and applying predictive algorithms to achieve this goal. Make sure to familiarize yourself with course 3 of this specialization before diving into these machine learning concepts. Building on Course 3, which introduces students to integral supervised machine learning concepts, this course will provide an overview of many additional concepts, techniques, and algorithms in machine learning, from basic classification to decision trees and clustering. By completing this course, you will learn how to apply, test, and interpret machine learning algorithms as alternative methods for addressing your research questions.Course 5: Data Analysis and Interpretation Capstone- The Capstone project will allow you to continue to apply and refine the data analytic techniques learned from the previous courses in the Specialization to address an important issue in society. You will use real world data to complete a project with our industry and academic partners. For example, you can work with our industry partner, DRIVENDATA, to help them solve some of the world's biggest social challenges! DRIVENDATA at www.drivendata.org, is committed to bringing cutting-edge practices in data science and crowdsourcing to some of the world's biggest social challenges and the organizations taking them on. Or, you can work with our other industry partner, The Connection (www.theconnectioninc.org) to help them better understand recidivism risk for people on parole seeking substance use treatment. For more than 40 years, The Connection has been one of Connecticut’s leading private, nonprofit human service and community development agencies. Each month, thousands of people are assisted by The Connection’s diverse behavioral health, family support and community justice programs. The Connection’s Institute for Innovative Practice was created in 2010 to bridge the gap between researchers and practitioners in the behavioral health and criminal justice fields with the goal of developing maximally effective, evidence-based treatment programs. A major component of the Capstone project is for you to be able to choose the information from your analyses that best conveys results and implications, and to tell a compelling story with this information. By the end of the course, you will have a professional quality report of your findings that can be shown to colleagues and potential employers to demonstrate the skills you learned by completing the Specialization."
https://www.classcentral.com/course/data-analytics-business-18633,"The Advanced Business Analytics Specialization brings together academic professionals and experienced practitioners to share real world data analytics skills you can use to grow your business, increase profits, and create maximum value for your shareholders. Learners gain practical skills in extracting and manipulating data using SQL code, executing statistical methods for descriptive, predictive, and prescriptive analysis, and effectively interpreting and presenting analytic results. The problems faced by decision makers in today’s competitive business environment are complex. Achieve a clear competitive advantage by using data to explain the performance of a business, evaluate different courses of action, and employ a structured approach to business problem-solving. Check out a one-minute video about this specialization to learn more!



          Course 1: Introduction to Data Analytics for Business- This course will expose you to the data analytics practices executed in the business world. We will explore such key areas as the analytical process, how data is created, stored, accessed, and how the organization works with data and creates the environment in which analytics can flourish. What you learn in this course will give you a strong foundation in all the areas that support analytics and will help you to better position yourself for success within your organization. You’ll develop skills and a perspective that will make you more productive faster and allow you to become a valuable asset to your organization. This course also provides a basis for going deeper into advanced investigative and computational methods, which you have an opportunity to explore in future courses of the Data Analytics for Business specialization.Course 2: Predictive Modeling and Analytics - Welcome to the second course in the Data Analytics for Business specialization! This course will introduce you to some of the most widely used predictive modeling techniques and their core principles. By taking this course, you will form a solid foundation of predictive analytics, which refers to tools and techniques for building statistical or machine learning models to make predictions based on data. You will learn how to carry out exploratory data analysis to gain insights and prepare data for predictive modeling, an essential skill valued in the business. You’ll also learn how to summarize and visualize datasets using plots so that you can present your results in a compelling and meaningful way. We will use a practical predictive modeling software, XLMiner, which is a popular Excel plug-in. This course is designed for anyone who is interested in using data to gain insights and make better business decisions. The techniques discussed are applied in all functional areas within business organizations including accounting, finance, human resource management, marketing, operations, and strategic planning. The expected prerequisites for this course include a prior working knowledge of Excel, introductory level algebra, and basic statistics.Course 3: Business Analytics for Decision Making- In this course you will learn how to create models for decision making. We will start with cluster analysis, a technique for data reduction that is very useful in market segmentation. You will then learn the basics of Monte Carlo simulation that will help you model the uncertainty that is prevalent in many business decisions. A key element of decision making is to identify the best course of action. Since businesses problems often have too many alternative solutions, you will learn how optimization can help you identify the best option. What is really exciting about this course is that you won’t need to know a computer language or advanced statistics to learn about these predictive and prescriptive analytic models. The Analytic Solver Platform and basic knowledge of Excel is all you’ll need. Learners participating in assignments will be able to get free access to the Analytic Solver Platform.Course 4: Communicating Business Analytics Results- The analytical process does not end with models than can predict with accuracy or prescribe the best solution to business problems. Developing these models and gaining insights from data do not necessarily lead to successful implementations. This depends on the ability to communicate results to those who make decisions. Presenting findings to decision makers who are not familiar with the language of analytics presents a challenge. In this course you will learn how to communicate analytics results to stakeholders who do not understand the details of analytics but want evidence of analysis and data. You will be able to choose the right vehicles to present quantitative information, including those based on principles of data visualization. You will also learn how to develop and deliver data-analytics stories that provide context, insight, and interpretation.Course 5: Advanced Business Analytics Capstone- The analytics process is a collection of interrelated activities that lead to better decisions and to a higher business performance. The capstone of this specialization is designed with the goal of allowing you to experience this process. The capstone project will take you from data to analysis and models, and ultimately to presentation of insights. In this capstone project, you will analyze the data on financial loans to help with the investment decisions of an investment company. You will go through all typical steps of a data analytics project, including data understanding and cleanup, data analysis, and presentation of analytical results. For the first week, the goal is to understand the data and prepare the data for analysis. As we discussed in this specialization, data preprocessing and cleanup is often the first step in data analytics projects. Needless to say, this step is crucial for the success of this project. In the second week, you will perform some predictive analytics tasks, including classifying loans and predicting losses from defaulted loans. You will try a variety of tools and techniques this week, as the predictive accuracy of different tools can vary quite a bit. It is rarely the case that the default model produced by ASP is the best model possible. Therefore, it is important for you to tune the different models in order to improve the performance. Beginning in the third week, we turn our attention to prescriptive analytics, where you will provide some concrete suggestions on how to allocate investment funds using analytics tools, including clustering and simulation based optimization. You will see that allocating funds wisely is crucial for the financial return of the investment portfolio. In the last week, you are expected to present your analytics results to your clients. Since you will obtain many results in your project, it is important for you to judiciously choose what to include in your presentation. You are also expected to follow the principles we covered in the courses in preparing your presentation."
https://www.classcentral.com/course/gcp-data-machine-learning-18608,"This online specialization provides participants a hands-on introduction to designing and building data pipelines on Google Cloud Platform. Through a combination of presentations, demos, and hand-on labs, participants will learn how to design data processing systems, build end-to-end data pipelines, analyze data and derive insights. The course covers structured, unstructured, and streaming data. This course teaches the following skills: • Design and build data pipelines on Google Cloud Platform • Lift and shift your existing Hadoop workloads to the Cloud using Cloud Dataproc. • Process batch and streaming data by implementing autoscaling data pipelines on Cloud Dataflow • Manage your data Pipelines with Data Fusion and Cloud Composer. • Derive business insights from extremely large datasets using Google BigQuery • Learn how to use pre-built ML APIs on unstructured data and build different kinds of ML models using BigQuery ML. • Enable instant insights from streaming data This class is intended for developers who are responsible for: • Extracting, Loading, Transforming, cleaning, and validating data • Designing pipelines and architectures for data processing • Integrating analytics and machine learning capabilities into data pipelines • Querying datasets, visualizing query results and creating reports >>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<



            Read more
          



          Course 1: Google Cloud Platform Big Data and Machine Learning Fundamentals- This 2-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities. At the end of this course, participants will be able to: • Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform • Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform • Employ BigQuery and Cloud Datalab to carry out interactive data analysis • Choose between Cloud SQL, BigTable and Datastore • Train and use a neural network using TensorFlow • Choose between different data processing products on the Google Cloud Platform Before enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following: • A common query language such as SQL • Extract, transform, load activities • Data modeling • Machine learning and/or statistics • Programming in Python Google Account Notes: • Google services are currently unavailable in China.Course 2: Modernizing Data Lakes and Data Warehouses with GCP- The two key components of any data pipeline are data lakes and warehouses. This course highlights use-cases for each type of storage and dives into the available data lake and warehouse solutions on Google Cloud Platform in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a cloud environment. Learners will get hands-on experience with data lakes and warehouses on Google Cloud Platform using QwikLabs.Course 3: Building Batch Data Pipelines on GCP- Data pipelines typically fall under one of the Extra-Load, Extract-Load-Transform or Extract-Transform-Load paradigms. This course describes which paradigm should be used and when for batch data. Furthermore, this course covers several technologies on Google Cloud Platform for data transformation including BigQuery, executing Spark on Cloud Dataproc, pipeline graphs in Cloud Data Fusion and serverless data processing with Cloud Dataflow. Learners will get hands-on experience building data pipeline components on Google Cloud Platform using QwikLabs.Course 4: Building Resilient Streaming Analytics Systems on GCP- *Note: this is a new course with updated content from what you may have seen in the previous version of this Specialization. Processing streaming data is becoming increasingly popular as streaming enables businesses to get real-time metrics on business operations. This course covers how to build streaming data pipelines on Google Cloud Platform. Cloud Pub/Sub is described for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Cloud Dataflow, and how to store processed records to BigQuery or Cloud Bigtable for analysis. Learners will get hands-on experience building streaming data pipeline components on Google Cloud Platform using QwikLabs.Course 5: Smart Analytics, Machine Learning, and AI on GCP- Incorporating machine learning into data pipelines increases the ability of businesses to extract insights from their data. This course covers several ways machine learning can be included in data pipelines on Google Cloud Platform depending on the level of customization required. For little to no customization, this course covers AutoML. For more tailored machine learning capabilities, this course introduces AI Platform Notebooks and BigQuery Machine Learning. Also, this course covers how to productionalize machine learning solutions using Kubeflow. Learners will get hands-on experience building machine learning models on Google Cloud Platform using QwikLabs."
https://www.classcentral.com/course/six-sigma-green-belt-18643,"This specialization is for you if you are looking to learn more about the more advanced components of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes, performance, and quality assurance. In this specialization, you will learn proven principles and tools specific to six sigma and lean. This is a sequential, linear designed specialization that covers a more advanced level of content (at the ""green belt"" level) of Six Sigma and Lean. Yellow Belt knowledge is needed before advancing to Green Belt (which is the second specialization offered here on Coursera by the USG). Green Belt knowledge is needed before moving to a Black Belt. The proper sequence of this specialization is: Course #1 - Six Sigma and the Organization (Advanced) Course #2 - Six Sigma Advanced Define and Measure Phase Course #3 - Six Sigma Advanced Analyze Phase Course #4 - Six Sigma Advanced Improve and Control Phase At the end of Course #4 (Six Sigma Advanced Improve and Control Phase), there is a peer-reviewed, capstone project. Successful completion of this project is necessary for full completion of this specialization. It should be noted that completing either the Yellow Belt or Green Belt Specializations does not give the learner ""professional accreditation"" in Six Sigma, but should assist in better preparation for such professional accreditation testing.



          Course 1: Six Sigma and the Organization (Advanced)- This course is for you if you are looking to dive deeper into Six Sigma or strengthen and expand your knowledge of the basic components of green belt level of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will introduce you to the purpose of Six Sigma and its value to an organization. You will learn about the basic principles of Six Sigma and Lean. Your instructors will introduce you to, and have you apply, some of the tools and metrics that are critical components of Six Sigma. This course will provide you with the basic knowledge of the principles, roles, and responsibilities of Six Sigma and Lean. Every module will include readings, videos, and a quiz to help make sure you understand the material and concepts that are studied. You will also have the opportunity to participate in discussions and peer review exercises to give you the opportunity to apply the material to your daily life. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Green Belt Handbook (2nd edition). The companion text is NOT required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly-recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).Course 2: Six Sigma Advanced Define and Measure Phases- This course is for you if you are looking to dive deeper into Six Sigma or strengthen and expand your knowledge of the basic components of green belt level of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will take you deeper into the principles and tools associated with the ""Design"" and ""Measure"" phases of the DMAIC structure of Six Sigma. It is highly recommended that you complete the ""Yellow Belt Specialization"" and the course ""Six Sigma and the Organization (Advanced)"" before beginning this course. In this course, your instructors will introduce you to, and have you apply, some of the tools and metrics that are critical components of Six Sigma. This course will provide you with the advanced knowledge of team dynamics and performance, process analysis, probability, statistics, statistical distributions, collecting and summarizing data, measurement systems analysis, process and performance capability, and exploratory data analysis associated with Six Sigma and Lean. Every module will include readings, videos, and quizzes to help make sure you understand the material and concepts that are studied. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Green Belt Handbook (2nd edition). The companion text is NOT required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly-recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).Course 3: Six Sigma Advanced Analyze Phase- This course is for you if you are looking to dive deeper into Six Sigma or strengthen and expand your knowledge of the basic components of green belt level of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will take you deeper into the principles and tools associated with the ""Analyze"" phase of the DMAIC structure of Six Sigma. It is highly recommended that you complete the ""Yellow Belt Specialization"" and the courses ""Six Sigma and the Organization (Advanced)"" and ""Six Sigma Advanced Design and Measure Phase"" before beginning this course. In this course, your instructors will introduce you to, and have you apply, some of the tools and metrics that are critical components of Six Sigma. This course will provide you with the advanced knowledge of hypothesis testing and design of experiments as they are associated with Six Sigma and Lean. Every module will include readings, videos, and quizzes to help make sure you understand the material and concepts that are studied. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Green Belt Handbook (2nd edition). The companion text is NOT required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly-recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ).Course 4: Six Sigma Advanced Improve and Control Phases- This course is for you if you are looking to dive deeper into Six Sigma or strengthen and expand your knowledge of the basic components of green belt level of Six Sigma and Lean. Six Sigma skills are widely sought by employers both nationally and internationally. These skills have been proven to help improve business processes and performance. This course will take you deeper into the principles and tools associated with the ""Improve"" and ""Control"" phases of the DMAIC structure of Six Sigma. It is highly recommended that you complete the ""Yellow Belt Specialization"" and the courses ""Six Sigma and the Organization (Advanced),"" ""Six Sigma Advanced Design and Measure Phase,"" and ""Six Sigma Advanced Analyze Phase"" before beginning this course. In this course, your instructors will introduce you to, and have you apply, some of the tools and metrics that are critical components of Six Sigma. This course will provide you with the advanced knowledge of root cause analysis, lean tools, control plan, process control, and statistical process control (SPC) as they are associated with Six Sigma and Lean. Every module will include readings, videos, and quizzes to help make sure you understand the material and concepts that are studied. Registration includes online access to course content, projects, and resources but does not include the companion text The Certified Six Sigma Green Belt Handbook (2nd edition). The companion text is NOT required to complete the assignments. However, the text is a recognized handbook used by professionals in the field. Also, it is a highly-recommended text for those wishing to move forward in Six Sigma and eventually gain certification from professional agencies such as American Society for Quality (ASQ)."
https://www.classcentral.com/course/executive-data-science-18529,"Assemble the right team, ask the right questions, and avoid the mistakes that derail data science projects.

In four intensive courses, you will learn what you need to know to begin assembling and leading a data science enterprise, even if you have never worked in data science before. You’ll get a crash course in data science so that you’ll be conversant in the field and understand your role as a leader. You’ll also learn how to recruit, assemble, evaluate, and develop a team with complementary skill sets and roles. You’ll learn the structure of the data science pipeline, the goals of each stage, and how to keep your team on target throughout. Finally, you’ll learn some down-to-earth practical skills that will help you overcome the common challenges that frequently derail data science projects.
      


          Course 1: A Crash Course in Data Science- By now you have definitely heard about data science and big data. In this one-week class, we will provide a crash course in what these terms mean and how they play a role in successful organizations. This class is for anyone who wants to learn what all the data science action is about, including those who will eventually need to manage data scientists. The goal is to get you up to speed as quickly as possible on data science without all the fluff. We've designed this course to be as convenient as possible without sacrificing any of the essentials. This is a focused course designed to rapidly get you up to speed on the field of data science. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know. 1. How to describe the role data science plays in various contexts 2. How statistics, machine learning, and software engineering play a role in data science 3. How to describe the structure of a data science project 4. Know the key terms and tools used by data scientists 5. How to identify a successful and an unsuccessful data science project 3. The role of a data science manager Course cover image by r2hox. Creative Commons BY-SA: https://flic.kr/p/gdMuhTCourse 2: Building a Data Science Team- Data science is a team sport. As a data science executive it is your job to recruit, organize, and manage the team to success. In this one-week course, we will cover how you can find the right people to fill out your data science team, how to organize them to give them the best chance to feel empowered and successful, and how to manage your team as it grows. This is a focused course designed to rapidly get you up to speed on the process of building and managing a data science team. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know. 1. The different roles in the data science team including data scientist and data engineer 2. How the data science team relates to other teams in an organization 3. What are the expected qualifications of different data science team members 4. Relevant questions for interviewing data scientists 5. How to manage the onboarding process for the team 6. How to guide data science teams to success 7. How to encourage and empower data science teams Commitment: 1 week of study, 4-6 hours Course cover image by JaredZammit. Creative Commons BY-SA. https://flic.kr/p/5vuWZzCourse 3: Managing Data Analysis- This one-week course describes the process of analyzing data and how to manage that process. We describe the iterative nature of data analysis and the role of stating a sharp question, exploratory data analysis, inference, formal statistical modeling, interpretation, and communication. In addition, we will describe how to direct analytic activities within a team and to drive the data analysis process towards coherent and useful results. This is a focused course designed to rapidly get you up to speed on the process of data analysis and how it can be managed. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know how to…. 1. Describe the basic data analysis iteration 2. Identify different types of questions and translate them to specific datasets 3. Describe different types of data pulls 4. Explore datasets to determine if data are appropriate for a given question 5. Direct model building efforts in common data analyses 6. Interpret the results from common data analyses 7. Integrate statistical findings to form coherent data analysis presentations Commitment: 1 week of study, 4-6 hours Course cover image by fdecomite. Creative Commons BY https://flic.kr/p/4HjmvDCourse 4: Data Science in Real Life- Have you ever had the perfect data science experience? The data pull went perfectly. There were no merging errors or missing data. Hypotheses were clearly defined prior to analyses. Randomization was performed for the treatment of interest. The analytic plan was outlined prior to analysis and followed exactly. The conclusions were clear and actionable decisions were obvious. Has that every happened to you? Of course not. Data analysis in real life is messy. How does one manage a team facing real data analyses? In this one-week course, we contrast the ideal with what happens in real life. By contrasting the ideal, you will learn key concepts that will help you manage real life analyses. This is a focused course designed to rapidly get you up to speed on doing data science in real life. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know how to: 1, Describe the “perfect” data science experience 2. Identify strengths and weaknesses in experimental designs 3. Describe possible pitfalls when pulling / assembling data and learn solutions for managing data pulls. 4. Challenge statistical modeling assumptions and drive feedback to data analysts 5. Describe common pitfalls in communicating data analyses 6. Get a glimpse into a day in the life of a data analysis manager. The course will be taught at a conceptual level for active managers of data scientists and statisticians. Some key concepts being discussed include: 1. Experimental design, randomization, A/B testing 2. Causal inference, counterfactuals, 3. Strategies for managing data quality. 4. Bias and confounding 5. Contrasting machine learning versus classical statistical inference Course promo: https://www.youtube.com/watch?v=9BIYmw5wnBI Course cover image by Jonathan Gross. Creative Commons BY-ND https://flic.kr/p/q1vudbCourse 5: Executive Data Science Capstone- The Executive Data Science Capstone, the specialization’s culminating project, is an opportunity for people who have completed all four EDS courses to apply what they've learned to a real-world scenario developed in collaboration with Zillow, a data-driven online real estate and rental marketplace, and DataCamp, a web-based platform for data science programming. Your task will be to lead a virtual data science team and make key decisions along the way to demonstrate that you have what it takes to shepherd a complex analysis project from start to finish. For the final project, you will prepare and submit a presentation, which will be evaluated and graded by your fellow capstone participants. Course cover image by Luckey_sun. Creative Commons BY-SA https://flic.kr/p/bx1jvU"
https://www.classcentral.com/course/investment-management-python-machine-learning-18815,"The Data Science and Machine Learning for Asset Management Specialization has been designed to deliver a broad and comprehensive introduction to modern methods in Investment Management, with a particular emphasis on the use of data science and machine learning techniques to improve investment decisions.By the end of this specialization, you will have acquired the tools required for making sound investment decisions, with an emphasis not only on the foundational theory and underlying concepts, but also on practical applications and implementation. Instead of merely explaining the science, we help you build on that foundation in a practical manner, with an emphasis on the hands-on implementation of those ideas in the Python programming language through a series of dedicated lab sessions.



          Course 1: Introduction to Portfolio Construction and Analysis with Python- The practice of investment management has been transformed in recent years by computational methods. This course provides an introduction to the underlying science, with the aim of giving you a thorough understanding of that scientific basis. However, instead of merely explaining the science, we help you build on that foundation in a practical manner, with an emphasis on the hands-on implementation of those ideas in the Python programming language. This course is the first in a four course specialization in Data Science and Machine Learning in Asset Management but can be taken independently. In this course, we cover the basics of Investment Science, and we'll build practical implementations of each of the concepts along the way. We'll start with the very basics of risk and return and quickly progress to cover a range of topics including several Nobel Prize winning concepts. We'll cover some of the most popular practical techniques in modern, state of the art investment management and portfolio construction. As we cover the theory and math in lecture videos, we'll also implement the concepts in Python, and you'll be able to code along with us so that you have a deep and practical understanding of how those methods work. By the time you are done, not only will you have a foundational understanding of modern computational methods in investment management, you'll have practical mastery in the implementation of those methods.Course 2: Advanced Portfolio Construction and Analysis with Python- The practice of investment management has been transformed in recent years by computational methods. Instead of merely explaining the science, we help you build on that foundation in a practical manner, with an emphasis on the hands-on implementation of those ideas in the Python programming language. In this course, we cover the estimation, of risk and return parameters for meaningful portfolio decisions, and also introduce a variety of state-of-the-art portfolio construction techniques that have proven popular in investment management and portfolio construction due to their enhanced robustness. As we cover the theory and math in lecture videos, we'll also implement the concepts in Python, and you'll be able to code along with us so that you have a deep and practical understanding of how those methods work. By the time you are done, not only will you have a foundational understanding of modern computational methods in investment management, you'll have practical mastery in the implementation of those methods. If you follow along and implement all the lab exercises, you will complete the course with a powerful toolkit that you will be able to use to perform your own analysis and build your own implementations and perhaps even use your newly acquired knowledge to improve on current methods.Course 3: Python and Machine Learning for Asset Management - This course will enable you mastering machine-learning approaches in the area of investment management. It has been designed by two thought leaders in their field, Lionel Martellini from EDHEC-Risk Institute and John Mulvey from Princeton University. Starting from the basics, they will help you build practical skills to understand data science so you can make the best portfolio decisions. The course will start with an introduction to the fundamentals of machine learning, followed by an in-depth discussion of the application of these techniques to portfolio management decisions, including the design of more robust factor models, the construction of portfolios with improved diversification benefits, and the implementation of more efficient risk management models. We have designed a 3-step learning process: first, we will introduce a meaningful investment problem and see how this problem can be addressed using statistical techniques. Then, we will see how this new insight from Machine learning can complete and improve the relevance of the analysis. You will have the opportunity to capitalize on videos and recommended readings to level up your financial expertise, and to use the quizzes and Jupiter notebooks to ensure grasp of concept. At the end of this course, you will master the various machine learning techniques in investment management.Course 4: Python and Machine-Learning for Asset Management with Alternative Data Sets- Over-utilization of market and accounting data over the last few decades has led to portfolio crowding, mediocre performance and systemic risks, incentivizing financial institutions which are looking for an edge to quickly adopt alternative data as a substitute to traditional data. This course introduces the core concepts around alternative data, the most recent research in this area, as well as practical portfolio examples and actual applications. The approach of this course is somewhat unique because while the theory covered is still a main component, practical lab sessions and examples of working with alternative datasets are also key. This course is fo you if you are aiming at carreers prospects as a data scientist in financial markets, are looking to enhance your analytics skillsets to the financial markets, or if you are interested in cutting-edge technology and research as they apply to big data. The required background is: Python programming, Investment theory , and Statistics. This course will enable you to learn new data and research techniques applied to the financial markets while strengthening data science and python skills."
https://www.classcentral.com/course/gcp-data-engineering-18598,"This program provides the skills you need to advance your career in data engineering and recommends training to support your preparation for the industry-recognized Google Cloud Professional Data Engineer certification. Through a combination of presentations, demos, and labs, you will enable data-driven decision making by collecting, transforming, and publishing data; and you'll gain real world experience through a number of hands-on Qwiklabs projects. You'll also have the opportunity to practice key job skills, including designing, building, and running data processing systems; and operationalizing machine-learning models. Upon successful completion of this program, you will earn a certificate of completion to share with your professional network and potential employers. If you would like to become Google Cloud certified and demonstrate your proficiency to design and build data processing systems and operationalize machine learning models on Google Cloud Platform, you will need to register for, and pass the official Google Cloud certification exam. You can find more details on how to register and additional resources to support your preparation at cloud.google.com/certifications.



          Course 1: Google Cloud Platform Big Data and Machine Learning Fundamentals- This 2-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities. At the end of this course, participants will be able to: • Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform • Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform • Employ BigQuery and Cloud Datalab to carry out interactive data analysis • Choose between Cloud SQL, BigTable and Datastore • Train and use a neural network using TensorFlow • Choose between different data processing products on the Google Cloud Platform Before enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following: • A common query language such as SQL • Extract, transform, load activities • Data modeling • Machine learning and/or statistics • Programming in Python Google Account Notes: • Google services are currently unavailable in China.Course 2: Modernizing Data Lakes and Data Warehouses with GCP- The two key components of any data pipeline are data lakes and warehouses. This course highlights use-cases for each type of storage and dives into the available data lake and warehouse solutions on Google Cloud Platform in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a cloud environment. Learners will get hands-on experience with data lakes and warehouses on Google Cloud Platform using QwikLabs.Course 3: Building Batch Data Pipelines on GCP- Data pipelines typically fall under one of the Extra-Load, Extract-Load-Transform or Extract-Transform-Load paradigms. This course describes which paradigm should be used and when for batch data. Furthermore, this course covers several technologies on Google Cloud Platform for data transformation including BigQuery, executing Spark on Cloud Dataproc, pipeline graphs in Cloud Data Fusion and serverless data processing with Cloud Dataflow. Learners will get hands-on experience building data pipeline components on Google Cloud Platform using QwikLabs.Course 4: Building Resilient Streaming Analytics Systems on GCP- *Note: this is a new course with updated content from what you may have seen in the previous version of this Specialization. Processing streaming data is becoming increasingly popular as streaming enables businesses to get real-time metrics on business operations. This course covers how to build streaming data pipelines on Google Cloud Platform. Cloud Pub/Sub is described for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Cloud Dataflow, and how to store processed records to BigQuery or Cloud Bigtable for analysis. Learners will get hands-on experience building streaming data pipeline components on Google Cloud Platform using QwikLabs.Course 5: Smart Analytics, Machine Learning, and AI on GCP- Incorporating machine learning into data pipelines increases the ability of businesses to extract insights from their data. This course covers several ways machine learning can be included in data pipelines on Google Cloud Platform depending on the level of customization required. For little to no customization, this course covers AutoML. For more tailored machine learning capabilities, this course introduces AI Platform Notebooks and BigQuery Machine Learning. Also, this course covers how to productionalize machine learning solutions using Kubeflow. Learners will get hands-on experience building machine learning models on Google Cloud Platform using QwikLabs.Course 6: Preparing for the Google Cloud Professional Data Engineer Exam- From the course: ""The best way to prepare for the exam is to be competent in the skills required of the job."" This course uses a top-down approach to recognize knowledge and skills already known, and to surface information and skill areas for additional preparation. You can use this course to help create your own custom preparation plan. It helps you distinguish what you know from what you don't know. And it helps you develop and practice skills required of practitioners who perform this job. The course follows the organization of the Exam Guide outline, presenting highest-level concepts, ""touchstones"", for you to determine whether you feel confident about your knowledge of that area and its dependent concepts, or if you want more study. You also will learn about and have the opportunity to practice key job skills, including cognitive skills such as case analysis, identifying technical watchpoints, and developing proposed solutions. These are job skills that are also exam skills. You will also test your basic abilities with Activity Tracking Challenge Labs. And you will have many sample questions similar to those on the exam, including solutions. The end of the course contains an ungraded practice exam quiz, followed by a graded practice exam quiz that simulates the exam-taking experience."
https://www.classcentral.com/course/learn-sql-basics-data-science-18613,"This Specialization is intended for a learner with no previous coding experience seeking to develop SQL query fluency. Through four progressively more difficult SQL projects with data science applications, you will cover topics such as SQL basics, data wrangling, SQL analysis, AB testing, distributed computing using Apache Spark, and more. These topics will prepare you to apply SQL creatively to analyze and explore data; demonstrate efficiency in writing queries; create data analysis datasets; conduct feature engineering, use SQL with other data analysis and machine learning toolsets; and use SQL with unstructured data sets.



          Course 1: SQL for Data Science- As data collection has increased exponentially, so has the need for people skilled at using and interacting with data; to be able to think critically, and provide insights to make better decisions and optimize their businesses. This is a data scientist, “part mathematician, part computer scientist, and part trend spotter” (SAS Institute, Inc.). According to Glassdoor, being a data scientist is the best job in America; with a median base salary of $110,000 and thousands of job openings at a time. The skills necessary to be a good data scientist include being able to retrieve and work with data, and to do that you need to be well versed in SQL, the standard language for communicating with database systems. This course is designed to give you a primer in the fundamentals of SQL and working with data so that you can begin analyzing it for data science purposes. You will begin to ask the right questions and come up with good answers to deliver valuable insights for your organization. This course starts with the basics and assumes you do not have any knowledge or skills in SQL. It will build on that foundation and gradually have you write both simple and complex queries to help you select data from tables. You'll start to work with different types of data like strings and numbers and discuss methods to filter and pare down your results. You will create new tables and be able to move data into them. You will learn common operators and how to combine the data. You will use case statements and concepts like data governance and profiling. You will discuss topics on data, and practice using real-world programming assignments. You will interpret the structure, meaning, and relationships in source data and use SQL as a professional to shape your data for targeted analysis purposes. Although we do not have any specific prerequisites or software requirements to take this course, a simple text editor is recommended for the final project. So what are you waiting for? This is your first step in landing a job in the best occupation in the US and soon the world!Course 2: Data Wrangling, Analysis and AB Testing with SQL- This course allows you to apply the SQL skills taught in “SQL for Data Science” to four increasingly complex and authentic data science inquiry case studies. We'll learn how to convert timestamps of all types to common formats and perform date/time calculations. We'll select and perform the optimal JOIN for a data science inquiry and clean data within an analysis dataset by deduping, running quality checks, backfilling, and handling nulls. We'll learn how to segment and analyze data per segment using windowing functions and use case statements to execute conditional logic to address a data science inquiry. We'll also describe how to convert a query into a scheduled job and how to insert data into a date partition. Finally, given a predictive analysis need, we'll engineer a feature from raw data using the tools and skills we've built over the course. The real-world application of these skills will give you the framework for performing the analysis of an AB test.Course 3: Distributed Computing with Spark SQL- This course is for students with SQL experience and now want to take the next step in gaining familiarity with distributed computing using Spark. Students will gain an understanding of when to use Spark and how Spark as an engine uniquely combines Data and AI technologies at scale. The four modules build on one another and by the end of the course the student will understand: Spark architecture, Spark DataFrame, optimizing reading/writing data, and how to build a machine learning model. The first module will introduce Spark, including how Spark works with distributed computing and what are Spark Dataframes. Module 2 covers the core concepts of Spark such as storage vs. computing, caching, partitions and Spark UI. The third module looks at Engineering Data Pipelines covering connecting to databases, schemas and type, file formats and writing good data. The final module looks at the application of Spark with Machine Learning through the business use case, a short introduction to what machine learning is, building and applying models and a final course conclusion. By understanding when to use Spark, either scaling out when the model or data is too large to process on a single machine, or having a need to simply speed up to get faster results, students will hone their SQL skills and become a more adept Data Scientist.Course 4: SQL for Data Science Capstone Project- Data science is a dynamic and growing career field that demands knowledge and skills-based in SQL to be successful. This course is designed to provide you with a solid foundation in applying SQL skills to analyze data and solve real business problems. Whether you have successfully completed the other courses in the Learn SQL Basics for Data Science Specialization or are taking just this course, this project is your chance to apply the knowledge and skills you have acquired to practice important SQL querying and solve problems with data. You will participate in your own personal or professional journey to create a portfolio-worthy piece from start to finish. You will choose a dataset and develop a project proposal. You will explore your data and perform some initial statistics you have learned through this specialization. You will uncover analytics for qualitative data and consider new metrics that make sense from the patterns that surface in your analysis. You will put all of your work together in the form of a presentation where you will tell the story of your findings. Along the way, you will receive feedback through the peer-review process. This community of fellow learners will provide additional input to help you refine your approach to data analysis with SQL and present your findings to clients and management."
https://www.classcentral.com/course/financial-management-18646,"This Specialization covers the fundamentals of strategic financial management, including financial accounting, investments, and corporate finance. You will learn to evaluate major strategic corporate and investment decisions and to understand capital markets and institutions from a financial perspective, and you will develop an integrated framework for value-based financial management and individual financial decision-making. The Financial Management Specialization is part of the University of Illinois iMBA Program. Each course in this Specialization also fulfills a portion of the requirements for a University of Illinois course that can earn you college credit. When you complete the Financial Management Specialization, you will: · Have a solid foundation in developing an integrated framework for strategic financial decision-making. · Have a thorough understanding of financial statements and the financial information they provide, and be able to critically evaluate and analyze cash flows statements. · Understand the management and evaluation of portfolios and firm valuation techniques. · Understand how to incorporate risk and uncertainty into investment decisions and understand how companies make financing and investment decisions.



          Course 1: Financial Accounting: Foundations- In this course, you will learn foundations of financial accounting information. You will start your journey with a general overview of what financial accounting information is and the main financial statements. You will then learn how to code financial transactions in financial accounting language. In the meantime, you will learn about the most important concept in contemporary financial accounting: accrual accounting. You will then critically analyze how firms recognize revenues. Finally, you will finish the course with an analysis of accounting for short-term assets where you will go into detail on how firms account for accounts receivables and inventories. Upon successful completion of this course, you will be able to: • Understand main financial statements and the financial information they provide • Write a financial transaction in financial accounting language and understand how this impacts main financial statements • Understand how accrual accounting and fundamental accounting concepts work • Understand revenue recognition principles and how they impact main financial statements • Account for accounts receivables and inventories. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 2: Financial Accounting: Advanced Topics- In this course, you will explore advanced topics in financial accounting. You will start your journey with accounting for assets with more than one-year life. You will learn in detail how firms account for fixed assets. You will then move to financing of assets and discuss accounting for liabilities. The course will continue with an in-depth exploration of shareholders’ equity. Finally, you will critically evaluate preparation, components, and analysis of cash flows statement. Upon successful completion of this course, you will be able to: • Account for fixed assets • Understand accounting for liabilities • Evaluate shareholders’ equity section of a balance sheet • Understand preparation and information provided by cash flows statement This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 3: Investments I: Fundamentals of Performance Evaluation- In this course, we will discuss fundamental principles of trading off risk and return, portfolio optimization, and security pricing. We will study and use risk-return models such as the Capital Asset Pricing Model (CAPM) and multi-factor models to evaluate the performance of various securities and portfolios. Specifically, we will learn how to interpret and estimate regressions that provide us with both a benchmark to use for a security given its risk (determined by its beta), as well as a risk-adjusted measure of the security’s performance (measured by its alpha). Building upon this framework, market efficiency and its implications for patterns in stock returns and the asset-management industry will be discussed. Finally, the course will conclude by connecting investment finance with corporate finance by examining firm valuation techniques such as the use of market multiples and discounted cash flow analysis. The course emphasizes real-world examples and applications in Excel throughout. This course is the first of two on Investments that I am offering online (“Investments II: Lessons and Applications for Investors” is the second course). The over-arching goals of this course are to build an understanding of the fundamentals of investment finance and provide an ability to implement key asset-pricing models and firm-valuation techniques in real-world situations. Specifically, upon successful completion of this course, you will be able to: • Explain the tradeoffs between risk and return • Form a portfolio of securities and calculate the expected return and standard deviation of that portfolio • Understand the real-world implications of the Separation Theorem of investments • Use the Capital Asset Pricing Model (CAPM) and 3-Factor Model to evaluate the performance of an asset (like stocks) through regression analysis • Estimate and interpret the ALPHA (α) and BETA (β) of a security, two statistics commonly reported on financial websites • Describe what is meant by market efficiency and what it implies for patterns in stock returns and for the asset-management industry • Understand market multiples and income approaches to valuing a firm and its stock, as well as the sensitivity of each approach to assumptions made • Conduct specific examples of a market multiples valuation and a discounted cash flow valuation This course was previously entitled “Financial Evaluation and Strategy: Investments” and was part of a previous specialization entitled ""Improving Business and Finances Operations"", which is now closed to new learner enrollment. “Financial Evaluation and Strategy: Investments” received an average rating of 4.8 out of 5 based on 199 reviews over the period August 2015 through August 2016. You can view a detailed summary of the ratings and reviews for this course in the Course Overview section. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 4: Investments II: Lessons and Applications for Investors- In this course, you will start by reviewing the fundamentals of investments, including the trading off of return and risk when forming a portfolio, asset pricing models such as the Capital Asset Pricing Model (CAPM) and the 3-Factor Model, and the efficient market hypothesis. You will be introduced to the two components of stock returns – dividends and capital gains – and will learn how each are taxed and the incentives provided to investors from a realization-based capital gains tax. You will examine the investment decisions (and behavioral biases) of participants in defined-contribution (DC) pension plans like 401(k) plans in the U.S. and will learn about the evidence regarding the performance of individual investors in their stock portfolios. The course concludes by discussing the evidence regarding the performance of actively-managed mutual funds. You will learn about the fees charged to investors by mutual funds and the evidence regarding the relation between fees charged and fund performance. Segments of the portfolios of mutual funds that may be more likely to outperform and examples of strategies designed to “earn alpha” will also be introduced. Learners are welcome to take this course even if they have not completed ""Investments I: Fundamentals of Performance Evaluation,"" as the first module contain a review of investment fundamentals and regression analysis to get everyone up to speed. Also, the course contains several innovative features, including creative out-of-the-studio introductions followed by quick-hitting ""Module in 60"" countdowns that highlight what will be covered in each module, four ""Faculty Focus"" interview episodes with leading professors in finance, and a summary of each module done with the help of animations! This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 5: Corporate Finance I: Measuring and Promoting Value Creation- In this course you will learn how to use key finance principles to understand and measure business success and to identify and promote true value creation. You will learn how to use accounting information to form key financial ratios to measure a company’s financial health and to manage a company's short-term and long-term liquidity needs. You will also learn how to use valuation techniques to make sound business investment and acquisition decisions. Finally, you will learn how to incorporate risk and uncertainty into investment decisions and how to evaluate the performance of existing investments. Upon successful completion of this course you will be able to: • Understand how companies make investment decisions that create value for shareholders • Use accounting statements to measure the financial health of a company • Forecast and manage a company’s short- and long-term liquidity needs • Measure the contribution of a new project or acquisition to shareholder value • Incorporate risk into investment decisions using the appropriate discount rates • Evaluate the performance of a company or divisions of a company This course was previously entitled Financial Evaluation and Strategy: Corporate Finance. The course received an average rating of 4.7 out of 5 based on 177 reviews over the period of September 2015 through August 2016. A detailed breakdown of ratings and reviews received for the prior version of the course, which is identical in content to the current course, is provided in the course orientation page. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 6: Corporate Finance II: Financing Investments and Managing Risk- In this course you will learn how companies decide on how much debt to take, and whether to raise capital from markets or from banks. You will also learn how to measure and manage credit risk and how to deal with financial distress. You will discuss the mechanics of dividends and share repurchases, and how to choose the best way to return cash to investors. You will also learn how to use derivatives and liquidity management to offset specific sources of financial risk, including currency risks. Finally, You will learn how companies finance merger and acquisition decisions, including leveraged buyouts, and how to incorporate large changes in leverage in standard valuation models. Upon successful completion of this course, you will be able to: • Understand how companies make financing, payout and risk management decisions that create value • Measure the effects of leverage on profitability, risk, and valuation • Manage credit risk and financial distress using appropriate financial tools • Understand the links between payout policies and company performance • Use derivatives and liquidity management to offset financial risks • Pick an appropriate financing package for an M&A or leveraged buyout deal This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu.Course 7: Financial Management Capstone- The Financial Management capstone will provide a learning experience that integrates across all the courses within this specialization. You will analyze a situation taking the vantage point of a company and develop a financial management plan (for instance, a global company working in a specific geography chosen by students’ region or country of residence, or other consideration). You will design a deliverable to create value from the perspective of potential employers while achieving pedagogical and experiential goals. This course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu."
