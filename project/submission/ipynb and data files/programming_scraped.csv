Link,Description
https://www.classcentral.com/course/independent-dco042-python-for-informatics-1010,"This is a course to teach basic Python programming skills through data analysis. The book and course materials are all free and licensed as Creative Commons. There is no complex math in the course, the programs are generally quite short, and the workload is no more than a few hours per week. By the time you complete the course, you will understand be able to read, parse, and manipulate data using Python. Hopefully at the end of the course you will like programming well enough to take another course in programming or web development. You can register and launch, take the course, and earn your place on the map at any time and at your own pace"
https://www.classcentral.com/course/udacity-data-analysis-with-r-1478,"Exploratory data analysis is an approach for summarizing and visualizing the important characteristics of a data set. Promoted by John Tukey, exploratory data analysis focuses on exploring data to understand the data’s underlying structure and variables, to develop intuition about the data set, to consider how that data set came into existence, and to decide how it can be investigated with more formal statistical methods.If you're interested in supplemental reading material for the course check out the Exploratory Data Analysis book. (Not Required)This course is also a part of our Data Analyst Nanodegree.Why Take This Course?You will...  Understand data analysis via EDA as a journey and a way to explore data  Explore data at multiple levels using appropriate visualizations  Acquire statistical knowledge for summarizing data  Demonstrate curiosity and skepticism when performing data analysis  Develop intuition around a data set and understand how the data was generated.



Lesson 1: What is EDA? (1 hour)We'll start by learn about what exploratory data analysis (EDA) is and why it is important. You'll meet the amazing instructors for the course and find out about the course structure and final project.Lesson 2: R Basics (3 hours)EDA, which comes before formal hypothesis testing and modeling, makes use of visual methods to analyze and summarize data sets. R will be our tool for generating those visuals and conducting analyses. In this lesson, we will install RStudio and packages, learn the layout and basic commands of R, practice writing basic R scripts, and inspect data sets.Lesson 3: Explore One Variable (4 hours)We perform EDA to understand the distribution of a variable and to check for anomalies and outliers. Learn how to quantify and visualize individual variables within a data set as we begin to make sense of a pseudo-data set of Facebook users. While the data set does not contain real user data, it does contain a wealth of information. Through the lesson, we will create histograms and boxplots, transform variables, and examine tradeoffs in visualizations.Problem Set 3 (2 hours)Lesson 4: Explore Two Variables (4 hours)EDA allows us to identify the most important variables and relationships within a data set before building predictive models. In this lesson, we will learn techniques for exploring the relationship between any two variables in a data set. We'll create scatter plots, calculate correlations, and investigate conditional means.Problem Set 4 (2 hours)Lesson 5: Explore Many Variables (4 hours)Data sets can be complex. In this lesson, we will learn powerful methods and visualizations for examining relationships among multiple variables. We'll learn how to reshape data frames and how to use aesthetics like color and shape to uncover more information. Extending our knowledge of previous plots, we'll continue to build intuition around the Facebook data set and explore some new data sets as well.Problem Set 5 (2 hours)Lesson 6: Diamonds and Price Predictions (2 hours)Investigate the diamonds data set alongside Facebook Data Scientist, Solomon Messing. He'll recap many of the strategies covered in the course and show how predictive modeling can allow us to determine a good price for a diamond. As a final project, you will create your own exploratory data analysis on a data set of your choice.Final Project (10+ hours)You've explored simulated Facebook user data and the diamonds data set. Now, it's your turn to conduct your own exploratory data analysis. Choose one data set to explore (one provided by Udacity or your own) and create a RMD file that uncovers the patterns, anomalies and relationships of the data set."
https://www.classcentral.com/course/edx-introduction-to-computational-thinking-and-data-science-1779,"6.00.2x will teach you how to use computation to accomplish a variety of goals and provides you with a brief introduction to a variety of topics in computational problem solving . This course is aimed at students with some prior programming experience in Python and a rudimentary knowledge of computational complexity. You will spend a considerable amount of time writing programs to implement the concepts covered in the course. For example, you will write a program that will simulate a robot vacuum cleaning a room or will model the population dynamics of viruses replicating and drug treatments in a patient's body.
Topics covered include:

Advanced programming in Python 3
Knapsack problem, Graphs and graph optimization
Dynamic programming
Plotting with the pylab package
Random walks
Probability, Distributions
Monte Carlo simulations
Curve fitting
Statistical fallacies"
https://www.classcentral.com/course/swayam-python-for-data-science-14266,"The course aims at equipping participants to be able to use python programming for solving data science problems.
 
INTENDED AUDIENCE : Final Year Undergraduates
PRE-REQUISITES : Knowledge of basic data science algorithms
INDUSTRY SUPPORT : Honeywell, ABB, Ford, Gyan data pvt. Ltd.

      


COURSE LAYOUT
Week 1:•BASICS OF PYTHON SPYDER (TOOL)


• Introduction Spyder
• Setting working Directory
• Creating and saving a script file
• File execution, clearing console, removing variables from environment, clearing environment
• Commenting script files
• Variable creation
• Arithmetic and logical operators
• Data types and associated operations



Week 2:•Data Structures


Lists
Tuples
Dictionary
Sets


• Numpy


Array
Matrix and associated operations
Linear algebra and related operations

Week 3:•Pandas dataframe and dataframe related operations on Toyota Corolla dataset


Reading files
Exploratory data analysis
Data preparation and preprocessing


•Data visualization on Toyoto Corolla dataset using matplotlib and seaborn libraries


Scatter plot
Line plot
Bar plot
Histogram
Box plot
Pair plot


•Control structures using Toyota Corolla dataset


if-else family
for loop
for loop with if break
while loop


•FunctionsWeek 4:CASE STUDY
•Regression


Predicting price of pre-owned cars


•Classification


Classifying personal income"
https://www.classcentral.com/course/edx-big-data-analysis-with-apache-spark-3026,"Organizations use their data to support and influence decisions and build data-intensive products and services, such as recommendation, prediction, and diagnostic systems. The collection of skills required by organizations to support these functions has been grouped under the term ‘data science’.
This statistics and data analysis course will attempt to articulate the expected output of data scientists and then teach students how to use PySpark (part of Spark) to deliver against these expectations. The course assignments include log mining, textual entity recognition, and collaborative filtering exercises that teach students how to manipulate data sets using parallel processing with PySpark.
This course covers advanced undergraduate-level material. It requires a programming background and experience with Python (or the ability to learn it quickly). All exercises will use PySpark (the Python API for Spark), and previous experience with Spark equivalent to Introduction to Apache Spark, is required."
https://www.classcentral.com/course/edx-data-science-r-basics-9253,"The first in our Professional Certificate Program in Data Science, this course will introduce you to the basics of R programming. You can better retain R when you learn it to solve a specific problem, so you'll use a real-world dataset about crime in the United States. You will learn the R skills needed to answer essential questions about differences in crime across the different states. 
We'll cover R's functions and data types, then tackle how to operate on vectors and when to use advanced functions like sorting. You'll learn how to apply general programming features like ""if-else,"" and ""for loop"" commands, and how to wrangle, analyze and visualize data. 
Rather than covering every R skill you might need, you'll build a strong foundation to prepare you for the more in-depth courses later in the series, where we cover concepts like probability, inference, regression, and machine learning. We help you develop a skill set that includes R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux, version control with git and GitHub, and reproducible document preparation with RStudio. 
The demand for skilled data science practitioners is rapidly growing, and this series prepares you to tackle real-world data analysis challenges."
https://www.classcentral.com/course/edx-foundations-of-data-science-computational-thinking-with-python-10319,"We live in an era of unprecedented access to data. Understanding how to organize and leverage the vast amounts of information at our disposal are critical skills that allow us to infer upon the world and make informed decisions. This course will introduce you to such skills. 
To work with large amounts of data, you will need to harness the power of computation through programming. This course teaches you basic programming skills for manipulating data. You will learn how to use Python to organize and manipulate data in tables, and to visualize data effectively. No prior experience with programming or Python is needed, nor is any statistics background necessary.
The examples given in the course involve real world data from diverse settings. Not all data is numerical – you will work with different types of data from a variety of domains. Though the term “data science” is relatively new, the fundamental ideas of data science are not. The course includes powerful examples that span the centuries from the Victorian era to the present day. 
This course emphasizes learning through doing: you will work on large real-world data sets through interactive assignments to apply the skills you learn. Throughout, the underlying thread is that data science is a way of thinking, not just an assortment of methods. You will also hone your interpretation and communication skills, which are essential skills for data scientists."
https://www.classcentral.com/course/comparinggenomes-3291,"Once we have sequenced genomes in the previous course, we would like to compare them to determine how species have evolved and what makes them different.

In the first half of the course, we will compare two short biological sequences, such as genes (i.e., short sequences of DNA) or proteins.  We will encounter a powerful algorithmic tool called dynamic programming that will help us determine the number of mutations that have separated the two genes/proteins.

In the second half of the course, we will ""zoom out"" to compare entire genomes, where we see large scale mutations called genome rearrangements, seismic events that have heaved around large blocks of DNA over millions of years of evolution.  Looking at the human and mouse genomes, we will ask ourselves: just as earthquakes are much more likely to occur along fault lines, are there locations in our genome that are ""fragile"" and more susceptible to be broken as part of genome rearrangements?  We will see how combinatorial algorithms will help us answer this question.

Finally, you will learn how to apply popular bioinformatics software tools to solve problems in sequence alignment, including BLAST.
      


          Week 1: Introduction to Sequence Alignment
    -Welcome to class!If you joined us in the previous course in this Specialization, then you became an expert at assembling genomes and sequencing antibiotics. The next natural question to ask is how to compare DNA and amino acid sequences. This question will motivate this week's discussion of sequence alignment, which is the first of two questions that we will ask in this class (the algorithmic methods used to answer them are shown in parentheses):How Do We Compare DNA Sequences? (Dynamic Programming)Are There Fragile Regions in the Human Genome? (Combinatorial Algorithms)As in previous courses, each of these two chapters is accompanied by a Bioinformatics Cartoon created by talented artist Randall Christopher and serving as a chapter header in the Specialization's bestselling print companion. You can find the first chapter's cartoon at the bottom of this message. Why have taxis suddenly become free of charge in Manhattan? Where did Pavel get so much spare change?  And how should you get dressed in the morning so that you aren't late to your job as a crime-stopping superhero? Answers to these questions, and many more, in this week's installment of the course.

Week 2: From Finding a Longest Path to Aligning DNA Strings
    -Welcome to Week 2 of the class!

Last week, we saw how touring around Manhattan and making change in a Roman shop help us find a longest common subsequence of two DNA or protein strings.

This week, we will study how to find a highest scoring alignment of two strings. We will see that regardless of the underlying assumptions that we make regarding how the strings should be aligned, we will be able to phrase our alignment problem as an instance of finding the longest path in a directed acyclic graph.

Week 3: Advanced Topics in Sequence Alignment
    -Welcome to Week 3 of the class!

Last week, we saw how a variety of different applications of sequence alignment can all be reduced to finding the longest path in a Manhattan-like graph.

This week, we will conclude the current chapter by considering a few advanced topics in sequence alignment. For example, if we need to align long strings, our current algorithm will consume a huge amount of memory. Can we find a more memory-efficient approach? And what should we do when we move from aligning just two strings at a time to aligning many strings?

Week 4: Genome Rearrangements and Fragility
    -Welcome to Week 4 of the class!

You now know how to compare two DNA (or protein) strings.  But what if we wanted to compare entire genomes? When we ""zoom out"" to the genome level, we find that substitutions, insertions, and deletions don't tell the whole story of evolution: we need to model more dramatic evolutionary events known as genome rearrangements, which wrench apart chromosomes and put them back together in a new order. A natural question to ask is whether there are ""fragile regions"" hidden in your genome where chromosome breakage has occurred more often over millions of years. This week, we will begin addressing this question by asking how we can compute the number of rearrangements on the evolutionary path connecting two species.

You can find this week's Bioinformatics Cartoon from Randall Christopher at the bottom of this E-mail. What do earthquakes and a stack of pancakes have to do with species evolution? Keep learning to find out!



Week 5: Applying Genome Rearrangement Analysis to Find Genome Fragility
    -Last week, we asked whether there are fragile regions in the human genome. Then, we took a lengthy detour to see how to compute a distance between species genomes, a discussion that we will continue this week.

It is probably unclear how computing the distance between two genomes can help us understand whether fragile regions exist. If so, please stay tuned -- we will see that the connection between these two concepts will yield a surprising conclusion to the class.

Week 6: Bioinformatics Application Challenge
    -In the sixth and final week of the course, we will apply sequence alignment algorithms to infer the non-ribosomal code."
https://www.classcentral.com/course/bigdataschool-2482,"This is not a class as it is commonly understood; it is the set of materials from a summer school offered by Caltech and JPL, in the sense used by most scientists: an intensive period of learning of some advanced topics, not on an introductory level. The school will cover a variety of topics, with a focus on practical 
computing applications in research: the skills needed for a 
computational (""big data"") science, not computer science.  The specific 
focus will be on applications in astrophysics, earth science (e.g., 
climate science) and other areas of space science, but with an emphasis 
on the general tools, methods, and skills that would apply across other 
domains as well.  It is aimed at an audience of practicing researchers who already have a strong background in computation and data analysis.  The lecturers include computational science and 
technology experts from Caltech and JPL.Students can evaluate their own progress, but there will be no tests, exams, and no formal credit or certificates will be offered.



The anticipated schedule of lectures (subject to changes):Each bullet bellow corresponds to a set of materials that includes approximately 2 hours of video lectures, various links and supplementary materials, plus some on-line, hands-on exercises.1. Introduction to the school.  Software architectures.  Introduction to Machine Learning.2. Best programming practices.  Information retrieval.3. Introduction to R.  Markov Chain Monte Carlo.4. Statistical resampling and inference.5. Databases.6. Data visualization.7. Clustering and classification.8. Decision trees and random forests.9. Dimensionality reduction.  Closing remarks."
https://www.classcentral.com/course/edx-quantitative-biology-workshop-1984,"Do you have an interest in biology and quantitative tools? Do you know computational methods but do not realize how they apply to biological problems? Do you know biology but do not understand how scientists really analyze complicated data? 7.QBWx: Quantitative Biology Workshop is designed to give learners exposure to the application of quantitative tools to analyze biological data at an introductory level. The Biology Department of MIT has run this workshop-style course as part of a one-week outreach program for students from other universities. With 7.QBWx, we can give more learners from around the world the chance to discover quantitative biology. We hope that this series of workshops encourages learners to explore new interests and take more biology and computational courses.
We expect that learners from 7.00x Introduction to Biology - The Secret of Life or an equivalent course can complete this workshop-based course without a background in programming. The course content will introduce programming languages but will not teach any one language in a comprehensive manner. The content of each week varies. We want learners to have an introduction to multiple languages and tools to find a topic that they would want to explore more. We recommend that learners try to complete each week to find what interests them the most. 
This workshop includes activities on the following biological topics: population biology, biochemical equilibrium and kinetics, molecular modeling of enzymes, visual neuroscience, global and single-cell gene expression, development, and genomics. The tools and programming languages include MATLAB, PyMOL, Python, and R. This course does not require learners to download MATLAB. All MATLAB activities run and are graded within the edX platform. We do recommend that participants download a few other free tools for the activities so that they learn how to use the same tools and programs that scientists use.
Workshop Content Creators and Residential Leaders
Gregory Hale, Michael Goard, Ben Stinson, Kunle Demuren, Sara Gosline, Glenna Foight, Leyla Isik, Samir El-Boustani, Gerald Pho, and Rajeev Rikhye
Residential Outreach Workshop Organizer and Creator
Mandana Sassanfar



            Read more"
https://www.classcentral.com/course/data-mining-with-weka-7805,"Learn how to mine your own data
Today’s world generates more data than ever before! Being able to turn it into useful information is a key skill. This course introduces you to practical data mining using the Weka workbench. We’ll dispel the mystery that surrounds the subject. We’ll explain the principles of popular algorithms. We’ll show you how to use them in practical applications. You’ll get plenty of experience actually mining data during the course, and afterwards you’ll be well equipped to mine your own. Weka originated at the University of Waikato in NZ, and Ian Witten has authored a leading book on data mining.
This course is aimed at anyone who deals in data. It involves no computer programming, although you need some experience with using computers for everyday tasks. High school maths should be more than enough and you’ll need an understanding of some elementary statistics concepts (means and variances).
You will download the free Weka software during Week 1. It runs on any computer, under Windows, Linux, or Mac. It has been downloaded millions of times and is being used all around the world.
(Note: Depending on your computer and system version, you may need admin access to install Weka.)"
https://www.classcentral.com/course/independent-advanced-data-mining-with-weka-6146,"This course follows on from Data Mining with Weka and More Data Mining with Weka. It provides a deeper account of specialized data mining tools and techniques. Again the emphasis is on principles and practical data mining using Weka, rather than mathematical theory or advanced details of particular algorithms. Students will analyse time series data, mine data streams, use Weka to access other data mining packages including the popular R statistical computing language, script Weka in Python, and deploy it within a cluster computing framework. The course also includes case studies of applications such as classifying tweets, functional MRI data, image classification, and signal peptide prediction.
 



Pre-course survey OpenClass 1 - Time series forecasting OpenClass 2 - Data stream mining with Weka and MOA Closed;Mid-course assessment Closed;Class 3 - Interfacing to R and other data mining packages Closed;Class 4 - Distributed processing with Apache SPARK Closed;Class 5 - Scripting Weka in Python Closed;Post-course assessment Closed;Post-course survey Closed;"
https://www.classcentral.com/course/datacamp-intro-to-python-for-data-science-7631,"Python is a general-purpose programming language that is becoming more and more popular for doing data science. Companies worldwide are using Python to harvest insights from their data and get a competitive edge. Unlike any other Python tutorial, this course focuses on Python specifically for data science. In our Intro to Python class, you will learn about powerful ways to store and manipulate data as well as cool data science tools to start your own analyses. Enter DataCamp’s online Python curriculum.



          Chapter One: Create your first variables and acquaint yourself with Python's basic data types
Chapter Two: Learn to store, access and manipulate data in Python lists
Chapter Three: Learn about using functions, methods and packages.
Chapter Four: Learn to work with the Numpy array"
https://www.classcentral.com/course/edx-data-science-visualization-10347,"As part of our Professional Certificate Program in Data Science, this course covers the basics of data visualization and exploratory data analysis. We will use three motivating examples and ggplot2, a data visualization package for the statistical programming language R. We will start with simple datasets and then graduate to case studies about world health, economics, and infectious disease trends in the United States. 
We'll also be looking at how mistakes, biases, systematic errors, and other unexpected problems often lead to data that should be handled with care. The fact that it can be difficult or impossible to notice a mistake within a dataset makes data visualization particularly important. 
The growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many areas. Data visualization provides a powerful way to communicate data-driven findings, motivate analyses, and detect flaws. This course will give you the skills you need to leverage data to reveal valuable insights and advance your career."
https://www.classcentral.com/course/dataanalysis-386,"You have probably heard that this is the era of “Big Data”. Stories about
    companies or scientists using data to recommend movies, discover who is
    pregnant based on credit card receipts, or confirm the existence of the
    Higgs Boson regularly appear in Forbes, the Economist, the Wall Street
    Journal, and The New York Times. But how does one turn data into this type
    of insight? The answer is data analysis and applied statistics. Data analysis
    is the process of finding the right data to answer your question, understanding
    the processes underlying the data, discovering the important patterns in
    the data, and then communicating your results to have the biggest possible
    impact. There is a critical shortage of people with these skills in the
    workforce, which is why Hal Varian (Chief Economist at Google) says that
    being a statistician will be the sexy job for the next 10 years.
    
This course is an applied statistics course focusing on data analysis.
    The course will begin with an overview of how to organize, perform, and
    write-up data analyses. Then we will cover some of the most popular and
    widely used statistical methods like linear regression, principal components
    analysis, cross-validation, and p-values. Instead of focusing on mathematical
    details, the lectures will be designed to help you apply these techniques
    to real data using the R statistical programming language, interpret the
    results, and diagnose potential problems in your analysis. You will also
    have the opportunity to critique and assist your fellow classmates with
    their data analyses.
      


            Read more"
https://www.classcentral.com/course/edx-introduction-to-r-for-data-science-3928,"R is rapidly becoming the leading language in data science and statistics. Today, R is the tool of choice for data science professionals in every industry and field. Whether you are full-time number cruncher, or just the occasional data analyst, R will suit your needs.
This introduction to R programming course will help you master the basics of R. In seven sections, you will cover its basic syntax, making you ready to undertake your own first data analysis using R. Starting from variables and basic operations, you will eventually learn how to handle data structures such as vectors, matrices, data frames and lists. In the final section, you will dive deeper into the graphical capabilities of R, and create your own stunning data visualizations. No prior knowledge in programming or data science is required.
What makes this course unique is that you will continuously practice your newly acquired skills through interactive in-browser coding challenges using the DataCamp platform. Instead of passively watching videos, you will solve real data problems while receiving instant and personalized feedback that guides you to the correct solution.
Enjoy! 
edX offers financial assistance for learners who want to earn Verified Certificates but who may not be able to pay the fee. To apply for financial assistance, enroll in the course, then follow this link to complete an application for assistance.



Section 1: Introduction to Basics
Take your first steps with R. Discover the basic data types in R and assign your first variable. 
Section 2: Vectors
Analyze gambling behaviour using vectors. Create, name and select elements from vectors. 
Section 3: Matrices
Learn how to work with matrices in R. Do basic computations with them and demonstrate your knowledge by analyzing the Star Wars box office figures. 
Section 4: Factors
R stores categorical data in factors. Learn how to create, subset and compare categorical data. 
Section 5: Data Frames
When working R, you'll probably deal with Data Frames all the time. Therefore, you need to know how to create one, select the most interesting parts of it, and order them. 
Section6: Lists
Lists allow you to store components of different types. Section 6 will show you how to deal with lists. 
Section 7: Basic Graphics
Discover R's packages to do graphics and create your own data visualizations."
https://www.classcentral.com/course/information-visualization-programming-d3-11817,"In this course you will learn how to use D3.js to create powerful visualizations for web. Learning D3.js will enable you to create many different types of visualization and to visualize many different data types. It will give you the freedom to create something as simple as a bar chart as well your own new revolutionary technique. 

In this course we will cover the basics of creating visualizations with D3 as well as how to deal with tabular data, geography and networks. By the end of this course you will be able to:

- Create bar and line charts
- Create choropleth and symbol maps
- Create node-link diagrams and tree maps
- Implement zooming and brushing
- Link two or more views through interaction

The course mixes theoretical and practical lectures. We will show you step by step how to use the library to build actual visualizations and what theoretical concepts lie behind them. Throughout the course you will learn skills that will lead you to building a whole application by the end of the lectures (a fully working visualization system to visualize airlines routes).

This course is the third one of the “Specialization in Information Visualization"". The course expects you to have some basic knowledge of programming as well as some basic visualization skills.
      


          Introduction to web and d3
    -In this module we will focus on the basics of web development and d3.js

Dealing  & drawing with data
    -In this week we will learn how can we load and manipulate data using d3.js

Lines, Arcs, and maps

Layouts and interaction"
https://www.classcentral.com/course/linear-models-6180,"Welcome to the Advanced Linear Models for Data Science Class 1: Least Squares. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:

- A basic understanding of linear algebra and multivariate calculus.
- A basic understanding of statistics and regression models.
- At least a little familiarity with proof based mathematics.
- Basic knowledge of the R programming language.

After taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models.
      


          Background
    -We cover some basic matrix algebra results that we will need throughout the class. This includes some basic vector derivatives. In addition, we cover some some basic uses of matrices to create summary statistics from data. This includes calculating and subtracting means from observations (centering) as well as calculating the variance.


One and two parameter regression
    -In this module, we cover the basics of regression through the origin and linear regression. Regression through the origin is an interesting case, as one can build up all of multivariate regression with it.

Linear regression
    -In this lecture, we focus on linear regression, the most standard technique for investigating unconfounded linear relationships. 

General least squares
    -We now move on to general least squares where an arbitrary full rank design matrix is fit to a vector outcome.

Least squares examples
    -Here we give some canonical examples of linear models to relate them to techniques that you may already be using.

Bases and residuals
    -Here we give a very useful kind of linear model, that is decomposing a signal into a basis expansion."
https://www.classcentral.com/course/data-science-8118,"Work with airline data to learn the fundamentals of the R platform.
We live in a data driven world. So how can we make the most of it? Have you ever wondered how data-driven decisions are made?
This course will use airline data to demonstrate key concepts involved in the analysis of big data. In this course you will learn how to use the R platform to manage data. The course serves as an introduction to the R software. It lays the foundation for anyone to begin studying data science and its applications, or to prepare learners to take more advanced courses related to data science, such as machine learning and computational statistics.
This course is for anyone who is interested in discovering more about data.  Beginners in using R are welcome.  Prior experience is not required.
You will need to download, install, and use the R platform.  A desktop or laptop computer is also needed to take this course as opposed to a mobile device."
https://www.classcentral.com/course/information-visualization-advanced-techn-11818,"This course aims to introduce learners to advanced visualization techniques beyond the basic charts covered in Information Visualization: Fundamentals. These techniques are organized around data types to cover advance methods for: temporal and spatial data, networks and trees and textual data. In this module we also teach learners how to develop innovative techniques in D3.js.

Learning Goals
Goal: Analyze the design space of visualization solutions for various kinds of data visualization problems. Learn what designs are available for a given problem and what are their respective advantages and disadvantages.
- Temporal
- Spatial
- Spatio-Temporal
- Networks
- Trees
- Text

This is the fourth course in the Information Visualization Specialization. The course expects you to have some basic knowledge of programming as well as some basic visualization skills (as those introduced in the first course of the specialization).
      


          Visualizing Geographical Data

Visualizing Network Data

Visualizing Temporal Data

Interaction and Multiple Views"
https://www.classcentral.com/course/scicomp-764,"Computation and simulation are increasingly important in all aspects of
    science and engineering. At the same time writing efficient computer programs
    to take full advantage of current computers is becoming increasingly difficult.
    Even laptops now have 4 or more processors, but using them all to solve
    a single problem faster often requires rethinking the algorithm to introduce
    parallelism, and then programming in a language that can express this parallelism. 
    Writing efficient programs also requires some knowledge of machine arithmetic,
    computer architecture, and memory hierarchies.
Although parallel computing will be covered, this is not a class
    on the most advanced techniques for using supercomputers, which these days
    have tens of thousands of processors and cost millions of dollars. Instead,
    the goal is to teach tools that you can use immediately on your own laptop,
    desktop, or a small cluster. Cloud computing will also be discussed, and
    students who don't have a multiprocessor computer of their own will still
    be able to do projects using Amazon Web Services at very low cost.
Along the way there will also be discussion of software engineering tools
    such as debuggers, unit testing, Makefiles, and the use of version control
    systems. After all, your time is more valuable than computer time, and
    a program that runs fast is totally useless if it produces the wrong results.
High performance programming is also an important aspect of high
    performance scientific computing, and so another main theme of the course
    is the use of basic tools and techniques to improve your efficiency as
    a computational scientist.



            Read more
          



The use of a variety of languages and techniques will be integrated throughout
    the course as much as possible, rather than taught linearly. The topics
    below will be covered at an introductory level, with the goal of learning
    enough to feel comfortable starting to use them in your everyday work.
    Once you've reached that level, abundant resources are available on the
    web to learn the more advanced features that are most relevant for you.

Working at the command line in Unix-like shells (e.g. Linux or a Mac OSX
        terminal).
Version control systems, particularly git, and the use of Github and Bitbucket
        repositories.
Work habits for documentation of your code and reproducibility of your
        results.
Interactive Python using IPython, and the IPython Notebook.
Python scripting and its uses in scientific computing.
Subtleties of computer arithmetic that can affect program correctness.
How numbers are stored: binary vs. ASCII representations, efficient I/O.
Fortran 90, a compiled language that is widely used in scientific computing.
Makefiles for building software and checking dependencies.
The high cost of data communication.  Registers, cache, main memory,
        and how this memory hierarchy affects code performance. 
OpenMP on top of Fortran for parallel programming of shared memory computers,
        such as a multicore laptop.
 MPI on top of Fortran for distributed memory parallel programming,
        such as on a cluster.
Parallel computing in IPython.
Debuggers, unit tests, regression tests, verification and validation of
        computer codes.
Graphics and visualization of computational results using Python."
https://www.classcentral.com/course/linear-models-2-7476,"Welcome to the Advanced Linear Models for Data Science Class 2: Statistical Linear Models. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:

- A basic understanding of linear algebra and multivariate calculus.
- A basic understanding of statistics and regression models.
- At least a little familiarity with proof based mathematics.
- Basic knowledge of the R programming language.

After taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models.
      


          Introduction and expected values
    -In this module, we cover the basics of the course as well as the prerequisites. We then cover the basics of expected values for multivariate vectors. We conclude with the moment properties of the ordinary least squares estimates. 

The multivariate normal distribution
    -In this module, we build up the multivariate and singular normal distribution by starting with iid normals.

Distributional results
    -In this module, we build the basic distributional results that we see in multivariable regression.

Residuals
    -In this module we will revisit residuals and consider their distributional results. We also consider the so-called PRESS residuals and show how they can be calculated without re-fitting the model."
https://www.classcentral.com/course/edx-stochastic-processes-data-analysis-and-computer-simulation-8246,"The motion of falling leaves or small particles diffusing in a fluid is highly stochastic in nature. Therefore, such motions must be modeled as stochastic processes, for which exact predictions are no longer possible. This is in stark contrast to the deterministic motion of planets and stars, which can be perfectly predicted using celestial mechanics.
This course is an introduction to stochastic processes through numerical simulations, with a focus on the proper data analysis needed to interpret the results. We will use the Jupyter (iPython) notebook as our programming environment. It is freely available for Windows, Mac, and Linux through the Anaconda Python Distribution.
The students will first learn the basic theories of stochastic processes. Then, they will use these theories to develop their own python codes to perform numerical simulations of small particles diffusing in a fluid. Finally, they will analyze the simulation data according to the theories presented at the beginning of course.
At the end of the course, we will analyze the dynamical data of more complicated systems, such as financial markets or meteorological data, using the basic theory of stochastic processes.



Week 1: Python programming for beginners  - Using Python, iPython, and Jupyter notebook - Making graphs with matplotlib - The Euler method for numerical integration - Simulating a damped harmonic oscillator Week 2: Distribution function and random number - Stochastic variable and distribution functions - Generating random numbers with Gaussian/binomial/Poisson distributions - The central limiting theorem - Random walkWeek 3: Brownian motion 1: basic theories - Basic knowledge of Stochastic process - Brownian motion and the Langevin equation - The linear response theory and the Green-Kubo formula Week 4: Brownian motion 2: computer simulation - Random force in the Langevin equation - Simple Python code to simulate Brownian motion - Simulations with on-the-fly animation Week 5: Brownian motion 3: data analyses - Distribution and time correlation - Mean square displacement and diffusion constant - Interacting Brownian particles Week 6: Stochastic processes in the real world - Time variations and distributions of real world processes - A Stochastic Dealer Model I - A Stochastic Dealer Model II - A Stochastic Dealer Model III"
https://www.classcentral.com/course/eduopen-data-mining-classification-7680,"Learn how to formulate and solve classification problems for use in Data Mining and Business Intelligence applications such as; fraud detection, customer churning, network intrusion detection, etc... You will learn how to develop, validate and apply a data mining workflow to solve binary and non-binary classification problems. The course is self-contained, and it does not require any programming skills. Hands-on lectures are based on the KNIME open source software platform."
https://www.classcentral.com/course/france-universite-numerique-exploratory-multivariate-data-analysis-9936,"About this course This 5th edition of the MOOC starts on March 2, 2020. Exploratory multivariate data analysis is studied and teached in a French-way since a long time in France. This course focuses on four essential and basic methods, those with the largest potential in terms of applications: principal component analysis (PCA) when variables are quantitative, correspondence analysis (CA) and multiple correspondence analysis (MCA) when variables are categorical and clustering. An extension to Multiple Factor Analysis (MFA) will give you the opportunity to analyse more complex dataset that are structured by groups. This course is application-oriented; formalism and mathematics writing have been reduced as much as possible while examples and intuition have been emphasized and the numerous exercises done with FactoMineR (a package of the free R software) will make the participant efficient and reliable face to data analysis. We hope that with this course, the participant will be fully equipped (theory, examples, software) to confront multivariate real-life data.



Course Schedule Week 1. Principal Component Analysis Data - Practicalities Studying individuals and variables Aids for interpretation PCA in practice using FactoMineR  Week 2. Correspondence Analysis Data - introduction and independence model Visualizing the row and column clouds Inertia and percentage of inertia Simultaneous representation Interpretation aids Correspondance Analysis in practice using FactoMineR  Week 3. Multiple Correspondence Analysis Data - issues Visualizing the point cloud of individuals Visualizing the point cloud of categories - simultaneous representation Interpretation aids Multiple Correspondance Analysis in practice using FactoMineR  Week 4. Clustering Hierarchical clustering An example, and choosing the number of classes Partitioning methods and other details Characterizing the classes Clustering in practice using FactoMineR  Week 5 : Multiple Factor Analysis Data - issues  Balancing groups and choosing a weighting for the variables  Studying and visualizing the groups of variables  Visualizing the partial points  Visualizing the separate analyses  Taking into account groups of categorical variables  Taking into account contingency tables Interpretation aids Multiple Factor Analysis in practice using FactoMineR"
https://www.classcentral.com/course/open-education-by-blackboard-python-for-informatics-exploring-information-795,"This is a course to teach basic Python programming skills through data analysis. The book and course materials are all free and licensed as Creative Commons. There is no complex math in the course, the programs are generally quite short, and the workload is no more than a few hours per week. By the time you complete the course, you will understand be able to read, parse, and manipulate data using Python. Hopefully at the end of the course you will like programming well enough to take another course in programming or web development. The free, reusable, and remixable course materials will be made available for you to download, use and reuse."
https://www.classcentral.com/course/edx-ph525x-data-analysis-for-genomics-1615,"*Note - This is an Archived course*

This is a past/archived course. At this time, you can only explore this course in a self-paced fashion. Certain features of this course may not be active, but many people enjoy watching the videos and working with the materials. Make sure to check for reruns of this course. 

The purpose of this course is to enable students to analyze and interpret data generated by modern genomics technology, specifically microarray data and next generation sequencing data. We will focus on applications common in public health and biomedical research: measuring gene expression differences between populations, associated genomic variants to disease, measuring epigenetic marks such as DNA methylation, and transcription factor binding sites.

The course covers the necessary statistical concepts needed to properly design experiments and analyze the high dimensional data produced by these technologies. These include estimation, hypothesis testing, multiple comparison corrections, modeling, linear models, principle component analysis, clustering, nonparametric and Bayesian techniques. Along the way, students will learn to analyze data using the R programming language and several packages from the Bioconductor project.

Currently, biomedical research groups around the world are producing more data than they can handle. The training and skills acquired by taking this course will be of significant practical use for these groups. The learning that will take place in this course will allow for greater success in making biological discoveries and improving individual and population health.

Before your course starts, try the new edX Demo where you can explore the fun, interactive learning environment and virtual labs. Learn more.
      


            Read more"
https://www.classcentral.com/course/bd2klincs-3024,"The Library of Integrative Network-based Cellular Signatures (LINCS) is an NIH Common Fund program. The idea is to perturb different types of human cells with many different types of perturbations such as: drugs and other small molecules; genetic manipulations such as knockdown or overexpression of single genes; manipulation of the extracellular microenvironment conditions, for example, growing cells on different surfaces, and more. These perturbations are applied to various types of human cells including induced pluripotent stem cells from patients, differentiated into various lineages such as neurons or cardiomyocytes. Then, to better understand the molecular networks that are affected by these perturbations, changes in level of many different variables are measured including: mRNAs, proteins, and metabolites, as well as cellular phenotypic changes such as changes in cell morphology. The BD2K-LINCS Data Coordination and Integration Center (DCIC) is commissioned to organize, analyze, visualize and integrate this data with other publicly available relevant resources. In this course we briefly introduce the DCIC and the various Centers that collect data for LINCS. We then cover metadata and how metadata is linked to ontologies. We then present data processing and normalization methods to clean and harmonize LINCS data. This follow discussions about how data is served as RESTful APIs. Most importantly, the course covers computational methods including: data clustering, gene-set enrichment analysis, interactive data visualization, and supervised learning. Finally, we introduce crowdsourcing/citizen-science projects where students can work together in teams to extract expression signatures from public databases and then query such collections of signatures against LINCS data for predicting small molecules as potential therapeutics.
      


            Read more
          



          The Library of Integrated Network-based Cellular Signatures (LINCS) Program Overview
    -This module provides an overview of the concept behind the LINCS program; and tutorials on how to get started with using the LINCS L1000 dataset.

Metadata and Ontologies
    -This module includes a broad high level description of the concepts behind metadata and ontologies and how these are applied to LINCS datasets.

Serving Data with APIs
    -In this module we explain the concept of accessing data through an application programming interface (API).

Bioinformatics Pipelines
    -This module describes the important concept of a Bioinformatics pipeline.

The Harmonizome
    -This module describes a project that integrates many resources that contain knowledge about genes and proteins. The project is called the Harmonizome, and it is implemented as a web-server application available at: http://amp.pharm.mssm.edu/Harmonizome/ 

Data Normalization
    -This module describes the mathematical concepts behind data normalization.

Data Clustering
    -This module describes the mathematical concepts behind data clustering, or in other words unsupervised learning - the identification of patterns within data without considering the labels associated with the data. 

Midterm Exam
    -The Midterm Exam consists of 45 multiple choice questions which covers modules 1-7. Some of the questions may require you to perform some analysis with the methods you learned throughout the course on new datasets. 

Enrichment Analysis
    -This module introduces the important concept of performing gene set enrichment analyses. Enrichment analysis is the process of querying gene sets from genomics and proteomics studies against annotated gene sets collected from prior biological knowledge.

Machine Learning
    -This module describes the mathematical concepts of supervised machine learning, the process of making predictions from examples that associate observations/features/attribute with one or more properties that we wish to learn/predict.

Benchmarking
    -This module discusses how Bioinformatics pipelines can be compared and evaluated.

Interactive Data Visualization
    -This module provides programming examples on how to get started with creating interactive web-based data visualization elements/figures.

Crowdsourcing Projects
    -This final module describes opportunities to work on LINCS related projects that go beyond the course.

Final Exam
    -The Final Exam consists of 60 multiple choice questions which covers all of the modules of the course. Some of the questions may require you to perform some analysis with the methods you learned throughout the course on new datasets."
https://www.classcentral.com/course/big-data-integration-processing-6467,"At the end of the course, you will be able to:

*Retrieve data from example database and big data management systems 
*Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications
*Identify when a big data problem needs data integration
*Execute simple big data integration and processing on Hadoop and Spark platforms

This course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.

Hardware Requirements: 
(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. 

Software Requirements: 
This course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.
      


            Read more
          



          Welcome to Big Data Integration and Processing
    -Welcome to the third course in the Big Data Specialization. This week you will be introduced to basic concepts in big data integration and processing. You will be guided through installing the Cloudera VM, downloading the data sets to be used for this course, and learning how to run the Jupyter server. 

Retrieving Big Data (Part 1)
    -This module covers the various aspects of data retrieval and relational querying. You will also be introduced to the Postgres database. 

Retrieving Big Data (Part 2)
    -This module covers the various aspects of data retrieval for NoSQL data, as well as data aggregation and working with data frames. You will be introduced to MongoDB and Aerospike, and you will learn how to use Pandas to retrieve data from them.

Big Data Integration
    -In this module you will be introduced to data integration tools including Splunk and Datameer, and you will gain some practical insight into how information integration processes are carried out. 

Processing Big Data
    -This module introduces Learners to big data pipelines and workflows as well as processing and analysis of big data using Apache Spark. 

Big Data Analytics using Spark
    -In this module, you will go deeper into big data processing by learning the inner workings of the Spark Core. You will be introduced to two key tools in the Spark toolkit: Spark MLlib and GraphX. 

Learn By Doing: Putting MongoDB and Spark to Work
    -In this module you will get some practical hands-on experience applying what you learned about Spark and MongoDB to analyze Twitter data."
https://www.classcentral.com/course/programacion-estadistica-r-4657,"Este curso te proporcionará las bases del lenguaje de programación estadística R, la lengua franca de la estadística, el cual te permitirá escribir programas que lean, manipulen y analicen datos cuantitativos. Te explicaremos la instalación del lenguaje; también verás una introducción a los sistemas base de gráficos y al paquete para graficar ggplot2, para visualizar estos datos. Además también abordarás la utilización de uno de los IDEs más populares entre la comunidad de usuarios de R, llamado RStudio.

Objetivo

Al término del curso:

Utilizarás el lenguaje de programación R con el fin de manipular datos, generar análisis estadísticos y representación gráfica, a través del procesamiento de datos cuantitativos.

Forma de trabajo

Este curso busca introducirte en el lenguaje de programación estadística R, un lenguaje computacional diseñado para el análisis estadístico de datos. Este curso está dirigido a estudiantes y profesionales que tienen interés en poder utilizar esta herramienta, para leer, manipular, analizar y graficar datos. 

Utilizarás un IDE (Ambiente de Desarrollo Integrado) muy popular para trabajar con el lenguaje R, llamado RStudio, que se ha vuelto el IDE de facto para programar en R.

En cada módulo encontrarás videos que te guiarán en la instalación de las herramientas a utilizar, así como explicaciones de las operaciones básicas y los elementos específicos que ofrecen un manejo más profundo del lenguaje. También hallarás algunas referencias bibliográficas para ahondar en el tema que sea de tu interés.

Para complementar las lecciones, realizarás prácticas con el lenguaje, las cuales tendrán valor para la evaluación.
      


            Read more
          



          Instalación de Herramientas
    -Una pequeña introducción a lo que veremos en el curso y la instalación de las herramientas a usar en él, tanto para sistemas operativos Windows, Linux y Mac OS. 

Introducción al Lenguaje
    -En esta primera semana, tendrás tus primeros pasos en el lenguaje, entenderás que son los objetos, las clases, los tipos de datos y podrás leer y escribir datos a distintas fuentes y tipos de archivos.

Utilización del Lenguaje
    -En esta semana, aprenderás a utilizar las estructuras de control para agilizar tareas repetitivas, a crear funciones para llevar a cabo tareas específicas o complejas,  las reglas de alcance, el manejo de las fechas y variables de fechas, la importancia de utilizar operaciones vectorizadas para hacer tu código más rápido, por qué es importante escribir código con un buen estilo. 

Acercamiento al Sistema de Gráficos de R
    -En esta semana primero conocerás una familia de funciones vectorizadas, las cuales te permitirán efectuar operaciones con muy pocas líneas de código, después conocerás el sistema de gráficos y las funciones que permiten realizar modificaciones en él.

Expresiones Regulares, Graficación con ggplot2 y Simulación
    -Ensta última semana aprenderás sobre expresiones regulares y cómo utilizarlas con R, graficarás con el paquete de gráficos ggplot2, entenderás cómo encontrar errores y arreglarlos y llevarás a cabo simulación."
https://www.classcentral.com/course/big-data-mathematical-modelling-5422,"Learn how mathematics underpins big data analysis and develop your skills.
Mathematics is everywhere, and with the rise of big data it becomes a useful tool when extracting information and analysing large datasets. We begin by explaining how maths underpins many of the tools that are used to manage and analyse big data. We show how very different applied problems can have common mathematical aims, and therefore can be addressed using similar mathematical tools. We then introduce three such tools, based on a linear algebra framework: eigenvalues and eigenvectors for ranking; 	graph Laplacian for clustering; and 	singular value decomposition for data compression.
This course is designed for anyone looking to add mathematical methods for data analytics to their skill set.  We provide a multi-layered approach, so you can learn about the methods even if you don’t have a strong maths background, but we provide further information for those with a sound knowledge of undergraduate mathematics. We will assume basic MATLAB (or other) programming skills for some of the practical exercises.
MathWorks will provide you with free access to MATLAB Online for the duration of the course so you can complete the programming exercises. Please visit MATLAB Online to ensure your system meets the minimum requirements."
https://www.classcentral.com/course/big-data-visualisation-5423,"##
Data visualisation is an important visual method for effective communication and analysing large datasets. Through data visualisations we are able to draw conclusions from data that are sometimes not immediately obvious and interact with the data in an entirely different way.
This course will provide you with an informative introduction to the methods, tools and processes involved in visualising big data. We will also take the time to examine briefly the use of visualisation throughout history dating back as far as 17000 BC.
We have designed the course for people from different fields who want to learn how to produce visualisations that help us better understand real-world big data problems. You will gain the most from the practical exercises if you are comfortable with computer programming however you don’t need to have any prior experience using the software listed below.
We will use a variety of tools so that you become comfortable engaging with different software and confident trialing new packages to find those that best meet your needs. Please review the product websites below to ensure your system meets the minimum requirements for the tools we will be using.

Tableau: You can use the free trial for a period of 2 weeks. Please do not start the trial until you are ready to do the Tableau exercises.
MATLAB Online: MathWorks will provide you with a license to use MATLAB online for the duration of the course.
D3.js: The D3 JavaScript library is available under BSD license.

You can still learn effectively even if you don’t have access to all of these tools as you will be able to see what they can do for you.



            Read more"
https://www.classcentral.com/course/edx-programming-with-python-for-data-science-6471,"This course is part of the Microsoft Professional Program Certificate in Data Science.
This practical course, developed in partnership with Coding Dojo, targets individuals who have introductory level Python programming experience. The course teaches students how to start looking at data with the lens of a data scientist by applying efficient, well-known mining models in order to unearth useful intelligence, using Python, one of the popular languages for Data Scientists. Topics include data visualization, feature importance and selection, dimensionality reduction, clustering, classification and more! All of the data sets used in this course are gathered live-data or inspired by real-world domains that can benefit from machine learning."
https://www.classcentral.com/course/bioinformatics2-2290,"This course is the second in a two-part series that begins with Bioinformatics Algorithms (Part 1).  It will build upon the biological and computational material covered in the first course to cover additional topics in modern computational biology.The format for this course will be the same as that of Part 1. Each chapter of course material will cover a single biological question and slowly build the algorithmic knowledge required to address this challenge.  Along the way, coding challenges and exercises (many of which ask you to apply your skills to real genetic data) will be directly integrated into the text at the exact moment they are needed.



          The course will be based on six ""chapters"" covering the following central questions, with the algorithmic ideas that we will use to solve them in parentheses:How Do We Locate Disease-Causing Mutations? (Combinatorial Pattern Matching)Which Animal Gave Us SARS? (Evolutionary Trees)How Did Yeast Become Such a Good Wine Brewer? (Clustering Algorithms)Why Do We Still Not Have an HIV Vaccine? (Hldden Markov Models)Was T. rex Just a Big Chicken? (Computational Proteomics)The grading for the course will be based on several weekly programming challenges, as well as a comprehension quiz at the end of each chapter."
https://www.classcentral.com/course/repdata-1716,"This course focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner. Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.  The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. This course will focus on literate statistical analysis tools which allow one to publish data analyses in a single document that allows others to easily execute the same analysis to obtain the same results.
      


          Week 1: Concepts, Ideas, & Structure
    -This week will cover the basic ideas of reproducible research since they may be unfamiliar to some of you. We also cover structuring and organizing a data analysis to help make it more reproducible. I recommend that you watch the videos in the order that they are listed on the web page, but watching the videos out of order isn't going to ruin the story. 

Week 2: Markdown & knitr
    -This week we cover some of the core tools for developing reproducible documents. We cover the literate programming tool knitr and show how to integrate it with Markdown to publish reproducible web documents. We also introduce the first peer assessment which will require you to write up a reproducible data analysis using knitr. 

Week 3: Reproducible Research Checklist & Evidence-based Data Analysis
    -This week covers what one could call a basic check list for ensuring that a data analysis is reproducible. While it's not absolutely sufficient to follow the check list, it provides a necessary minimum standard that would be applicable to almost any area of analysis.

Week 4: Case Studies & Commentaries
    -This week there are two 
case studies involving the importance of reproducibility in science for you to watch."
https://www.classcentral.com/course/edx-analyzing-data-with-python-12531,"LEARN TO ANALYZE DATA WITH PYTHON
Learn how to analyze data using Python. This coursewill take you from the basics of Python to exploring many different types of data. You will learn how to prepare data for analysis, perform simple statistical analyses, create meaningful data visualizations, predict future trends from data, and more!



COURSE SYLLABUS
Module 1 - Importing Datasets

Learning Objectives
Understanding the Domain
Understanding the Dataset
Python package for data science
Importing and Exporting Data in Python
Basic Insights from Datasets

Module 2 - Cleaning and Preparing the Data

Identify and Handle Missing Values
Data Formatting
Data Normalization Sets
Binning
Indicator variables

Module 3 - Summarizing the Data Frame

Descriptive Statistics
Basic of Grouping
ANOVA
Correlation
More on Correlation

Module 4 - Model Development

Simple and Multiple Linear Regression
Model EvaluationUsingVisualization
Polynomial Regression and Pipelines
R-squared and MSE for In-Sample Evaluation
Prediction and Decision Making

Module 5 - Model Evaluation

Model Evaluation
Over-fitting, Under-fitting and Model Selection
Ridge Regression
Grid Search
Model Refinement"
https://www.classcentral.com/course/edx-high-dimensional-data-analysis-2949,"If you’re interested in data analysis and interpretation, then this is the data science course for you. We start by learning the mathematical definition of distance and use this to motivate the use of the singular value decomposition (SVD) for dimension reduction of high-dimensional data sets, and multi-dimensional scaling and its connection to principle component analysis. We will learn about the batch effect, the most challenging data analytical problem in genomics today, and describe how the techniques can be used to detect and adjust for batch effects. Specifically, we will describe the principal component analysis and factor analysis and demonstrate how these concepts are applied to data visualization and data analysis of high-throughput experimental data.
Finally, we give a brief introduction to machine learning and apply it to high-throughput, large-scale data. We describe the general idea behind clustering analysis and descript K-means and hierarchical clustering and demonstrate how these are used in genomics and describe prediction algorithms such as k-nearest neighbors along with the concepts of training sets, test sets, error rates and cross-validation.
Given the diversity in educational background of our students we have divided the series into seven parts. You can take the entire series or individual courses that interest you. If you are a statistician you should consider skipping the first two or three courses, similarly, if you are biologists you should consider skipping some of the introductory biology lectures. Note that the statistics and programming aspects of the class ramp up in difficulty relatively quickly across the first three courses. By the third course will be teaching advanced statistical concepts such as hierarchical models and by the fourth advanced software engineering skills, such as parallel computing and reproducible research concepts.
These courses make up 2 XSeries and are self-paced:
PH525.1x: Statistics and R for the Life Sciences
PH525.2x: Introduction to Linear Models and Matrix Algebra
PH525.3x: Statistical Inference and Modeling for High-throughput Experiments
PH525.4x: High-Dimensional Data Analysis
PH525.5x: Introduction to Bioconductor: annotation and analysis of genomes and genomic assays 
PH525.6x: High-performance computing for reproducible genomics
PH525.7x: Case studies in functional genomics
This class was supported in part by NIH grant R25GM114818.



            Read more"
https://www.classcentral.com/course/edx-introduction-to-python-for-data-science-5683,"Python is a very powerful programming language used for many different applications. Over time, the huge community around this open source language has created quite a few tools to efficiently work with Python. In recent years, a number of tools have been built specifically for data science. As a result, analyzing data with Python has never been easier.
In this practical course, you will start from the very beginning, with basic arithmetic and variables, and learn how to handle data structures, such as Python lists, Numpy arrays, and Pandas DataFrames. Along the way, you'll learn about Python functions and control flow. Plus, you'll look at the world of data visualizations with Python and create your own stunning visualizations based on real data. 
edX offers financial assistance for learners who want to earn Verified Certificates but who may not be able to pay the fee. To apply for financial assistance, enroll in the course, then follow this link to complete an application for assistance.



Section 1: Python Basics
Take your first steps in the world of Python. Discover the different data types and create your first variable. 
Section 2: Python Lists
Get the know the first way to store many different data points under a single name. Create, subset and manipulate Lists in all sorts of ways. 
Section 3: Functions and Packages
Learn how to get the most out of other people's efforts by importing Python packages and calling functions. 
Section 4: Numpy
Write superfast code with Numerical Python, a package to efficiently store and do calculations with huge amounts of data. 
Section 5: Matplotlib
Create different types of visualizations depending on the message you want to convey. Learn how to build complex and customized plots based on real data. 
Section 6: Control flow and Pandas
Write conditional constructs to tweak the execution of your scripts and get to know the Pandas DataFrame: the key data structure for Data Science in Python."
https://www.classcentral.com/course/social-media-data-analytics-7019,"Learner Outcomes: After taking this course, you will be able to:
- Utilize various Application Programming Interface (API) services to collect data from different social media sources such as YouTube, Twitter, and Flickr.
- Process the collected data - primarily structured - using methods involving correlation, regression, and classification to derive insights about the sources and people who generated that data.
- Analyze unstructured data - primarily textual comments - for sentiments expressed in them.
- Use different tools for collecting, analyzing, and exploring social media data for research and development purposes.

Sample Learner Story: Data analyst wanting to leverage social media data.
Isabella is a Data Analyst working as a consultant for a multinational corporation. She has experience working with Web analysis tools as well as marketing data. She wants to now expand into social media arena, trying to leverage the vast amounts of data available through various social media channels. Specifically, she wants to see how their clients, partners, and competitors view their products/services and talk about them. She hopes to build a new workflow of data analytics that incorporates traditional data processing using Web and marketing tools, as well as newer methods of using social media data.

Sample Job Roles requiring these skills: 
- Social Media Analyst
- Web Analyst
- Data Analyst
- Marketing and Public Relations 

Final Project Deliverable/ Artifact: The course will have a series of small assignments or mini-projects that involve data collection, analysis, and presentation involving various social media sources using the techniques learned in the class.
      


            Read more
          



          Introduction to Data Analytics
    -In this first unit of the course, several concepts related to social media data and data analytics are introduced. We start by first discussing two kinds of data - structured and unstructured. Then look at how structured data, the primary focus of this course, is analyzed and what one could gain by doing such analysis. Finally, we briefly cover some of the visualizations for exploring and presenting data.Make sure to go through the material for this unit in the sequence it's provided. First, watch the four short videos, then take the practice test, followed by the two quizzes. Finally, read the documents about installation and configuration of Python and R. This is very important - before proceeding to the next units, make sure you have installed necessary tools, and also learned how to install new packages/libraries for them. The course expects students to have programming experience in Python and R.

Collecting and Extracting Social Media Data
    -In this unit we will see how to collect data from Twitter and YouTube. The unit will start with an introduction to Python programming. Then we will use a Python script, with a little editing, to extract data from Twitter. A similar exercise will then be done with YouTube. In both the cases, we will also see how to create developer accounts and what information to obtain to use the data collection APIs.

Once again, make sure to go item-by-item in the order provided. Before beginning this unit, ensure that you have all the right tools (Python, R, Anaconda) ready and configured. The lessons depend on them and also your ability to install required packages.

Data Analysis, Visualization, and Exploration
    -In this unit, we will focus on analyzing and visualizing the data from various social media services. We will first use the data collected before from YouTube to do various statistics analyses such as correlation and regression. We will then introduce R - a platform for doing statistical analysis. Using R, then we will analyze a much larger dataset obtained from Yelp.

Make sure you have covered the material in the previous units before proceeding with this. That means, having all the tools (Anaconda, Python, and R) as well as various packages installed. We will also need new packages this time, so make sure you know how to install them to your Python or R. If needed, please review some basic concepts in statistics - specifically, correlation and regression - before or during working on this unit.

Case Studies
    -In the final unit of this course, we will work on two case studies - both using Twitter and focusing on unstructured data (in this case, text). The first case study will involve doing sentiment analysis with Python. The second case study will take us through basic text mining application using R. We wrap up the unit with a conclusion of what we did in this course and where to go next for further learning and exploration."
https://www.classcentral.com/course/big-data-analysys-6288,"Manipulating big data distributed over a cluster using functional concepts is rampant in industry, and is arguably one of the first widespread industrial uses of functional ideas. This is evidenced by the popularity of MapReduce and Hadoop, and most recently Apache Spark, a fast, in-memory distributed collections framework written in Scala. In this course, we'll see how the data parallel paradigm can be extended to the distributed case, using Spark throughout. We'll cover Spark's programming model in detail, being careful to understand how and when it differs from familiar programming models, like shared-memory parallel collections or sequential Scala collections. Through hands-on examples in Spark and Scala, we'll learn when important issues related to distribution like latency and network communication should be considered and how they can be addressed effectively for improved performance.

Learning Outcomes. By the end of this course you will be able to:

- read data from persistent storage and load it into Apache Spark,
- manipulate data with Spark and Scala,
- express algorithms for data analysis in a functional style, 
- recognize how to avoid shuffles and recomputation in Spark,

Recommended background: You should have at least one year programming experience. Proficiency with Java or C# is ideal, but experience with other languages such as C/C++, Python, Javascript or Ruby is also sufficient. You should have some familiarity using the command line. This course is intended to be taken after Parallel Programming: https://www.coursera.org/learn/parprog1.
      


            Read more
          



          Getting Started + Spark Basics
    -Get up and running with Scala on your computer. Complete an example assignment to familiarize yourself with our unique way of submitting assignments. In this week, we'll bridge the gap between data parallelism in the shared memory scenario (learned in the Parallel Programming course, prerequisite) and the distributed scenario. We'll look at important concerns that arise in distributed systems, like latency and failure. We'll go on to cover the basics of Spark, a functionally-oriented framework for big data processing in Scala. We'll end the first week by exercising what we learned about Spark by immediately getting our hands dirty analyzing a real-world data set.

Reduction Operations & Distributed Key-Value Pairs
    -This week, we'll look at a special kind of RDD called pair RDDs. With this specialized kind of RDD in hand, we'll cover essential operations on large data sets, such as reductions and joins.

Partitioning and Shuffling
    -This week we'll look at some of the performance implications of using operations like joins. Is it possible to get the same result without having to pay for the overhead of moving data over the network? We'll answer this question by delving into how we can partition our data to achieve better data locality, in turn optimizing some of our Spark jobs.

Structured data: SQL, Dataframes, and Datasets
    -With our newfound understanding of the cost of data movement in a Spark job, and some experience optimizing jobs for data locality last week, this week we'll focus on how we can more easily achieve similar optimizations. Can structured data help us? We'll look at Spark SQL and its powerful optimizer which uses structure to apply impressive optimizations. We'll move on to cover DataFrames and Datasets, which give us a way to mix RDDs with the powerful automatic optimizations behind Spark SQL."
https://www.classcentral.com/course/python-for-applied-data-science-11194,"This introduction to Python will kickstart your learning of Python for data science, as well as programming in general. This beginner-friendly Python course will take you from zero to programming in Python in a matter of hours.

Module 1 - Python Basics
o	Your first program
o	Types
o	Expressions and Variables
o	String Operations

Module 2 - Python Data Structures
o	Lists and Tuples
o	Sets
o	Dictionaries

Module 3 - Python Programming Fundamentals
o	Conditions and Branching
o	Loops
o	Functions
o	Objects and Classes

Module 4 - Working with Data in Python
o	Reading files with open
o	Writing files with open
o	Loading data with Pandas
o	Numpy 

Finally, you will create a project to test your skills.

LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.
      


          Python Basics 

Python Data Structures 

Python Programming Fundamentals 

Working with Data in Python 

Analyzing US Economic Data and Building a Dashboard"
https://www.classcentral.com/course/python-for-applied-data-science-ai-14403,"This introduction to Python will kickstart your learning of Python for data science, as well as programming in general. This beginner-friendly Python course will take you from zero to programming in Python in a matter of hours.

Module 1 - Python Basics
o	Your first program
o	Types
o	Expressions and Variables
o	String Operations

Module 2 - Python Data Structures
o	Lists and Tuples
o	Sets
o	Dictionaries

Module 3 - Python Programming Fundamentals
o	Conditions and Branching
o	Loops
o	Functions
o	Objects and Classes

Module 4 - Working with Data in Python
o	Reading files with open
o	Writing files with open
o	Loading data with Pandas
o	Numpy 

Finally, you will create a project to test your skills.
      


          Python Basics 

Python Data Structures 

Python Programming Fundamentals 

Working with Data in Python 

Analyzing US Economic Data and Building a Dashboard"
https://www.classcentral.com/course/bioinformatics-745,"Are you interested in learning how to program (in Python) within a scientific setting?

This course will cover algorithms for solving various biological problems along with a handful of programming challenges helping you implement these algorithms in Python.  It offers a gently-paced introduction to our Bioinformatics Specialization (https://www.coursera.org/specializations/bioinformatics), preparing learners to take the first course in the Specialization, ""Finding Hidden Messages in DNA"" (https://www.coursera.org/learn/dna-analysis).

Each of the four weeks in the course will consist of two required components.  First, an interactive textbook provides Python programming challenges that arise from real biological problems.  If you haven't programmed in Python before, not to worry! We provide ""Just-in-Time"" exercises from the Codecademy Python track (https://www.codecademy.com/learn/python). And each page in our interactive textbook has its own discussion forum, where you can interact with other learners. Second, each week will culminate in a summary quiz.

Lecture videos are also provided that accompany the material, but these videos are optional.
      


          Week 1
    -Where in the Genome Does Replication Begin? (Part 1)

Week 2
    -Where in the Genome Does Replication Begin? (Part 2)

Week 3
    -Which DNA Patterns Play the Role of Molecular Clocks? (Part 1)

Week 4
    -Which DNA Patterns Play the Role of Molecular Clocks? (Part 2)"
https://www.classcentral.com/course/datacamp-introduction-to-r-7630,"In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!



          Chapter One: learn how to use the console as a calculator and how to assign variables
Chapter Two: Learn how to create vectors in R, name them, select elements from them and compare different vectors.
Chapter Three: Learn how to work with matrices in R
Chapter Four: Learn how to create, subset and compare matrices.
Chapter Five: See how to create a data frame, select interesting parts of a data frame and order a data frame according to certain variables
Chapter Six: This final chapter will teach you how to create, name and subset these lists"
https://www.classcentral.com/course/futurelearn-learn-to-code-for-data-analysis-3997,"Learn to code in Python and analyse real, open data
This hands-on course will teach you how to write your own computer programs, one line of code at a time. You’ll learn how to access open data, clean it and analyse it, and produce visualisations. You will also learn how to write up and share your analyses, privately or publicly.
You will install free software to learn to code in Python, a widely used programming language. You will write up analyses and do coding exercises using the popular Jupyter Notebook platform. And you will look at real data from the World Health Organisation, the World Bank and other organisations.
The course does not assume prior experience in programming or data analysis. Basic familiarity with a spreadsheet application will be an advantage.
The course does not require any knowledge of statistics, but you need to have basic numeracy skills, like writing arithmetic expressions, using percentages and understanding scientific notation. If you wish to brush up on your numeracy skills, we recommend the FutureLearn course Basic Science: Understanding Numbers from The Open University.
To study this course you will use specialist software. You can use the software online, via a free account on a website, or offline, by downloading and installing a free software package. You will receive instructions about both options via email before the course starts. The online solution requires a good internet connection and has some limitations.
The offline software has no limitations and is the recommended option. However, you will need access to a desktop or laptop computer on which you can install software. The software is free and there are versions available for Windows, Mac and Linux platforms. You will need about 3 GB of free disk space to download and install the software, and to store datasets that will be provided in the course.
Whether you choose the online or offline software option, you will need to be proficient in basic computer tasks, like creating folders, downloading files and copying them to specific folders, etc. In terms of accessibility, you will be asked to use your web browser and to type code.



            Read more"
https://www.classcentral.com/course/edx-fa19-deterministic-optimization-9947,"This course blends optimization theory and computation and its teachings can be applied to modern data analytics, economics, and engineering. Organized across four modules, it takes learners through basic concepts, models, and algorithms in linear optimization, convex optimization, and integer optimization. The first module of the course is a general overview of key concepts in linear algebra, calculus, and optimization. The second module of the course is on linear optimization, covering modeling techniques with many applications, basic polyhedral theory, simplex method, and duality theory. The third module is on convex conic optimization, which is a significant generalization of linear optimization. The fourth and final module focuses on integer optimization, which augments the previously covered optimization models with the flexibility of integer decision variables.



Week 1

Module 1: Introduction
Module 2: Illustration of the Optimization Problems

Week 2

Module 3: Review of Mathematical Concepts
Module 4: Convexity

Week 3

Module 5: Outcomes of Optimization
Module 6: Optimality Certificates

Week 4

Module 7: Unconstrained Optimization: Derivate Based
Module 8: Unconstrained Optimization: Derivative Free

Week 5

Module 9: Linear Optimization Modeling – Network Flow Problems
Module 10: Linear Optimization Modeling – Electricity Markets

Week 6

Module 11: Linear Optimization Modeling – Decision-Making Under Uncertainty
Module 12: Linear Optimization Modeling – Handling Nonlinearity 

Week 7

Module 13: Geometric Aspects of Linear Optimization
Module 14: Algebraic Aspect of Linear Optimization

MidtermWeek 8

Module 15: Simplex Method in a Nutshell
Module 16: Further Development of Simplex Method

Week 9

Module 17: Linear Programming Duality
Module 18: Robust Optimization

Week 10

Module 19: Nonlinear Optimization Modeling – Approximation and Fitting
Module 20: Nonlinear Optimization Modeling – Statistical Estimation

Week 11

Module 21: Convex Conic Programming – Introduction
Module 22: Second-Order Conic Programming – Examples

Week 12

Module 23: Second-Order Conic Programming – Advanced Modeling
Module 24: Semi-definite Programming – Advanced Modeling

Week 13

Module 25: Discrete Optimization: Introduction
Module 26: Discrete Optimization: Modeling with binary variables - 1

Week 14

Module 27: Discrete Optimization: Modeling with binary variables – 2
Module 28: Discrete Optimization: Modeling exercises

Week 15

Module 29: Discrete Optimization: Linear programming relaxation
Module 30: Discrete Optimization: Solution methods"
https://www.classcentral.com/course/edx-explore-statistics-with-r-1836,"Do you want to learn how to harvest health science data from the Internet? Or learn to understand the world through data analysis? Start by learning R Statistics!
Skilled professionals who can process and analyze data are in great demand today. In this course you will explore concepts in statistics to make sense out of data. You will learn the practical skills necessary to find, import, analyze and visualize data. We will take a look under the hood of statistics and equip you with broad tools for understanding statistical inference and statistical methods. You will also perform some really complicated calculations and visualizations, following in the footsteps of Karolinska Institute’s researchers.
Statistical programming is an essential skill in our golden age of data abundance. Health science has become a field of big data, just like so many other fields of study. New techniques make it possible and affordable to generate massive data sets in biology. Researchers and clinicians can measure the activity for each of 30000 genes of a patient. They can read the complete genome sequence of a patient. Thanks to another trend of the decade, open access publishing, the results of such large scale health science are very often published for you to read free of charge. You can even access the raw data from open databases such as the gene expression database of the NCBI, National Center for Biotechnology Information.
We will dive into this data together. Learn how to use R, a powerful open source statistical programming language, and see why it has become the tool of choice in many industries in this introductory R statistics course. 



            Read more"
https://www.classcentral.com/course/algobioprogramming-2291,"The sequencing of the human genome at the start of this century fueled a computational revolution in biology. As a result, modern biology produces as many new algorithms as any other fundamental realm of science.Once we have sequenced a genome, it may look like an incomprehensible string of the nucleotides A, C, G, and T. Yet hidden in these four letters is a secret language. In this course, we will start understanding this language by using computer programming. What makes this course distinct is that we assume that you have never programmed before.While learning Python from the ground up, we will write algorithms to determine where a bacterium starts replicating its genome, a problem with applications in genetic engineering.  We will also use programming to learn how a cell knows what time of day it is and how the bacterium causing tuberculosis can hide from antibiotics.This course offers a much gentler-paced alternative to Finding Hidden Messages in DNA, the first course in the Bioinformatics Specialization.  After completing it, we hope you will be well prepared to jump into the full Specialization!



          The course will be based around the following biological questions, with the algorithmic ideas that we will use to solve them in parentheses:Where Does DNA Replication Begin? (Algorithmic Warm-up)Which DNA Patterns Act As Molecular Clocks? (Greedy and Randomized Algorithms)Some of these topics will require you to complete coding challenges in order to implement bioinformatics algorithms, followed by a comprehension quiz at the end of the topic.  Other topics will carry lighter workloads and consist primarily of lecture videos and short quizzes."
https://www.classcentral.com/course/r-packages-7175,"Writing good code for data science is only part of the job. In order to maximizing the usefulness and reusability of data science software, code must be organized and distributed in a manner that adheres to community-based standards and provides a good user experience. This course covers the primary means by which R software is organized and distributed to others. We cover R package development, writing good documentation and vignettes, writing robust software, cross-platform development, continuous integration tools, and distributing packages via CRAN and GitHub. Learners will produce R packages that satisfy the criteria for submission to CRAN.
      


          Getting Started with R Packages

Documentation and Testing

Licensing, Version Control, and Software Design

Continuous Integration and Cross Platform Development"
https://www.classcentral.com/course/cloud-applications-2-6309,"Welcome to the Cloud Computing Applications course, the second part of a two-course series designed to give you a comprehensive view on the world of Cloud Computing and Big Data!

In this second course we continue Cloud Computing Applications by exploring how the Cloud opens up data analytics of huge volumes of data that are static or streamed at high velocity and represent an enormous variety of information. Cloud applications and data analytics represent a disruptive change in the ways that society is informed by, and uses information. We start the first week by introducing some major systems for data analysis including Spark and the major frameworks and distributions of analytics applications including Hortonworks, Cloudera, and MapR. By the middle of week one we introduce the HDFS distributed and robust file system that is used in many applications like Hadoop and finish week one by exploring the powerful MapReduce programming model and how distributed operating systems like YARN and Mesos support a flexible and scalable environment for Big Data analytics. In week two, our course introduces large scale data storage and the difficulties and problems of consensus in enormous stores that use quantities of processors, memories and disks. We discuss eventual consistency, ACID, and BASE and the consensus algorithms used in data centers including Paxos and Zookeeper. Our course presents Distributed Key-Value Stores and in memory databases like Redis used in data centers for performance. Next we present NOSQL Databases. We visit HBase, the scalable, low latency database that supports database operations in applications that use Hadoop. Then again we show how Spark SQL can program SQL queries on huge data. We finish up week two with a presentation on Distributed Publish/Subscribe systems using Kafka, a distributed log messaging system that is finding wide use in connecting Big Data and streaming applications together to form complex systems. Week three moves to fast data real-time streaming and introduces Storm technology that is used widely in industries such as Yahoo. We continue with Spark Streaming, Lambda and Kappa architectures, and a presentation of the Streaming Ecosystem. Week four focuses on Graph Processing, Machine Learning, and Deep Learning. We introduce the ideas of graph processing and present Pregel, Giraph, and Spark GraphX. Then we move to machine learning with examples from Mahout and Spark. Kmeans, Naive Bayes, and fpm are given as examples. Spark ML and Mllib continue the theme of programmability and application construction. The last topic we cover in week four introduces Deep Learning technologies including Theano, Tensor Flow, CNTK, MXnet, and Caffe on Spark.
      


            Read more
          



          Course Orientation
    -You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.

Module 1: Spark, Hortonworks, HDFS, CAP
    -In Module 1, we introduce you to the world of Big Data applications. We start by introducing you to Apache Spark, a common framework used for many different tasks throughout the course. We then introduce some Big Data distro packages, the HDFS file system, and finally the idea of batch-based Big Data processing using the MapReduce programming paradigm. 

Module 2: Large Scale Data Storage
    -In this module, you will learn about large scale data storage technologies and frameworks. We start by exploring the challenges of storing large data in distributed systems. We then discuss in-memory key/value storage systems, NoSQL distributed databases, and distributed publish/subscribe queues. 

Module 3: Streaming Systems
    -This module introduces you to real-time streaming systems, also known as Fast Data. We talk about Apache Storm in length, Apache Spark Streaming, and Lambda and Kappa architectures. Finally, we contrast all these technologies as a streaming ecosystem. 

Module 4: Graph Processing and Machine Learning
    -In this module, we discuss the applications of Big Data. In particular, we focus on two topics: graph processing, where massive graphs (such as the web graph) are processed for information, and machine learning, where massive amounts of data are used to train models such as clustering algorithms and frequent pattern mining. We also introduce you to deep learning, where large data sets are used to train neural networks with effective results."
https://www.classcentral.com/course/edx-introduction-to-apache-spark-5855,"Spark is rapidly becoming the compute engine of choice for big data. Spark programs are more concise and often run 10-100 times faster than Hadoop MapReduce jobs. As companies realize this, Spark developers are becoming increasingly valued.
This statistics and data analysis course will teach you the basics of working with Spark and will provide you with the necessary foundation for diving deeper into Spark. You’ll learn about Spark’s architecture and programming model, including commonly used APIs. After completing this course, you’ll be able to write and debug basic Spark applications. This course will also explain how to use Spark’s web user interface (UI), how to recognize common coding errors, and how to proactively prevent errors. The focus of this course will be Spark Core and Spark SQL.
This course covers advanced undergraduate-level material. It requires a programming background and experience with Python (or the ability to learn it quickly). All exercises will use PySpark (the Python API for Spark), but previous experience with Spark or distributed computing is NOT required. Students should take this Python mini-quiz before the course and take this Python mini-course if they need to learn Python or refresh their Python knowledge."
https://www.classcentral.com/course/sql-data-science-11067,"Much of the world's data resides in databases. SQL (or Structured Query Language) is a powerful language which is used for communicating with and extracting data from databases. A working knowledge of databases and SQL is a must if you want to become a data scientist.

The purpose of this course is to introduce relational database concepts and help you learn and apply foundational knowledge of the SQL language. It is also intended to get you started with performing SQL access in a data science environment.  

The emphasis in this course is on hands-on and practical learning . As such, you will work with real databases, real data science tools, and real-world datasets. You will create a database instance in the cloud. Through a series of hands-on labs you will practice building and running SQL queries. You will also learn how to access databases from Jupyter notebooks using SQL and Python.

No prior knowledge of databases, SQL, Python, or programming is required.

Anyone can audit this course at no-charge. If you choose to take this course and earn the Coursera course certificate, you can also earn an IBM digital badge upon successful completion of the course.

LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.
      


          Week 1 - Introduction to Databases and Basic SQL
    -In Week 1 you will be introduced to databases. You will create a database instance on the cloud.  You will learn some of the basic SQL statements. You will also write and practice basic SQL hands-on on a live database.

Week 2 - Advanced SQL
    -By the end of this module, you will learn the following: (1) Learn how to use string patterns and ranges to search data and how to sort and group data in result sets.  (2) Learn how to work with multiple tables in a relational database using  join operations.

Week 3 - Accessing Databases using Python
    -After completing the lessons in this week, you will learn how to explain the basic concepts related to using Python to connect to databases and then create tables, load data, query data using SQL, and analyze data using Python 

Week 4: Course Assignment
    -As a hands-on Data Science assignment, you will be working with multiple real world datasets for the city of Chicago. You will be asked questions that will help you understand the data just like a data scientist would. You will be assessed both on the correctness of your SQL queries and results."
https://www.classcentral.com/course/coursera-big-data-cloud-computing-cdn-emerging-technologies-3931,"This is a notice to inform you that the “Big Data, Cloud Computing, & CDN Emerging Technologies” course will close for new learner enrollment on September 17, 2018. Since you have already enrolled, you will continue to see it on your Coursera Dashboard as long as you remain enrolled in the course. If you are interested in earning a Course Certificate for this course, please upgrade or apply for Financial Aid by September 16, 2018, if you have not already done so. In order to earn a Course Certificate, you will need to complete all graded assignments, including peer reviews, by March 17, 2019. After that point, no new assignment submissions will be accepted for Certificate credit. The reason to close this course is because a new upgraded ""Emerging Technologies: From Smartphones to IoT to Big Data"" Specialization has been prepared and will be launched in a few weeks. This new upgraded Specialization includes 4 courses titled “Big Data Emerging Technologies,” “Smart Device & Mobile Emerging Technologies,” ""IoT (Internet of Things) Wireless & Cloud Computing Emerging Technologies,"" and ""AR (Augmented Reality) & Video Streaming Services Emerging Technologies."" These four courses (which all include projects) include the contents of the former courses, but with much more new state-of-the-art technologies added. Among these courses, I highly recommended you to take the “Big Data Emerging Technologies” course (because it covers details on Apache Hadoop, Spark, Storm, ML (Machine Learning) data analysis technology and IBM SPSS Statistics projects), the ""IoT (Internet of Things) Wireless & Cloud Computing Emerging Technologies"" course (because it covers details on Cloud Computing, MEC, Fog computing, Cloudlets, and AWS (Amazon Web Service) EC2 cloud projects), and the ""AR (Augmented Reality) & Video Streaming Services Emerging Technologies"" course (because it covers details on H.264/MPEG-4 AVC, MPEG-DASH, CDN video streaming technology). While we hope that you will be able to complete the course, you can find more information about requesting a refund (https://learner.coursera.help/hc/en-us/articles/209819043-Request-a-refund) or unenrolling from a course (https://learner.coursera.help/hc/en-us/articles/208279756-Unenroll-from-a-course) in our Learner Help Center. We sincerely thank you for your interest and contributions to this course. In addition, we sincerely hope you will be interested in the new courses as well.
      


            Read more
          



Cloud ComputingThese lectures focus on the major features and functionalities of Cloud Computing. The lectures start with first answering the question “What does Cloud Computing do?” and then provide Cloud Computing Application Examples, and then provide description on the Cloud Models IaaS (Infrastructure as a Service), PaaS (Platform as a Service), SaaS (Software as a Service), as well as VM (Virtual Machine). Next, the lecture covers Cloud Services based on examples of the Google Cloud, Amazon Cloud, and the iCloud.Big DataThese lectures focus on the features and architecture of Big Data. The lectures start with providing Big Data examples, which include the past event of the H1N1 flu virus spread prevention, smartphone marketing, and industrial examples of Big Data in use by Wal-Mart, Amazon.com, and Citibank. To explain the major technical challenges of Big Data, the influence of data Volume, Variety, Velocity, and Veracity are discussed. Then, to provide a realistic overview of a reliable shared storage Big Data analysis system, the architecture of Hadoop is described. The description includes details on MapReduce and HDFS (Hadoop Distributed FileSystem), which are the two major components of Hadoop.CDN (Content Delivery Network)Everybody using smartphones, PCs, and the Internet have already been using CDN (Content Delivery Network) services, and will continue to use CDN services for their entire life. Obviously, CDN technology is very important. These lectures focus on the market, services, features, operations, and architecture of CDN technology. The lectures start with CDN Motivation and Structure, then the lecture provides details on CDN Procedures and Hierarchical Content Delivery Models. Next, the CDN Market and Major Service Providers are introduced followed by CDN Research & Development topics."
https://www.classcentral.com/course/gcp-big-data-ml-fundamentals-8234,"This 2-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities.

At the end of this course, participants will be able to:
• Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform
• Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform
• Employ BigQuery and Cloud Datalab to carry out interactive data analysis
• Choose between Cloud SQL, BigTable and Datastore
• Train and use a neural network using TensorFlow
• Choose between different data processing products on the Google Cloud Platform

Before enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following:
• A common query language such as SQL
• Extract, transform, load activities
• Data modeling
• Machine learning and/or statistics
• Programming in Python

Google Account Notes:
• Google services are currently unavailable in China.
      


          Introduction to the Data and Machine Learning on Google Cloud Platform Specialization .
    -Welcome to the Big Data and Machine Learning fundamentals on GCP course. Here you will learn the basics of how the course is structured and the four main big data challenges you will solve for.

Recommending Products using Cloud SQL and Spark
    -In this module you will have an existing Apache SparkML recommendation model that is running on-premise. You will learn about recommendation models and how you can run them in the cloud with Cloud Dataproc and Cloud SQL.

Predict Visitor Purchases with BigQuery ML
    -In this module, you will learn the foundations of BigQuery and big data analysis at scale. You will then learn how to build your own custom machine learning model to predict visitor purchases using just SQL with BigQuery ML. 

Create Streaming Data Pipelines with Cloud Pub/sub and Cloud Dataflow
    -In this module you will engineer and build an auto-scaling streaming data pipeline to ingest, process, and visualize data on a dashboard. Before you build your pipeline you'll learn the foundations of message-oriented architecture and pitfalls to avoid when designing and implementing modern data pipelines.

Classify Images with Pre-Built Models using Vision API and Cloud AutoML
    -Don't want to create a custom ML model from scratch? Learn how to leverage and extend pre-built ML models like the Vision API and Cloud AutoML for image classification.

Summary
    -In this final module, we will review the key challenges, solutions, and topics covered as part of this fundamentals course. We will also review additional resources and the steps you can take to get certified as a Google Cloud Data Engineer."
https://www.classcentral.com/course/edx-statistics-and-r-2960,"This course teaches the R programming language in the context of statistical data and statistical analysis in the life sciences.
We will learn the basics of statistical inference in order to understand and compute p-values and confidence intervals, all while analyzing data with R code. We provide R programming examples in a way that will help make the connection between concepts and implementation. Problem sets requiring R programming will be used to test understanding and ability to implement basic data analyses. We will use visualization techniques to explore new data sets and determine the most appropriate approach. We will describe robust statistical techniques as alternatives when data do not fit assumptions required by the standard approaches. By using R scripts to analyze data, you will learn the basics of conducting reproducible research.
Given the diversity in educational background of our students we have divided the course materials into seven parts. You can take the entire series or individual courses that interest you. If you are a statistician you should consider skipping the first two or three courses, similarly, if you are biologists you should consider skipping some of the introductory biology lectures. Note that the statistics and programming aspects of the class ramp up in difficulty relatively quickly across the first three courses. We start with simple calculations and descriptive statistics. By the third course will be teaching advanced statistical concepts such as hierarchical models and by the fourth advanced software engineering skills, such as parallel computing and reproducible research concepts.
These courses make up 2 XSeries and are self-paced:
PH525.1x: Statistics and R for the Life Sciences
PH525.2x: Introduction to Linear Models and Matrix Algebra
PH525.3x: Statistical Inference and Modeling for High-throughput Experiments
PH525.4x: High-Dimensional Data Analysis
PH525.5x: Introduction to Bioconductor: annotation and analysis of genomes and genomic assays 
PH525.6x: High-performance computing for reproducible genomics
PH525.7x: Case studies in functional genomics
This class was supported in part by NIH grant R25GM114818.



            Read more"
https://www.classcentral.com/course/edx-python-basics-for-data-science-12115,"Kickstart your learning of Python for data science, as well as programming in general with this introduction to Python course. This beginner-friendly Python course will quickly take you from zero to programming in Python in a matter of hours. 
Upon its completion, you'll be able to write your own Python scripts and perform basic hands-on data analysis using our Jupyter-based lab environment. If you want to learn Python from scratch, this course is for you. 
You can start creating your own data science projects and collaborating with other data scientists using IBM Watson Studio. When you sign up, you will receive free access to Watson Studio. Start now and take advantage of this platform and learn the basics of programming, machine learning and data visualization with this introductory course.



Module 1 - Python Basics
Your first program
Types
Expressions and Variables
String Operations 
Module 2 - Python Data Structures
Lists and Tuples
Sets
Dictionaries 
Module 3 - Python Programming Fundamentals
Conditions and Branching
Loops
Functions
Objects and Classes 
Module 4 - Working with Data in Python
Reading files with open
Writing files with open
Loading data with Pandas
Working with and Saving data with Pandas 
Module 5 - Working with Numpy Arrays
Numpy 1d Arrays
Numpy 2d Arrays"
https://www.classcentral.com/course/sql-for-data-science-9725,"As data collection has increased exponentially, so has the need for people skilled at using and interacting with data; to be able to think critically, and provide insights to make better decisions and optimize their businesses. This is a data scientist, “part mathematician, part computer scientist, and part trend spotter” (SAS Institute, Inc.). According to Glassdoor, being a data scientist is the best job in America; with a median base salary of $110,000 and thousands of job openings at a time. The skills necessary to be a good data scientist include being able to retrieve and work with data, and to do that you need to be well versed in SQL, the standard language for communicating with database systems.

This course is designed to give you a primer in the fundamentals of SQL and working with data so that you can begin analyzing it for data science purposes. You will begin to ask the right questions and come up with good answers to deliver valuable insights for your organization. This course starts with the basics and assumes you do not have any knowledge or skills in SQL. It will build on that foundation and gradually have you write both simple and complex queries to help you select data from tables.  You'll start to work with different types of data like strings and numbers and discuss methods to filter and pare down your results. 

You will create new tables and be able to move data into them. You will learn common operators and how to combine the data. You will use case statements and concepts like data governance and profiling. You will discuss topics on data, and practice using real-world programming assignments. You will interpret the structure, meaning, and relationships in source data and use SQL as a professional to shape your data for targeted analysis purposes. 

Although we do not have any specific prerequisites or software requirements to take this course, a simple text editor is recommended for the final project. So what are you waiting for? This is your first step in landing a job in the best occupation in the US and soon the world!
      


            Read more
          



          Getting Started and Selecting & Retrieving Data with SQL
    -In this module, you will be able to define SQL and discuss how SQL differs from other computer languages. You will be able to compare and contrast the roles of a database administrator and a data scientist, and explain the differences between one-to-one, one-to-many, and many-to-many relationships with databases. You will be able to use the SELECT statement and talk about some basic syntax rules. You will be able to add comments in your code and synthesize its importance.

Filtering, Sorting, and Calculating Data with SQL
    -In this module, you will be able to use several more new clauses and operators including WHERE, BETWEEN, IN, OR, NOT, LIKE, ORDER BY, and GROUP BY. You will be able to use the wildcard function to search for more specific or parts of records, including their advantages and disadvantages, and how best to use them. You will be able to discuss how to use basic math operators, as well as aggregate functions like AVERAGE, COUNT, MAX, MIN, and others to begin analyzing our data.

Subqueries and Joins in SQL
    -In this module, you will be able to discuss subqueries, including their advantages and disadvantages, and when to use them. You will be able to recall the concept of a key field and discuss how these help us link data together with JOINs. You will be able to identify and define several types of JOINs, including the Cartesian join, an inner join, left and right joins, full outer joins, and a self join. You will be able to use aliases and pre-qualifiers to make your SQL code cleaner and efficient.

Modifying and Analyzing Data with SQL
    -In this module, you will be able to discuss how to modify strings by concatenating, trimming, changing the case, and using the substring function. You will be able to discuss the date and time strings specifically. You will be able to use case statements and finish this module by discussing data governance and profiling. You will also be able to apply fundamental principles when using SQL for data science. You'll be able to use tips and tricks to apply SQL in a data science context."
https://www.classcentral.com/course/miriadax-analisis-estadistico-de-datos-con-r-1440,"Descripción Gracias por tu interés. Aunque este MOOC ya se realizó, si te inscribes podrás acceder a los contenidos más importantes y a los vídeos. Sin embargo, no podrás realizar ninguna de las actividades ni te podrás certificar. Cuando esté disponible una nueva edición podrás inscribirte para que obtengas la experiencia completa de un MOOC de Miríadax. R es un entorno informático de computación estadística y de generación de gráficos. R funciona en un amplio rango de sistemas operativos como UNIX, Windows o MacOS. Pese a su potencialidad, versatilidad y flexibilidad; R puede parecer árido en el momento en que el usuario trata de interaccionar con sus componentes. Se suele decir que “la curva de aprendizaje es lenta”. Sin embargo, los resultados que produce son ampliamente satisfactorios. Este curso está destinado a “lubricar” esos primeros encuentros con éste entorno estadístico. 
      


          Módulo 0. Presentación Módulo 1. Uso de programas estadísticos en la investigación científica Módulo 2. Entorno gráfico de trabajo de R. Interacción con R Módulo 3. Objetos o estructuras de datos que se manejan en el entorno R Módulo 4. Generación y personalización de gráficos estadísticos Módulo 5. Análisis estadísticos descriptivos. Elementos clave de la estadística inferencial Módulo 6. Consejos sobre la utilización de paquetes y utilización de la documentación que se les adjunta"
https://www.classcentral.com/course/swayam-data-science-for-engineers-10096,"Learning Objectives :Introduce R as a programming languageIntroduce the mathematical foundations required for data scienceIntroduce the first level data science algorithmsIntroduce a data analytics problem solving frameworkIntroduce a practical capstone case studyLearning Outcomes:Describe a flow process for data science problems (Remembering)Classify data science problems into standard typology (Comprehension)Develop R codes for data science solutions (Application)Correlate results to the solution approach followed (Analysis)Assess the solution approach (Evaluation)Construct use cases to validate approach and identify modifications required (Creating)INTENDED AUDIENCE:  Any interested learnerPREREQUISITES: 10 hrs of pre-course material will be provided, learners need to practise this to be ready to take the course.INDUSTRY SUPPORT: HONEYWELL, ABB, FORD, GYAN DATA PVT. LTD. 
      


COURSE LAYOUT Week 1: Course philosophy and introduction to RWeek 2: Linear algebra for data science 1. Algebraic view - vectors, matrices, product of matrix & vector, rank, null space, solution of over-determined  set of equations and pseudo-inverse) 2. Geometric view - vectors, distance, projections, eigenvalue decompositionWeek 3:Statistics (descriptive statistics, notion of probability, distributions, mean, variance, covariance, covariance  matrix, understanding univariate and multivariate normal distributions, introduction to hypothesis testing, confidence  interval for estimates)Week 4: OptimizationWeek 5: 1. Optimization 2. Typology of data science problems and a solution frameworkWeek 6: 1. Simple linear regression and verifying assumptions used in linear regression 2. Multivariate linear regression, model assessment, assessing importance of different variables, subset selectionWeek 7: Classification using logistic regressionWeek 8: Classification using kNN and k-means clustering"
https://www.classcentral.com/course/edx-visualizing-data-with-python-12530,"""A picture is worth a thousand words"". We are all familiar with this expression. It especially applies when trying to explain the insights obtained from the analysis of increasingly large datasets. Data visualization plays an essential role in the representation of both small and large-scale data. 
One of the key skills of a data scientist is the ability to tell a compelling story, visualizing data and findings in an approachable and stimulating way. 
In this course, you will learnhow to leverage a software tool to visualize datathat will also enable you to extract information, better understand the data, and make more effective decisions. 
You can start creating your own data science projects and collaborating with other data scientists using IBM Watson Studio. When you sign up, you get free access to Watson Studio. Start now and take advantage of this platform.



Module 1 -Introduction to Visualization Tools

Introduction to Data Visualization
Introduction to Matplotlib
Basic Plotting with Matplotlib
Dataset on Immigration to Canada
Line Plots

Module 2 -Basic Visualization Tools

Area Plots
Histograms
Bar Charts

Module 3 -Specialized Visualization Tools

Pie Charts
Box Plots
Scatter Plots
Bubble Plots

Module 4 -Advanced Visualization Tools

Waffle Charts
Word Clouds
Seaborn and Regression Plots

Module 5 -Creating Maps and Visualizing Geospatial Data

Introduction to Folium
Maps with Markers
Choropleth Maps"
https://www.classcentral.com/course/open-source-tools-for-data-science-10620,"What are some of the most popular data science tools, how do you use them, and what are their features? In this course, you'll learn about Jupyter Notebooks, RStudio IDE, Apache Zeppelin and Data Science Experience. You will learn about what each tool is used for, what programming languages they can execute, their features and limitations. With the tools hosted in the cloud on Cognitive Class Labs, you will be able to test each tool and follow instructions to run simple code in Python, R or Scala. To end the course, you will create a final project with a Jupyter Notebook on IBM Data Science Experience and demonstrate your proficiency preparing a notebook, writing Markdown, and sharing your work with your peers.

LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.
      


          Introducing Skills Network Labs
    -This week, you will get an overview of the various data science tools available to you, hosted on Skills Network Labs. You will create an account and start exploring some of the features.

Jupyter Notebooks
    -This week, you will learn about a popular data science tool, Jupyter Notebooks, its features, and why they are so popular among data scientists today.

Apache Zeppelin Notebooks
    -This week, you will learn about Apache Zeppelin Notebooks, its feature, and how they are different from Jupyter Notebooks.

RStudio IDE
    -This week, you will learn about a popular data science tool used by R programmers. You'll learn about the user interface and how to use its various features.

IBM Watson Studio
    -This week, you will learn about an enterprise-ready data science platform by IBM, called Watson Studio (formerley known as Data Science Experience). You'll learn about some of the features and capabilities of what data scientists use in the industry.

Project: Create and share a Jupyter Notebook"
https://www.classcentral.com/course/data-analytics-accountancy-1-9051,"Welcome to Data Analytics Foundations for Accountancy I! You’re joining thousands of learners currently enrolled in the course. I'm excited to have you in the class and look forward to your contributions to the learning community.

To begin, I recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class.

If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.

Good luck as you get started, and I hope you enjoy the course!
      


          Course Orientation
    -You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.

Module 1: Foundations
    -This module serves as the introduction to the course content and the course Jupyter server, where you will run your analytics scripts. First, you will read about specific examples of how analytics is being employed by Accounting firms. Next, you will learn about the capabilities of the course Jupyter server, and how to create, edit, and run notebooks on the course server. After this, you will learn how to write Markdown formatted documents, which is an easy way to quickly write formatted text, including descriptive text inside a course notebook. Finally, you will begin learning about Python, the programming language used in this course for data analytics.

Module 2: Introduction to Python
    -This module focuses on the basic features in the Python programming language that underlie most data analytics scripts. First, you will read about why accounting students should learn to write computer programs. Second, you will learn about basic data structures commonly used in Python programs. Third, you will learn how to write functions, which can be repeatedly called, in Python, and how to use them effectively in your own programs. Finally, you will learn how to control the execution process of your Python program by using conditional statements and looping constructs. At the conclusion of this module, you will be able to write Python scripts to perform basic data analytic tasks.

Module 3: Introduction to Data Analysis
    -This module introduces fundamental concepts in data analysis. First, you will read a report from the Association of Accountants and Financial Professionals in Business that explores Big Data in Accountancy. Next, you will learn about the Unix file system, which is the operating system used for most big data processing (as well as Linux and Mac OSX desktops and many mobile phones). Second, you will learn how to read and write data to a file from within a Python program. Finally, you will learn about the Pandas Python module that can simplify many challenging data analysis tasks, and includes the DataFrame, which programmatically mimics many of the features of a traditional spreadsheet.

Module 4: Statistical Data Analysis
    -This module introduces fundamental concepts in data analysis. First, you will read about how to perform many basic tasks in Excel by using the Pandas module in Python. Second, you will learn about the Numpy module, which provides support for fast numerical operations within Python. This module will focus on using Numpy with one-dimensional data (i.e., vectors or 1-D arrays), but a later module will explore using Numpy for higher-dimensional data. Third, you will learn about descriptive statistics, which can be used to characterize a data set by using a few specific measurements. Finally, you will learn about advanced functionality within the Pandas module including masking, grouping, stacking, and pivot tables.

Module 5: Introduction to Visualization
    -This module introduces visualization as an important tool for exploring and understanding data. First, the basic components of visualizations are introduced with an emphasis on how they can be used to convey information. Also, you will learn how to identify and avoid ways that a visualization can mislead or confuse a viewer. Next, you will learn more about conveying information to a user visually, including the use of form, color, and location. Third, you will learn how to actually create a simple visualization (basic line plot) in Python, which will introduce creating and displaying a visualization within a notebook, how to annotate a plot, and how to improve the visual aesthetics of a plot by using the Seaborn module. Finally, you will learn how to explore a one-dimensional data set by using rug plots, box plots, and histograms.

Module 6: Introduction to Probability
    -In this Module, you will learn the basics of probability, and how it relates to statistical data analysis. First, you will learn about the basic concepts of probability, including random variables, the calculation of simple probabilities, and several theoretical distributions that commonly occur in discussions of probability. Next, you will learn about conditional probability and Bayes theorem. Third, you will learn to calculate probabilities and to apply Bayes theorem directly by using Python. Finally, you will learn to work with both empirical and theoretical distributions in Python, and how to model an empirical data set by using a theoretical distribution.

Module 7: Exploring Two-Dimensional Data
    -This modules extends what you have learned in previous modules to the visual and analytic exploration of two-dimensional data. First, you will learn how to make two-dimensional scatter plots in Python and how they can be used to graphically identify a correlation and outlier points. Second, you will learn how to work with two-dimensional data by using the Numpy module, including a discussion on analytically quantifying correlations in data. Third, you will read about statistical issues that can impact understanding multi-dimensional data, which will allow you to avoid them in the future. Finally, you will learn about ordinary linear regression and how this technique can be used to model the relationship between two variables.

Module 8: Introduction to Density Estimation
    -Often, as part of exploratory data analysis, a histogram is used to understand how data are distributed, and in fact this technique can be used to compute a probability mass function (or PMF) from a data set as was shown in an earlier module. However, the binning approach has issues, including a dependance on the number and width of the bins used to compute the histogram. One approach to overcome these issues is to fit a function to the binned data, which is known as parametric estimation. Alternatively, we can construct an approximation to the data by employing a non-parametric density estimation. The most commonly used non-parametric technique is kernel density estimation (or KDE). In this module, you will learn about density estimation and specifically how to employ KDE. One often overlooked aspect of density estimation is the model representation that is generated for the data, which can be used to emulate new data. This concept is demonstrated by applying density estimation to images of handwritten digits, and sampling from the resulting model."
https://www.classcentral.com/course/edx-data-analysis-take-it-to-the-max-2916,"EX101x is for all of those struggling with data analysis. That crazy data collection from your boss? Megabytes of sensor data to analyze? Looking for a smart way visualize your data in order to make sense out of it? We’ve got you covered!
Using video lectures and hands-on exercises, we will teach you cutting-edge techniques and best practices that will boost your data analysis and visualization skills.
We will take a deep dive into data analysis with spreadsheets: PivotTables, VLOOKUPS, Named ranges, what-if analyses, making great graphs - all those will be covered in the first weeks of the course. After that, we will investigate the quality of the spreadsheet model, and especially how to make sure your spreadsheet remains error-free and robust.
Finally, once we have mastered spreadsheets, we will demonstrate other ways to store and analyze data. We will also look into how Python, a programming language, can help us with analyzing and manipulating data in spreadsheets.
EX101x is created using Excel 2013 and Windows. Most assignments can be made using another spreadsheet program and operating system as well, but we cannot offer full support for all configurations.
The goal of this course is to help you to overcome data analysis challenges in your work, research or studies. Therefore we encourage you to participate actively and to raise real data analysis problems that you face in our discussion forums. 
LICENSE
The course materials of this course are Copyright Delft University of Technology and are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike (CC-BY-NC-SA) 4.0 International License.



            Read more"
https://www.classcentral.com/course/big-data-management-6466,"Once you’ve identified a big data issue to analyze, how do you collect, store and organize your data using Big Data solutions?  In this course, you will experience various data genres and management tools appropriate for each.  You will be able to describe the reasons behind the evolving plethora of new big data platforms from the perspective of big data management systems and analytical tools.  Through guided hands-on tutorials, you will become familiar with techniques using real-time and semi-structured data examples.  Systems and tools discussed include: AsterixDB, HP Vertica, Impala, Neo4j, Redis, SparkSQL. This course provides techniques to extract value from existing untapped data sources and discovering new data sources.

At the end of this course, you will be able to:
 * Recognize different data elements in your own work and in everyday life problems
 * Explain why your team needs to design a Big Data Infrastructure Plan and Information System Design
 * Identify the frequent data operations required for various types of data
 * Select a data model to suit the characteristics of your data 
 * Apply techniques to handle streaming data
 * Differentiate between a traditional Database Management System and a Big Data Management System
 * Appreciate why there are so many data management systems
 * Design a big data information system for an online game company

This course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.

Hardware Requirements: 
(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. 

Software Requirements: 
This course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.
      


            Read more
          



          Introduction to Big Data Modeling and Management
    -Welcome to this course on big data modeling and management. Modeling and managing data is a central focus of all big data projects. In these lessons we introduce you to the concepts behind big data modeling and management and set the stage for the remainder of the course. 

Big Data Modeling
    -Modeling big data depends on many factors including data structure, which operations may be performed on the data, and what constraints are placed on the models. In these lessons you will learn the details about big data modeling and you will gain the practical skills you will need for modeling your own big data projects.

Big Data Modeling (Part 2)
    -These lessons continue to shed light on big data modeling with specific approaches including vector space models, graph data models, and more. 

Working With Data Models
    -Data models deal with many different types of data formats. Streaming data is becoming ubiquitous, and working with streaming data requires a different approach from working with static data. In these lessons you will gain practical hands-on experience working with different forms of streaming data including weather data and twitter feeds. 

Big Data Management: The ""M"" in DBMS
    -Managing big data requires a different approach to database management systems because of the wide variation in data structure which does not lend itself to traditional DBMSs. There are many applications available to help with big data management. In these lessons we introduce you to some of these applications and provide insight into how and when they might be appropriate for your own big data management challenges. 

Designing a Big Data Management System for an Online Game
    -In these lessons we give you the opportunity to learn about big data modeling and management using a fictitious online game called ""Catch the Pink Flamingo""."
https://www.classcentral.com/course/r-programming-environment-7170,"This course provides a rigorous introduction to the R programming language, with a  particular focus on using R for software development in a data science setting. Whether you are part of a data science team or working individually within a community of developers, this course will give you the knowledge of R needed to make useful contributions in those settings. As the first course in the Specialization, the course provides the essential foundation of R needed for the following courses. We cover basic R concepts and language fundamentals, key concepts like tidy data and related ""tidyverse"" tools, processing and manipulation of complex and large datasets, handling textual data, and basic data science tasks. Upon completing this course, learners will have fluency at the R console and will be able to create tidy datasets from a wide range of possible data sources.
      


          Basic R Language
    -In this module, you'll learn the basics of R, including syntax, some tidy data principles and processes, and how to read data into R.  

Basic R Language: Lesson Choices

Data Manipulation
    -During this module, you'll learn to summarize, filter, merge, and otherwise manipulate data in R, including working through the challenges of dates and times. 

Data Manipulation: Lesson Choices

Text Processing, Regular Expression, & Physical Memory
    -During this module, you'll learn to use R tools and packages to deal with text and regular expressions. You'll also learn how to manage and get the most from your computer's physical memory when working in R. 

Text Processing, Regular Expression, & Physical Memory: Lesson Choices
    -Choice 1: Get credit while using swirl | Choice 2: Get credit by providing a code from swirl

Large Datasets
    -In this final module, you'll learn how to overcome the challenges of working with large datasets both in memory and out as well as how to diagnose problems and find help."
https://www.classcentral.com/course/compdata-388,"In this course you will learn how to program in R and how to use R for
effective data analysis. You will learn how to install and configure software
necessary for a statistical programming environment, discuss generic programming
language concepts as they are implemented in a high-level statistical language.
The course covers practical issues in statistical computing which includes
programming in R, reading data into R, creating informative data graphics,
accessing R packages, creating R packages with documentation, writing R
functions, debugging, and organizing and commenting R code. Topics in statistical
data analysis and optimization will provide working examples.
      


A student who has completed this course is able to:Read formatted data into RSubset, remove missing values from, and clean tabular dataWrite custom functions in R to implement new functionality and making use of control structures such as loops and conditionalsUse the R code debugger to identify problems in R functionsMake a scatterplot/boxplot/histogram/image plot and modify a plot with custom annotationsDefine a new data class in R and write methods for that class"
https://www.classcentral.com/course/edx-sp20-computing-for-data-analysis-8223,"The modern data analysis pipeline involves collection, preprocessing, storage, analysis, and interactive visualization of data.
The goal of this course, part of the Analytics: Essential Tools and Methods MicroMasters program, is for you to learn how to build these components and connect them using modern tools and techniques.
In the course, you’ll see how computing and mathematics come together. For instance, “under the hood” of modern data analysis lies numerical linear algebra, numerical optimization, and elementary data processing algorithms and data structures. Together, they form the foundations of numerical and data-intensive computing.
The hands-on component of this course will develop your proficiency with modern analytical tools. You will learn how to mash up Python, R, and SQL through Jupyter notebooks, among other tools. Furthermore, you will apply these tools to a variety of real-world datasets, thereby strengthening your ability to translate principles into practice."
https://www.classcentral.com/course/data-manipulation-4473,"Data analysis has replaced data acquisition as the bottleneck to evidence-based decision making --- we are drowning in it.  Extracting knowledge from large, heterogeneous, and noisy datasets requires not only powerful computing resources, but the programming abstractions to use them effectively.  The abstractions that emerged in the last decade blend ideas from parallel databases, distributed systems, and programming languages to create a new class of scalable data analytics platforms that form the foundation for data science at realistic scales.

In this course, you will learn the landscape of relevant systems, the principles on which they rely, their tradeoffs, and how to evaluate their utility against your requirements. You will learn how practical systems were derived from the frontier of research in computer science and what systems are coming on the horizon.   Cloud computing, SQL and NoSQL databases, MapReduce and the ecosystem it spawned, Spark and its contemporaries, and specialized systems for graphs and arrays will be covered.

You will also learn the history and context of data science, the skills, challenges, and methodologies the term implies, and how to structure a data science project.  At the end of this course, you will be able to:

Learning Goals: 
1. Describe common patterns, challenges, and approaches associated with data science projects, and what makes them different from projects in related fields.
2. Identify and use the programming models associated with scalable data manipulation, including relational algebra, mapreduce, and other data flow models.
3. Use database technology adapted for large-scale analytics, including the concepts driving parallel databases, parallel query processing, and in-database analytics
4. Evaluate key-value stores and NoSQL systems, describe their tradeoffs with comparable systems, the details of important examples in the space, and future trends.
5. “Think” in MapReduce to effectively write algorithms for systems including Hadoop and Spark.  You will understand their limitations, design details, their relationship to databases, and their associated ecosystem of algorithms, extensions, and languages.
write programs in Spark
6. Describe the landscape of specialized Big Data systems for graphs, arrays, and streams
      


            Read more
          



          Data Science Context and Concepts
    -Understand the terminology and recurring principles associated with data science, and understand the structure of data science projects and emerging methodologies to approach them.    Why does this emerging field exist?  How does it relate to other fields?  How does this course distinguish itself?  What do data science projects look like, and how should they be approached?  What are some examples of data science projects?  

Relational Databases and the Relational Algebra
    -Relational Databases are the workhouse of large-scale data management.  Although originally motivated by problems in enterprise operations, they have proven remarkably capable for analytics as well.  But most importantly, the principles underlying relational databases are universal in managing, manipulating, and analyzing data at scale.  Even as the landscape of large-scale data systems has expanded dramatically in the last decade, relational models and languages have remained a unifying concept.  For working with large-scale data, there is no more important programming model to learn.

MapReduce and Parallel Dataflow Programming
    -The MapReduce programming model (as distinct from its implementations) was proposed as a simplifying abstraction for parallel manipulation of massive datasets, and remains an important concept to know when using and evaluating modern big data platforms.  

NoSQL: Systems and Concepts
    -NoSQL systems are purely about scale rather than analytics, and are arguably less relevant for the practicing data scientist.  However, they occupy an important place in many practical big data platform architectures, and data scientists need to understand their limitations and strengths to use them effectively.

Graph Analytics
    -Graph-structured data are increasingly common in data science contexts due to their ubiquity in modeling the communication between entities: people (social networks), computers (Internet communication), cities and countries (transportation networks), or corporations (financial transactions).  Learn the common algorithms for extracting information from graph data and how to scale them up."
https://www.classcentral.com/course/bigdata-454,"The past decade has witnessed the successful of application of many AI techniques used at `web-scale’, on what are popularly referred to as big data platforms based on the map-reduce parallel computing paradigm and associated technologies such as distributed file systems, no-SQL databases and stream computing engines. Online advertising, machine translation, natural language understanding, sentiment mining, personalized medicine, and national security are some examples of such AI-based web-intelligence applications that are already in the public eye. Others, though less apparent, impact the operations of large enterprises from sales and marketing to manufacturing and supply chains. In this course we explore some such applications, the AI/statistical techniques that make them possible, along with parallel implementations using map-reduce and related platforms.

This course was offered thrice during Fall 2012, Spring 2012 and Fall 2013; in Fall of both years it was also taken for credit at IIT Delhi and IIIT Delhi. During this period, I also wrote a book to elucidate the ideas discussed in the course at a 'popular' level:

The Intelligent Web: Search, Smart Algorithms and Big Data published by Oxford University Press, UK, in November 2013.

 Now in this edition, the course is being offered in 'self-study' mode.



          Introduction and Overview 
Look: Search, Indexing and Memory
Listen: Streams, Information and Language, Analyzing Sentiment and Intent
Load: Databases and their Evolution, Big data Technology and TrendsProgramming: Map-Reduce
Learn: Classification, Clustering, and Mining, Information Extraction
Connect: Reasoning: Logic and its Limits, Dealing with UncertaintyProgramming: Bayesian Inference for Medical Diagnostics
Predict: Forecasting, Neural Models, Deep Learning, and Research TopicsData Analysis: Regression and Feature Selection"
https://www.classcentral.com/course/graph-analytics-4249,"Want to understand your data network structure and how it changes under different conditions? Curious to know how to identify closely interacting clusters within a graph? Have you heard of the fast-growing area of graph analytics and want to learn more? This course gives you a broad overview of the field of graph analytics so you can learn new ways to model, store, retrieve and analyze graph-structured data.

After completing this course, you will be able to model a problem into a graph database and perform analytical tasks over the graph in a scalable manner.  Better yet, you will be able to apply these techniques to understand the significance of your data sets for your own projects.
      


          Welcome to Graph Analytics
    -Meet your instructor, Amarnath Gupta and learn about the course objectives.

Introduction to Graphs
    -Welcome! This week we will get a first exposure to graphs and their use in everyday life.  By the end of the module you will be able to create a graph applying core mathematical properties of graphs, and identify the kinds of analysis questions one might be able to ask of such a graph.  We hope the you will be inspired as to how graphical representations might enable you to answer new Big Data problems!

Graph Analytics

Graph Analytics Techniques
    -Welcome to the 4th module in the Graph Analytics course. Last week, we got a glimpse of a number of graph properties and why they are important. This week we will use those properties for analyzing graphs using a free and powerful graph analytics tool called Neo4j. We will demonstrate how to use Cypher, the query language of Neo4j, to perform a wide range of analyses on a variety of graph networks. 

Computing Platforms for Graph Analytics
    -In the last two modules we have learned about graph analytics and graph data management. This week we will study how they come together. There are programming models and software frameworks created specifically for graph analytics.  In this module we'll give an introductory tour of these models and frameworks.  We will learn to implement what you learned in Week 2 and build on it using GraphX and Giraph."
https://www.classcentral.com/course/genpython-3476,"This class provides an introduction to the Python programming language and the iPython notebook. This is the third course in the Genomic Big Data Science Specialization from Johns Hopkins University.
      


          Week One
    -This week we will have an overview of Python and take the first steps towards programming.

Week Two
    -In this module, we'll be taking a look at Data Structures and Ifs and Loops.

Week Three
    -In this module, we have a long three-part lecture on Functions as well as a 10-minute look at Modules and Packages.

Week Four
    -In this module, we have another long three-part lecture, this time about Communicating with the Outside, as well as a final lecture about Biopython."
https://www.classcentral.com/course/intro-to-big-data-4164,"Interested in increasing your knowledge of the Big Data landscape?  This course is for those new to data science and interested in understanding why the Big Data Era has come to be.  It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems.  It is for those who want to start thinking about how Big Data might be useful in their business or career.  It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world!

At the end of this course, you will be able to:

* Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. 

* Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.

* Get value out of Big Data by using a 5-step process to structure your analysis. 

* Identify what are and what are not big data problems and be able to recast big data problems as data science questions.

* Provide an explanation of the architectural components and programming models used for scalable big data analysis.

* Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model.

* Install and run a program using Hadoop!

This course is for those new to data science.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  

Hardware Requirements:
(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size.  

Software Requirements:
This course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.
      


            Read more
          



          Welcome 
    -Welcome to the Big Data Specialization!  We're excited for you to get to know us and we're looking forward to learning about you! 

Big Data: Why and Where
    -Data -- it's been around (even digitally) for a while.  What makes data ""big"" and where does this big data come from?

Characteristics of Big Data and Dimensions of Scalability
    -You may have heard of the ""Big Vs"".  We'll give examples and descriptions of the commonly discussed 5.  But, we want to propose a 6th V and we'll ask you to practice writing Big Data questions targeting this V -- value.

Data Science: Getting Value out of Big Data
    -We love science and we love computing, don't get us wrong.  But the reality is we care about Big Data because it can bring value to our companies, our lives, and the world.  In this module we'll introduce a 5 step process for approaching data science problems.

Foundations for Big Data Systems and Programming
    -Big Data requires new programming frameworks and systems.  For this course, we don't  programming knowledge or experience -- but we do want to give you a grounding in some of the key concepts.

Systems: Getting Started with Hadoop
    -Let's look at some details of Hadoop and MapReduce.  Then we'll go ""hands on"" and actually perform a simple MapReduce task in the Cloudera VM.  Pay attention - as we'll guide you in ""learning by doing"" in diagramming a MapReduce task as a Peer Review."
https://www.classcentral.com/course/rprog-1713,"In this course you will learn how to program in R and how to use R for effective data analysis. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language. The course covers practical issues in statistical computing which includes programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting R code. Topics in statistical data analysis will provide working examples.
      


          Week 1: Background, Getting Started, and Nuts & Bolts
    -This week covers the basics to get you started up with R. The Background Materials lesson contains information about course mechanics and some videos on installing R. The Week 1 videos cover the history of R and S, go over the basic data types in R, and describe the functions for reading and writing data. I recommend that you watch the videos in the listed order, but watching the videos out of order isn't going to ruin the story. 

Week 2: Programming with R
    -Welcome to Week 2 of R Programming. This week, we take the gloves off, and the lectures cover key topics like control structures and functions. We also introduce the first programming assignment for the course, which is due at the end of the week.

Week 3: Loop Functions and Debugging
    -We have now entered the third week of R Programming, which also marks the halfway point. The lectures this week cover loop functions and the debugging tools in R. These aspects of R make R useful for both interactive work and writing longer code, and so they are commonly used in practice.

Week 4: Simulation & Profiling
    -This week covers how to simulate data in R, which serves as the basis for doing simulation studies. We also cover the profiler in R which lets you collect detailed information on how your R functions are running and to identify bottlenecks that can be addressed. The profiler is a key tool in helping you optimize your programs. Finally, we cover the str function, which I personally believe is the most useful function in R."
https://www.classcentral.com/course/python-data-analysis-6671,"This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses. 

This course should be taken before any of the other Applied Data Science with Python courses: Applied Plotting, Charting & Data Representation in Python, Applied Machine Learning in Python, Applied Text Mining in Python, Applied Social Network Analysis in Python.
      


          Week 1
    -In this week you'll get an introduction to the field of data science, review common Python functionality and features which data scientists use, and be introduced to the Coursera Jupyter Notebook for the lectures. All of the course information on grading, prerequisites, and expectations are on the course syllabus, and you can find more information about the Jupyter Notebooks on our Course Resources page.

Week 2
    -In this week of the course you'll learn the fundamentals of one of the most important toolkits Python has for data cleaning and processing -- pandas. You'll learn how to read in data into DataFrame structures, how to query these structures, and the details about such structures are indexed. The module ends with a programming assignment and a discussion question.

Week 3
    -In this week you'll deepen your understanding of the python pandas library by learning how to merge DataFrames, generate summary tables, group data into logical pieces, and manipulate dates. We'll also refresh your understanding of scales of data, and discuss issues with creating metrics for analysis. The week ends with a more significant programming assignment.

Week 4
    -In this week of the course you'll be introduced to a variety of statistical techniques such a distributions, sampling and t-tests. The majority of the week will be dedicated to your course project, where you'll engage in a real-world data cleaning activity and provide evidence for (or against!) a given hypothesis. This project is suitable for a data science portfolio, and will test your knowledge of cleaning, merging, manipulating, and test for significance in data. The week ends with two discussions of science and the rise of the fourth paradigm -- data driven discovery."
https://www.classcentral.com/course/advanced-r-7174,"This course covers advanced topics in R programming that are necessary for developing powerful, robust, and reusable data science tools. Topics covered include functional programming in R, robust error handling, object oriented programming, profiling and benchmarking, debugging, and proper design of functions. Upon completing this course you will be able to identify and abstract common data analysis tasks and to encapsulate them in user-facing functions. Because every data science environment encounters unique data challenges, there is always a need to develop custom software specific to your organization’s mission. You will also be able to define new data types in R and to develop a universe of functionality specific to those data types to enable cleaner execution of data science tasks and stronger reusability within a team.
      


          Welcome to Advanced R Programming
    -This course covers advanced topics in R programming that are necessary for developing powerful, robust, and reusable data science tools. Topics covered include functional programming in R, robust error handling, object oriented programming, profiling and benchmarking, debugging, and proper design of functions. Upon completing this course you will be able to identify and abstract common data analysis tasks and to encapsulate them in user-facing functions. Because every data science environment encounters unique data challenges, there is always a need to develop custom software specific to your organization’s mission. You will also be able to define new data types in R and to develop a universe of functionality specific to those data types to enable cleaner execution of data science tasks and stronger reusability within a team.

Functions
    -This module begins with control structures in R for controlling the logical flow of an R program. We then move on to functions, their role in R programming, and some guidelines for writing good functions.

Functions: Lesson Choices

Functional Programming
    -Functional programming is a key aspect of R and is one of R's differentiating factors as a data analysis language. Understanding the concepts of functional programming will help you to become a better data science software developer. In addition, we cover error and exception handling in R for writing robust code.

Functional Programming: Lesson Choices

Debugging and Profiling
    -Debugging tools are useful for analyzing your code when it exhibits unexpected behavior. We go through the various debugging tools in R and how they can be used to identify problems in code. Profiling tools allow you to see where your code spends its time and to optimize your code for maximum efficiency.

Object-Oriented Programming
    -Object oriented programming allows you to define custom data types or classes and a set of functions for handling that data type in a way that you define. R has a three different methods for implementing object oriented programming and we will cover them in this section."
https://www.classcentral.com/course/python-analysis-9551,"This course will continue the introduction to Python programming that started with Python Programming Essentials and Python Data Representations.  We'll learn about reading, storing, and processing tabular data, which are common tasks.  We will also teach you about CSV files and Python's support for reading and writing them.  CSV files are a generic, plain text file format that allows you to exchange tabular data between different programs. These concepts and skills will help you to further extend your Python programming knowledge and allow you to process more complex data.

By the end of the course, you will be comfortable working with tabular data in Python. This will extend your Python programming expertise, enabling you to write a wider range of scripts using Python.

This course uses Python 3.  While most Python programs continue to use Python 2, Python 3 is the future of the Python programming language. This course uses basic desktop Python development environments, allowing you to run Python programs directly on your computer.
      


          Dictionaries
    -This module will teach you about Python's dictionary data type and its capabilities.  Dictionaries are used to map keys to values within programs.

Tabular Data and Nested Data Structures
    -This module will teach you about storing tabular data within Python programs using lists and dictionaries.

Tabular Data and CSV Files
    -This module will teach you the basics of CSV files and how to read them from Python programs. We will discuss the use of Python's csv module to help you access tabular data in CSV files.

Organizing Data
    -This module will teach you how to sort data in Python. You will organize and analyze tabular data."
https://www.classcentral.com/course/data-science-k-means-clustering-python-13623,"Organisations all around the world are using data to predict behaviours and extract valuable real-world insights to inform decisions. Managing and analysing big data has become an essential part of modern finance, retail, marketing, social science, development and research, medicine and government.

This MOOC, designed by an academic team from Goldsmiths, University of London, will quickly introduce you to the core concepts of Data Science to prepare you for intermediate and advanced Data Science courses. It focuses on the basic mathematics, statistics and programming skills that are necessary for typical data analysis tasks. 

You will consider these fundamental concepts on an example data clustering task, and you will use this example to learn basic programming skills that are necessary for mastering Data Science techniques. During the course, you will be asked to do a series of mathematical and programming exercises and a small data clustering project for a given dataset.
      


          Week 1: Foundations of Data Science: K-Means Clustering in Python
    -This week we will introduce you to the course and to the team who will be guiding you through the course over the next 5 weeks. The aim of this week's material is to gently introduce you to Data Science through some real-world examples of where Data Science is used, and also by highlighting some of the main concepts involved.

Week 2: Means and Deviations in Mathematics and Python

Week 3: Moving from One to Two Dimensional Data

Week 4: Introducing Pandas and Using K-Means to Analyse Data

Week 5: A Data Clustering Project"
https://www.classcentral.com/course/edx-data-science-for-construction-architecture-and-engineering-19193,"The building industry is exploding with data sources that impact the energy performance of the built environment and health and well-being of occupants. Spreadsheets just don’t cut it anymore as the sole analytics tool for professionals in this field. Participating in mainstream data science courses might provide skills such as programming and statistics, however the applied context to buildings is missing, which is the most important part for beginners.
This course focuses on the development of data science skills for professionals specifically in the built environment sector. It targets architects, engineers, construction and facilities managers with little or no previous programming experience. An introduction to data science skills is given in the context of the building life cycle phases. Participants will use large, open data sets from the design, construction, and operations of buildings to learn and practice data science techniques.
Essentially this course is designed to add new tools and skills to supplement spreadsheets. Major technical topics include data loading, processing, visualization, and basic machine learning using the Python programming language, the Pandas data analytics and sci-kit learn machine learning libraries, and the web-based Colaboratory environment. In addition, the course will provide numerous learning paths for various built environment-related tasks to facilitate further growth.



Week 1: Introduction to Course and Python Fundamentals – In this introduction, an overview of key Python concepts is covered as well as the motivating factors for building industry professionals to learn to code. The NZEB at the NUS School of Design and Environment is introduced as an example of a building that uses various data science-related technologies in its design, construction, and operations.
Week 2: Introduction to the Pandas Data Analytics Library and Design Phase Application Examples – The foundational functions of Pandas are demonstrated in the context of the integrated design process through the processing of data from parametric EnergyPlus models. Further future learning path examples are introduced for the Design Phase including building information modeling (BIM) using Revit or Rhino, spatial analytics, and building performance modeling Python libraries.
Week 3: Pandas Analysis of Time-Series Data from IoT and Construction Phase Application Examples – Time-series analysis Pandas functions are demonstrated in the Construction Phase through the analysis of hourly IoT data from electrical energy meters. Further future learning path examples are introduced for the Construction Phase including project management, building management system (BMS) data analysis, and digital construction such as robotic fabrication.
Week 4: Statistics and Visualization Basics and Operations Phase Application Examples – Various statistical aggregations and visualization techniques using Pandas and the Seaborn library are demonstrated on Operations Phase occupant comfort data from the ASHRAE Thermal Comfort Database II. Further future learning path examples are introduced for the Operations Phase including energy auditing, IoT analysis, and occupant detection and reinforcement learning.
Week 5: Introduction to Machine Learning for the Built Environment – This concluding section gives an overview of the motivations and opportunities for the use of prediction in the built environment. Prediction, classification, and clustering using the sci-kit learn library is demonstrated on electrical meter and occupant comfort data. The course is concluded with suggestions on more in-depth Python, Data Science, and Statistics courses on EDx.
Development of this curriculum was led by Dr. Clayton Miller with support from NUS students Charlene Tan, Chun Fu, James Zhan, Matias Quintana, and Vanessa Neo."
https://www.classcentral.com/course/missing-data-6086,"This course will cover the steps used in weighting sample surveys, including methods for adjusting for nonresponse and using data external to the survey for calibration.  Among the techniques discussed are adjustments using estimated response propensities, poststratification, raking, and general regression estimation.  Alternative techniques for imputing values for missing items will be discussed.  For both weighting and imputation, the capabilities of different statistical software packages will be covered, including R®, Stata®, and SAS®.
      


          General Steps in Weighting
    -Weights are used to expand a sample to a population.  To accomplish this, the weights may correct for coverage errors in the sampling frame, adjust for nonresponse, and reduce variances of estimators by incorporating covariates. The series of steps needed to do this are covered in Module 1.

Specific Steps
    -Specific steps in weighting include computing base weights, adjusting if there are cases whose eligibility we are unsure of, adjusting for nonresponse, and using covariates to calibrate the sample to external population controls.  We flesh out the general steps with specific details here.

Implementing the Steps
    -Software is critical to implementing the steps, but the R system is an excellent source of free routines. This module covers several R packages, including sampling, survey, and PracTools that will select samples and compute weights.

Imputing for Missing Items
    -In most surveys there will be items for which respondents do not provide information, even though the respondent completed enough of the data collection instrument to be considered ""complete"".  If only the cases with all items present are retained when fitting a model, quite a few cases may be excluded from the analysis. Imputing for the missing items avoids dropping the missing cases.  We cover methods of doing the imputing and of reflecting the effects of imputations on standard errors in this module.

Summary of Course 5
    -We briefly summarize the methods of weighting and imputation that were covered in Course 5."
https://www.classcentral.com/course/edx-data-science-research-methods-r-edition-11640,"Data scientists are often trained in the analysis of data. However, the goal of data science is to produce good understanding of some problem or idea and build useful models on this understanding. Because of the principle of ""garbage in, garbage out,"" it is vital that the data scientist know how to evaluate the quality of information that comes into a data analysis. This is especially the case when data are collected specifically for some analysis (e.g., a survey). 
In this course, you will learn the fundamentals of the research process--from developing a good question to designing good data collection strategies to putting results in context. Although the data scientist may often play a key part in data analysis, the entire research process must work cohesively for valid insights to be gleaned. 
Developed as a language with statistical analysis and modeling in mind, R has become an essential tool for doing real-world Data Science. With this edition of Data Science Research Methods, all of the labs are done with R, while the videos are tool-agnostic. If you prefer your Data Science to be done with Python, please see Data Science Research Methods: Python Edition. 
edX offers financial assistance for learners who want to earn Verified Certificates but who may not be able to pay the fee. To apply for financial assistance, enroll in the course, then follow this link to complete an application for assistance.




The Research Process
Planning for Analysis
Research Claims
Measurement
Correlational and Experimental Design

Note: This syllabus is preliminary and subject to change."
https://www.classcentral.com/course/data-collection-analytics-project-6083,"In this course you will learn how to use survey weights to estimate descriptive statistics, like means and totals, and more complicated quantities like model parameters for linear and logistic regressions.  Software capabilities will be covered with R® receiving particular emphasis.  The course will also cover the basics of record linkage and statistical matching—both of which are becoming more important as ways of combining data from different sources.  Combining of datasets raises ethical issues which the course reviews.  Informed consent may have to be obtained from persons to allow their data to be linked. You will learn about differences in the legal requirements in different countries.
      


          Basic Estimation
    -After completing Modules 1 and 2 of this course you will understand how to estimate descriptive statistics, overall and for subgroups, when you deal with survey data.  We will review software for estimation (R, Stata, SAS) with examples for how to estimate things like means, proportions, and totals.  You will also learn how to estimate parameters in linear, logistic, and other models and learn software options with emphasis on R. Module 3 and 4 discuss how you can add additional data to your analysis. This requires knowing about record linkage techniques, and what it takes to get permission to link data.

Models
    -Module 2 covers how to estimate linear and logistic model parameters using survey data. After completing this module, you will understand how the methods used differ from the ones for non-survey data. We also cover the features of survey data sets that need to be accounted for when estimating standard errors of estimated model parameters.

Record Linkage
    -Module starts with the current debate on using more (linked) administrative records in the U.S. Federal Statistical System, and a general motivation for linking records. Several examples will be given on why it is useful to link data. Challenges of record linkage will be discussed. A brief overview over key linkage techniques is included as well.

Ethics
    -This module will discuss key issues in obtaining consent to record linkage. Failure to consent can lead to bias estimates. Current research examples will be given as well as practical suggestions on how to obtain linkage consent."
https://www.classcentral.com/course/intro-accounting-data-analytics-visual-14477,"Accounting has always been about analytical thinking. From the earliest days of the profession, Luca Pacioli emphasized the importance of math and order for analyzing business transactions. The skillset that accountants have needed to perform math and to keep order has evolved from pencil and paper, to typewriters and calculators, then to spreadsheets and accounting software. A new skillset that is becoming more important for nearly every aspect of business is that of big data analytics: analyzing large amounts of data to find actionable insights. This course is designed to help accounting students develop an analytical mindset and prepare them to use data analytic programming languages like Python and R.

We’ve divided the course into three main sections. In the first section, we bridge accountancy to analytics. We identify how tasks in the five major subdomains of accounting (i.e., financial, managerial, audit, tax, and systems) have historically required an analytical mindset, and we then explore how those tasks can be  completed more effectively and efficiently by using big data analytics. We then present a FACT framework for guiding big data analytics: Frame a question, Assemble data, Calculate the data, and Tell others about the results.

In the second section of the course, we emphasize the importance of assembling data. Using financial statement data, we explain desirable characteristics of both data and datasets that will lead to effective calculations and visualizations.

In the third, and largest section of the course, we demonstrate and explore how Excel and Tableau can be used to analyze big data. We describe visual perception principles and then apply those principles to create effective visualizations. We then examine fundamental data analytic tools, such as regression, linear programming (using Excel Solver), and clustering in the context of point of sale data and loan data. We conclude by demonstrating the power of data analytic programming languages to assemble, visualize, and analyze data. We introduce Visual Basic for Applications  as an example of a programming language, and the Visual Basic Editor as an example of an integrated development environment (IDE).
      


            Read more
          



          INTRODUCTION TO THE COURSE
    -In this module, you will become familiar with the course, your instructor and your classmates, and our learning environment. This orientation module will also help you obtain the technical skills required to navigate and be successful in this course.

MODULE 1: INTRODUCTION TO ACCOUNTANCY ANALYTICS
    -In this module, you will learn how the accounting profession has evolved. You will recognize how data analytics has influenced the accounting profession and how accountants have the ability to impact how data analytics is used in the profession, as well as in an organization. Finally, you will learn how data analytics is influencing the different subdomains within accounting.

MODULE 2: ACCOUNTING ANALYSIS AND AN ANALYTICS MINDSET
    -In this module, you will learn to recognize the importance of making room for empirical enquiry in decision making. You will explore characteristics of an analytical mindset in business and accounting contexts, and link those to your core courses. You will then evaluate a framework for making data-driven decisions using big data.

MODULE 3: DATA AND ITS PROPERTIES
    -This module looks at specific characteristics of data that make it useful for decision making.

MODULE 4: DATA VISUALIZATION 1
    -In this module, you will learn fundamental principles that underlie data visualizations. Using those principles, you will identify use cases for different charts and learn how to build those charts in Excel. You will then use your knowledge of different charts to identify alternative charts that are better suited for directing attention.

MODULE 5: DATA VISUALIZATION 2
    -In this module, you’ll learn how to use Tableau to do with data what spies do when observing their surroundings: get an overview of the data, narrow in on certain aspects of the data that seem abnormal, and then analyze the data. Tableau is a great tool for facilitating the overview, zoom, then filter details-on-demand approach. Tableau is a lot like a more powerful version of Excel's pivot table and pivot chart functionality.

MODULE 6: ANALYTIC TOOLS IN EXCEL 1
    -In this module, you'll be guided through a mini-case study that will illustrate the first three parts of the FACT model, with a focus on the C, or calculations part of the FACT model. First, you will perform a correlation analysis to identify two-way relationships, and analyze correlations using a correlation matrix and scatter plots. You will then build on your knowledge of correlations and learn how to perform regression analysis in Excel. Finally, you will learn how to interpret and evaluate the diagnostic metrics and plots of a regression analysis.

MODULE 7: ANALYTIC TOOLS IN EXCEL 2
    -In this module, you’ll learn how the regression algorithm can be applied to fit a wide variety of relationships among data. Specifically, you’ll learn how to set up the data and run a regression to estimate the parameters of nonlinear relationships, categorical independent variables. You’ll also investigate if the effect of an independent variable depends on the level of another independent variable by including interaction terms in the multiple regression model. Another aspect of this module is learning how to evaluate models, regression or otherwise, to find the most favorable levels of the independent variables. For models that explain revenue, the most favorable levels of the independent variables will maximize revenue. In contrast, if you have a model that describes costs, like a budget, then the most favorable levels of the independent variables will minimize costs. Optimizing models can be difficult because there are so many inputs and constraints that need to be managed. In this module, you’ll learn how to use the Solver Add-In to find the optimal level of inputs. For some models, the dependent variable is a binary variable that has only two values, such as true/false, win/lose, or invest/not invest. In these situations, a special type of regression, called logistic regression, is used to predict how each observation should be classified. You’ll learn about the logit transformation that’s used to convert a binary outcome to a linear relationship with the independent variables. Excel doesn’t have a built-in logistic regression tool, so you’ll learn how to manually design a logistic regression model, and then optimize the parameters using the Solver Add-In tool.

MODULE 8: AUTOMATION IN EXCEL
    -The lessons in this module are organized around several useful tasks, including stacking multiple dataframes together into one dataframe, creating multiple histograms to accompany the descriptive statistics, and learning how to perform k-means clustering. After going through this module, you’ll not only gain a foundation to help you understand coding, but you’ll also learn more about analyzing financial data. Along the way, I hope that you’ll also pick up on a few other useful Excel functions."
https://www.classcentral.com/course/gaming-big-data-11192,"Use data analysis to build better gaming experiences
The video games industry collects vast amounts of data from its users. But most of this data is disregarded despite its value to the gaming industry.
This course will show you how to store and analyse data effectively and gain insights into game users’ actions and behaviours.
You’ll find out about the different models of data, such as tabular data, atomic data, and relational data.
You’ll understand how to store non-relational data at scale, and why data can be hard to distribute.
You’ll learn how to build better gaming experiences and increase profits.
This course is aimed at those who already work in the games industry, but may also be of interest to those looking to work in the sector.
In order to get the best out of this course you should have a laptop or desktop computer (Windows or Mac) that can run virtual machine software such as VirtualBox or Docker.  You should be happy to install software on your machine such as Python or R Studio.  Links and instructions for installation and use will be included during the course."
https://www.classcentral.com/course/sql-data-science-capstone-17298,"Data science is a dynamic and growing career field that demands knowledge and skills-based in SQL to be successful. This course is designed to provide you with a solid foundation in applying SQL skills to analyze data and solve real business problems.

Whether you have successfully completed the other courses in the Learn SQL Basics for Data Science Specialization or are taking just this course, this project is your chance to apply the knowledge and skills you have acquired to practice important SQL  querying and solve problems with data. You will participate in your own personal or professional journey to create a portfolio-worthy piece from start to finish. You will choose a dataset and develop a project proposal. You will explore your data and perform some initial statistics you have learned through this specialization. You will uncover analytics for qualitative data and consider new metrics that make sense from the patterns that surface in your analysis. You will put all of your work together in the form of a presentation where you will tell the story of your findings. Along the way, you will receive feedback through the peer-review process. This community of fellow learners will provide additional input to help you refine your approach to data analysis with SQL and present your findings to clients and management.
      


          Getting Started and Milestone 1: Project Proposal and Data Selection/Preparation
    -In this first milestone, you will select your client and import your dataset. You will begin to explore your data to understand it and make assumptions about your data. You will draft a project proposal to act as a guide as you explore your data and prove or disprove your hypotheses.

Milestone 2: Descriptive Stats & Understanding Your Data
    -In this milestone, you will start to execute your project proposal. You will start looking at your data and perform initial statistic models to explore your data and determine what you have available to you. 

Milestone 3: Beyond Descriptive Stats (Dive Deeper/Go Broader)
    -In this milestone, you will go beyond the descriptive statistics you completed in the last milestone. This milestone is really about diving deeper to analyze your data, beyond descriptive stats. Maybe you need to analyze qualitative data or textual data to get a full picture.

Milestone 4: Presenting Your Findings (Storytelling)
    -In this milestone, you will present your findings. You will identify your audience and create a presentation tailored to them. You will be able to tell the story of analyses and make recommendations."
https://www.classcentral.com/course/introduction-clinical-data-science-12837,"This course will prepare you to complete all parts of the Clinical Data Science Specialization. In this course you will learn how clinical data are generated, the format of these data, and the ethical and legal restrictions on these data. You will also learn enough SQL and R programming skills to be able to complete the entire Specialization - even if you are a beginner programmer. While you are taking this course you will have access to an actual clinical data set and a free, online computational environment for data science hosted by our Industry Partner Google Cloud. 

At the end of this course you will be prepared to embark on your clinical data science education journey, learning how to take data created by the healthcare system and improve the health of tomorrow's patients.
      


          Welcome to the Clinical Data Science Specialization
    -Learn what clinical data science is all about and get access to the free technology environment hosted by Google Cloud!

Introduction: Clinical Data
    -Clinical data are complex. Walk through the four-W's of clinical data to understand where they come from and what they look like. 

Tools: SQL
    -Develop basic skills in SQL (Structured Query Language) and query the real clinical data set used in the Clinical Data Science Specialization.

Tools: R and the Tidyverse
    -Learn how to use the tidyverse to implement your Clinical Data Science Workflow in R."
https://www.classcentral.com/course/introduction-to-dplyr-17063,"In this 2-hour long project-based course, you will learn one of the most powerful data analysis tools of the experts: the DPLYR package.  By learning the six main verbs of the package (filter, select, group by, summarize, mutate, and arrange), you will have the knowledge and tools to complete your next data analysis project or data transformation.  

By the end of this project, you will be able to:
Use the six main dplyr verbs
Understand the dplyr package and its capabilities 
Get hands-on practice using R and dplyr functions 

This course runs on Coursera's hands-on project platform called Rhyme. On Rhyme, you do projects in a hands-on manner in your browser. You will get instant access to pre-configured cloud desktops containing all of the software and data you need for the project. Everything is already set up directly in your internet browser so you can just focus on learning. For this project, this means instant access to a cloud desktop with R and the appropriate packages installed.  

Notes:
- You will be able to access the cloud desktop 5 times. However, you will be able to access instructions videos as many times as you want.
- This course works best for learners who are based in the North America region. We’re currently working on providing the same experience in other regions.
      


          Intro to dplyr
    -Welcome to the first module! In this module, you will be taken to Rhyme where a Virtual Machine with R, R Studio and dplyr awaits.  Once there you will begin the Project where you will be introduced to the Rhyme Interface and subsequently learn the dplyr verbs through hands on exercises.   Come in and get experience using R and the dplyr functions."
https://www.classcentral.com/course/python-visualization-9552,"This if the final course in the specialization which builds upon the knowledge learned in Python Programming Essentials, Python Data Representations, and Python Data Analysis.  We will learn how to install external packages for use within Python, acquire data from sources on the Web, and then we will clean, process, analyze, and visualize that data. This course will combine the skills learned throughout the specialization to enable you to write interesting, practical, and useful programs.

By the end of the course, you will be comfortable installing Python packages, analyzing existing data, and generating visualizations of that data.  This course will complete your education as a scripter, enabling you to locate, install, and use Python packages written by others. You will be able to effectively utilize tools and packages that are widely available to amplify your effectiveness and write useful programs.
      


          Week 1 
    -This module will discuss the importance of using and writing documentation. The Python documentation is a valuable resource for learning about language features you haven't seen yet.

Week 2
    -This module will teach you about packages and modules in Python, including how to install packages and how to create your own modules. You will also learn to use the Pygal plotting library.

Week 3
    -This module will teach you about Python sets. Sets are used to hold unordered collections of data without duplicates. We will also discuss efficiency.

Week 4
    -The final project of the specialization will enable you to demonstrate mastery of the concepts you have learned up to this point. You will also be able to understand and compare different approaches to reconciling two data sets."
https://www.classcentral.com/course/statistics-project-6100,"The capstone project will be an analysis using R that answers a specific scientific/business question provided by the course team. A large and complex dataset will be provided to learners and the analysis will require the application of a variety of methods and techniques introduced in the previous courses, including exploratory data analysis through data visualization and numerical summaries, statistical inference, and modeling as well as interpretations of these results in the context of the data and the research question. The analysis will implement both frequentist and Bayesian techniques and discuss in context of the data how these two approaches are similar and different, and what these differences mean for conclusions that can be drawn from the data.

A sampling of the final projects will be featured on the Duke Statistical Science department website.

Note: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone.
      


          About the Capstone Project
    -Welcome to the capstone project! This week's content is an introduction to the project assignment and goals. The readings in this week will introduce the data set that you will be analyzing for your project and the specific questions you will answer using data analysis techniques we learned in the previous courses. It is important to understand what we will be doing in the course before jumping into the detailed analysis. So we encourage you to start with the first lecture to get the big picture, and then delve into the specifics of the analysis. Enjoy, and good luck! Remember, if you have questions, you can post them on the discussion forums.

Exploratory Data Analysis (EDA)
    -This week you will work on conducting an exploratory analysis of the housing data. Exploratory analysis is an essential first step for familiarizing yourself with and understanding the data. 

In this week, you will complete a quiz which will guide you through certain important aspects of the data. The insights you gain through this assignment will help inform modeling in the future quizzes and peer assessments. 

Feel free to post questions about this assignment on the discussion forum. 

EDA and Basic Model Selection - Submission
    -This week we will dig deeper into our exploratory data analysis of the data. We now have all the information and data necessary to perform a deep dive into the EDA and it is time start your initial analysis report! We encourage you to start your analysis report (presented in peer-review format next week) early so you will have enough time to complete it. You will conduct exploratory data analysis, model selection, and model evaluation, and then complete a written report which answers several questions which will guide you through the process. This report will be your first peer-review assignment in this course. 

EDA and Basic Model Selection - Evaluation
    -Great work so far! We hope you will also learn as much from evaluating your peers' work as completing your own assignment. Happy learning!

Model Selection and Diagnostics
    -We are half way through the course! In this week, you will continue model selection and model diagnostics, which will serve a starting point for your final project. You will be assessed on your work through a quiz. If you have any questions so far, don't hesitate to post on the forum so that others can help and discuss the question together.

Out of Sample Prediction
    -In this week, you will gain experience using your model to perform out-of-sample prediction and validation.  The skills honed this week will guide you through your final analysis in the weeks to come.  Please feel free to go back to prior weeks and review the necessary background knowledge. 

Final Data Analysis - Submission
    -In the next two weeks, you will complete your final data analysis project. You will submit your answers using the Final Data Analysis peer review assignment link in Week 8.

Final Data Analysis - Evaluation
    -Congratulations on making through to the final week of the course! In this week, we will finish this data analysis project by completing the evaluation of three of your peers' assignments."
https://www.classcentral.com/course/python-for-data-science-11228,"Data science — одна из самых горячих областей на сегодняшний день, а Python — один из самых популярных инструментов для анализа данных. В этом курсе вы узнаете, как применять свои навыки программирования для построения предиктивных моделей, визуализации данных и работы с нейросетями.
Курс ориентирован на практику и позволит вам сразу приступить к работе с данными и построению моделей.
      


          Математика и Python для анализа данных

Визуализация данных и статистика

Обучение с учителем

Методы обучения без учителя

Нейронные сети

Курсовой проект"
https://www.classcentral.com/course/datascimed-10645,"An increasing volume of data is becoming available in biomedicine and healthcare, from genomic data, to electronic patient records and data collected by wearable devices. Recent advances in data science are transforming the life sciences, leading to precision medicine and stratified healthcare. 

In this course, you will learn about some of the different types of data and computational methods involved in stratified healthcare and precision medicine.  You will have a hands-on experience of working with such data.  And you will learn from leaders in the field about successful case studies. 

Topics include: (i) Sequence Processing, (ii) Image Analysis, (iii) Network Modelling, (iv) Probabilistic Modelling, (v) Machine Learning, (vi) Natural Language Processing, (vii) Process Modelling and (viii) Graph Data.

Watch the course promo video here: http://edin.ac/2pn350P
      


          Welcome to the Course
    -Join us this week to find out how the course works and to try your hand at programming in Python!

WELCOME TO WEEK 2
    -This week you will be introduced to Sequence Processing and Medical Image Analysis. Explore the course materials to find out about recent advances in these areas and how they contribute to Precision Medicine!

WELCOME TO WEEK 3
    -This week you will learn about Probabilistic and Network Modelling, and how they are applied to biomedicine. You will also be introduced to Machine Learning and explore the opportunities it brings to the medical field.

WELCOME TO WEEK 4
    -This week you will discover how clinical notes and other free-form text can be analysed with the use of Natural Language Processing techniques. You will also find out how Process Modelling can help us understand, stratify and improve healthcare processes.

WELCOME TO WEEK 5
    -In this final week of the course you will learn how the Graph Data model allows for effective linkage of different data in the life sciences. You will also explore societal, legal and ethical implications of precision medicine and stratified healthcare."
https://www.classcentral.com/course/edx-programming-in-r-for-data-science-6038,"This course is part of the Microsoft Professional Program Certificate in Data Science.
In this computer science course from Microsoft, developed in collaboration with the Technical University of Denmark (DTU), get the knowledge and skills you need to use R, the statistical programming language for data scientists, in the field of your choice.
In this course you will learn all you need to get up to speed with programming in R. Explore R data structures and syntaxes, see how to read and write data from a local file to a cloud-hosted database, work with data, get summaries, and transform them to fit your needs. Plus, find out how to perform predictive analytics using R and how to create visualizations using the popular ggplot2 package.



Section 1: IntroductionSection 2: FunctionsSection 3: Control flow and LoopsSection 4: Working with Vectors and MatricesSection 5: Reading in DataSection 6: Writing DataSection 7: Reading from SQL ServerSection 8: Working with DataSection 9: Manipulating DataSection 10: SimulationSection 11: Linear modelSection 12: Graphics in R"
https://www.classcentral.com/course/data-science-for-business-innovation-14369,"The course is a compendium of the must-have expertise in data science for executive and middle-management to foster data-driven innovation. It consists of introductory lectures spanning big data, machine learning, data valorization and communication. Topics cover the essential concepts and intuitions on data needs, data analysis, machine learning methods, respective pros and cons, and practical applicability issues.	

The course covers terminology and concepts, tools and methods, use cases and success stories of data science applications. 
The course explains what is Data Science and why it is so hyped. It discusses the value that Data Science can create, the main classes of problems that Data Science can solve, the difference is between descriptive, predictive and prescriptive analytics, and the roles of machine learning and artificial intelligence.

From a more technical perspective, the course covers supervised, unsupervised and semi-supervised methods, and explains what can be obtained with classification, clustering, and regression techniques. It discusses the role of NoSQL data models and technologies, and the role and impact of scalable cloud-based computation platforms.
All topics are covered with example-based lectures, discussing use cases, success stories and realistic examples.
      


          Introduction to Data-driven Business 
    -This module introduces the course and offers some basic overview of the topics. It presents the crucial concepts related to data science and big data and provides an outlook on how to use them in real world settings for increasing business value.

Terminology and Foundational Concepts
    -In this module, you will learn the foundational concepts of machine learning and data science. You will understand how these techniques can be useful in terms of increased business value for organizations, thanks to the discussion of a very well known success story, namely Netflix, which can be deemed as a completely data-driven business. You will also understand how machine learning is different from programming.

Data Science Methods for Business
    -In this module, you will learn the concepts and intuitions about the basic approaches for data analysis, including linear regression, naive Bayes, decision trees, clustering, and logistic regression. All the methods are presented starting from typical business uses and are covered in an intuitive way through a guided explanation of how the approach works on simple examples.

Challenges and Conclusions
    -This module summarizes the concepts learned so far and introduces a set of challenges and risks that data-savvy managers must take into account when deciding for a data-driven strategy."
https://www.classcentral.com/course/edx-programming-for-data-science-8162,"There is a rising demand for people with the skills to work with Big Data sets and this course can start you on your journey through our Big Data MicroMasters program towards a recognised credential in this highly competitive area.
Using practical activities and our innovative ProcessingJS Workspace application you will learn how digital technologies work and will develop your coding skills through engaging and collaborative assignments.
You will learn algorithm design as well as fundamental programming concepts such as data selection, iteration and functional decomposition, data abstraction and organisation. In addition to this you will learn how to perform simple data visualisations using ProcessingJS and embed your learning using problem-based assignments.
This course will test your knowledge and skills in solving small-scale data science problems working with real-world datasets and develop your understanding of big data in the world around you.



Section 1: Creative code - Computational thinking
Understanding what you can do with ProcessingJS and apply the basics to start coding with colour; Learn how to qualify and express how algorithms work. 
Section 2: Building blocks - Breaking it down and building it up
Understand how data can be represented and used as variables and learn to manipulate shape attributes and work with weights and shapes using code. 
Section 3: Repetition - Creating and recognising patterns
Explain how and why using repetiton can aid in creating code and begin using repetition to manipulate and visualise data. 
Section 4: Choice - Which path to follow
How to create simple and complicated choices and how to create and use decision points in code. 
Section 5: Repetition - Going further
Discussing advantages of repetition for data visualisation and applying and reflecting on the power of repetitions in code. Creating curves, shapes and scale data in code. 
Section 6: Testing and Debugging
Understanding why and how to comprehensively test your code and debug code examples using line tracing techniques. 
Section 7: Arranging our data
Exploring how and why arrays are used to represent data and how static and dynamic arrays can be used to represent data. 
Section 8: Functions - Reusable code
Understand how functions work in ProcessingJS and demonstate how to deconstruct a problem into useable functions. 
Section 9: Data Science in practice
Exploring how data science is used to solve programming problems and how to solve big data problems by applying skills and knowledge learned throughout the course. 
Section 10: Where next?
Understand the context of big data in programming and transform a problem description into a complete working solution using the skills and knowledge you've learned throughout the course, and explore how you can expand the skills learned in this course by participating in future courses."
https://www.classcentral.com/course/discrete-math-and-analyzing-social-graphs-17336,"The main goal of this course is to introduce topics in Discrete Mathematics relevant to Data Analysis.

We will start with a brief introduction to combinatorics, the branch of mathematics that studies how to count. Basics of this topic are critical for anyone working in Data Analysis or Computer Science. We will illustrate new knowledge, for example, by counting the number of features in data or by estimating the time required for a Python program to run.

Next, we will apply our knowledge in combinatorics to study basic Probability Theory. Probability is everywhere in Data Analysis and we will study it in much more details later. Our goals for probability section in this course will be to give initial flavor of this field.

Finally, we will study the combinatorial structure that is the most relevant for Data Analysis, namely graphs. Graphs can be found everywhere around us and we will provide you with numerous examples. We will mainly concentrate in this course on the graphs of social networks. We will provide you with relevant notions from the graph theory, illustrate them on the graphs of social networks and will study their basic properties. In the end of the course we will have a project related to social network graphs.

As prerequisites we assume only basic math (e.g., we expect you to know what is a square or how to add fractions), basic programming in Python (functions, loops, recursion), common sense and curiosity. Our intended audience are all people that work or plan to work in Data Analysis, starting from motivated high school students.
      


            Read more
          



          Basic Combinatorics
    -Suppose we need to count certain objects. Can we do anything better than just list all the objects? Do we need to create a list of all our data entries to check whether we have enough data to teach our ML model? Is there a way to tell whether our algorithm will run in a reasonable time before implementing and actually running it? All these questions are addressed by a mathematical field called Combinatorics. In this module we will give an introduction to this field that will help us to answer basic versions of the above questions.

Advanced Combinatorics
    -In the first week we have already considered most of the standard settings in Combinatorics, that allow us to address many counting problems. However, successful application of this knowledge on practice requires considerable experience in this kind of problems. The goal of this module is twofold. First, we study extensively more advanced combinatorial settings. We discuss in more details binomial coefficients. Also, we address one more standard setting, combinations with repetitions. The second gaol of the course is to practice counting. We will gain some experience in this by discussing various problems in Combinatorics.

Discrete Probability
    -Probability theory is a mathematical foundation of Statistics, the core of Data Science. During this week we study discrete probability, the first chapter of the probability theory, closely related to combinatorics. We discuss random experiments, their outcomes and events, introduce the notion of probability and some basic rules that follow immediately from the combinatorial results studied before. We also study simple probabilistic models like coin-tossing that will be used later.

Introduction to Graphs
    -Graphs represent objects and relations between them in a compact geometric form. Objects are represented by vertices of a graph and relations correspond to edges. Applications of graphs include geoinformational systems (vertices are cities, edges are roads), social network analysis (people and friendship relations), chemistry (graphs of molecular structure), computer network topology, and many more. During this week, we introduce basic notions of graph theory and discuss basic algorithms on graphs.

Basic Graph Parameters
    -Graph parameters, also called graph properties and graph invariants, are values (usually numerical), which are calculated for a given graph and depend only on its abstract structure (not, say, on a particular way of drawing the graph on a plane). Graph parameters are useful in data science, since they reduce a big amount of data (the graph) to a small one (the parameter), while conveying important information about the graph. We discuss some of the basic graph parameters in this module.

Graphs of Social Networks
    -In this final part of the course we discuss a Python library for working with graphs, called NetworkX. In NetworkX, one can create and modify graphs, compute graph parameters, visualize graphs, etc. We shall show how NetworkX is used to operate on graphs coming from a real-world dataset."
https://www.classcentral.com/course/mutations-3557,"In previous courses in the Specialization, we have discussed how to sequence and compare genomes. This course will cover advanced topics in finding mutations lurking within DNA and proteins.

In the first half of the course, we would like to ask how an individual's genome differs from the ""reference genome"" of the species. Our goal is to take small fragments of DNA from the individual and ""map"" them to the reference genome.  We will see that the combinatorial pattern matching algorithms solving this problem are elegant and extremely efficient, requiring a surprisingly small amount of runtime and memory.

In the second half of the course, we will learn how to identify the function of a protein even if it has been bombarded by so many mutations compared to similar proteins with known functions that it has become barely recognizable.  This is the case, for example, in HIV studies, since the virus often mutates so quickly that researchers can struggle to study it.  The approach we will use is based on a powerful machine learning tool called a hidden Markov model.

Finally, you will learn how to apply popular bioinformatics software tools applying hidden Markov models to compare a protein against a related family of proteins.
      


          Week 1: Introduction to Read Mapping
    -Welcome to our class! We are glad that you decided to join us.In this class, we will consider the following two central biological questions (the computational approaches needed to solve them are shown in parentheses):How Do We Locate Disease-Causing Mutations? (Combinatorial Pattern Matching)Why Have Biologists Still Not Developed an HIV Vaccine? (Hidden Markov Models)As in previous courses, each of these two chapters is accompanied by a Bioinformatics Cartoon created by talented artist Randall Christopher and serving as a chapter header in the Specialization's bestselling print companion. You can find the first chapter's cartoon at the bottom of this message. 

Week 2: The Burrows-Wheeler Transform
    -Welcome to week 2 of the class!

This week, we will introduce a paradigm called the Burrows-Wheeler transform; after seeing how it can be used in string compression, we will demonstrate that it is also the foundation of modern read-mapping algorithms.

Week 3: Speeding Up Burrows-Wheeler Read Mapping
    -Welcome to week 3 of class!

Last week, we saw how the Burrows-Wheeler transform could be applied to multiple pattern matching.  This week, we will speed up our algorithm and generalize it to the case that patterns have errors, which models the biological problem of mapping reads with errors to a reference genome.

Week 4: Introduction to Hidden Markov Models 
    -Welcome to week 4 of class!

This week, we will start examining the case of aligning sequences with many mutations -- such as related genes from different HIV strains -- and see that our problem formulation for sequence alignment is not adequate for highly diverged sequences.

To improve our algorithms, we will introduce a machine-learning paradigm called a hidden Markov model and see how dynamic programming helps us answer questions about these models.

Week 5: Profile HMMs for Sequence Alignment
    -Welcome to week 5 of class!

Last week, we introduced hidden Markov models.  This week, we will see how hidden Markov models can be applied to sequence alignment with a profile HMM.  We will then consider some advanced topics in this area, which are related to advanced methods that we considered in a previous course for clustering.

Week 6: Bioinformatics Application Challenge
    -Welcome to the sixth and final week of class!

This week brings our Application Challenge, in which we apply the HMM sequence alignment algorithms that we have developed."
https://www.classcentral.com/course/ds-10631,"Apache Spark is the de-facto standard for large scale data processing. This is the first course of a series of courses towards the IBM Advanced Data Science Specialization. We strongly believe that is is crucial for success to start learning a scalable data science platform since memory and CPU constraints are to most limiting factors when it comes to building advanced machine learning models.

In this course we teach you the fundamentals of Apache Spark using python and pyspark. We'll introduce Apache Spark in the first two weeks and learn how to apply it to compute basic exploratory and data pre-processing tasks in the last two weeks. Through this exercise you'll also be introduced to the most fundamental statistical measures and data visualization technologies.

This gives you enough knowledge to take over the role of a data engineer in any modern environment. But it gives you also the basis for advancing your career towards data science. 

Please have a look at the full specialization curriculum:
https://www.coursera.org/specializations/advanced-data-science-ibm

If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge.  To find out more about IBM digital badges follow the link ibm.biz/badging.


After completing this course, you will be able to:
•	Describe how basic statistical measures, are used to reveal  patterns within the data 
•	Recognize data characteristics, patterns, trends, deviations or inconsistencies, and potential outliers.
•	Identify useful techniques for working with big data such as dimension reduction and feature selection methods 
•	Use advanced tools and charting libraries to:
      o	improve efficiency of analysis of big-data with partitioning and parallel analysis 
      o	Visualize the data in an number of 2D and 3D formats (Box Plot, Run Chart, Scatter Plot, Pareto Chart, and Multidimensional Scaling)

For successful completion of the course, the following prerequisites are recommended: 
•	Basic programming skills in python
•	Basic math
•	Basic SQL (you can get it easily from https://www.coursera.org/learn/sql-data-science if needed)

In order to complete this course, the following technologies will be used:
(These technologies are introduced in the course as necessary so no previous knowledge is required.)
•	Jupyter notebooks (brought to you by IBM Watson Studio for free)
•	ApacheSpark (brought to you by IBM Watson Studio for free)
•	Python

We've been reported that some of the material in this course is too advanced. So in case you feel the same, please have a look at the following materials first before starting this course, we've been reported that this really helps.

Of course, you can give this course a try first and then in case you need, take the following courses / materials. It's free...

https://cognitiveclass.ai/learn/spark

https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/f8982db1-5e55-46d6-a272-fd11b670be38/view?access_token=533a1925cd1c4c362aabe7b3336b3eae2a99e0dc923ec0775d891c31c5bbbc68

This course takes four weeks, 4-6h per week
      


            Read more
          



          Introduction the course and grading environment

Tools that support BigData solutions

Scaling Math for Statistics on Apache Spark

Data Visualization of Big Data"
https://www.classcentral.com/course/edx-iot-programming-and-big-data-9753,"The Internet of Things is creating massive quantities of data, and managing and analysing it requires a unique approach to programming and statistics for distributed data sources.
This course will teach introductory programming concepts that allow connection to, and implementation of some functionality on, IoT devices, using the Python programming language. In addition, students will learn how to use Python to process text log files, such as those generated automatically by IoT sensors and other network-connected systems.
Learners do not need prior programming experience to undertake this course, and will not learn a specific programming language - however Python will be used for demonstrations. This course will focus on learning by working through realistic examples."
https://www.classcentral.com/course/pkubioinfo-1209,"A big welcome to “Bioinformatics: Introduction and Methods” from Peking University! In this MOOC you will become familiar with the concepts and computational methods in the exciting interdisciplinary field of bioinformatics and their applications in biology, the knowledge and skills in bioinformatics you acquired will help you in your future study and research.

Course materials are available under the CC BY-NC-SA License.
      


          Introduction and History of Bioinformatics
    -Welcome to “Bioinformatics: Introduction and Methods! Upon completion of this module you will be able to: become familiar with the essential concepts of bioinformatics; explore the history of this young area; experience how rapidly bioinformatics is growing. Our supplementary materials will give you a better understanding of the course lectures through they are not required in quizzes or exams

Sequence Alignment
    -Upon completion of this module, you will be able to: describe dynamic programming based sequence alignment algorithms; differentiate between the Needleman-Wunsch algorithm for global alignment and the Smith-Waterman algorithm for local alignment; examine the principles behind gap penalty and time complexity calculation which is crucial for you to apply current bioinformatic tools in your research; experience the discovery of Smith-Waterman algorithm with Dr. Michael Waterman himself.

Sequence Database Search
    -Upon completion of this module, you will be able to: become familiar with sequence databse search and most common databases; explore the algoritm behind BLAST and the evaluation of BLAST results; ajdust BLAST parameters base on your own research project.

Markov Model
    -Upon completion of this module, you will be able to: recognize state transitions, Markov chain and Markov models; create a hidden Markov model by yourself; make predictuions in a real biological problem with hidden Markov model.

Next Generation Sequencing (NGS): Mapping of Reads From Resequencing and Calling of Genetic Variants
    -Upon completion of this module, you will be able to: describe the features of NGS; associate NGS results you get with the methods for reads mapping and models for variant calling; examine pipelines in NGS data analysis; experience how real NGS data were analyzed using bioinformatic tools. This module is required before entering Module 8.

Functional Prediction of Genetic Variants
    -Upon completion of this module you will able to: describe what is variant prediction and how to carry out variant predictions; associate variant databases with your own research projects after you get a list of variants; recognize different principles behind prediction tools and know how to use tools such as SIFT, Polyphen and SAPRED according to your won scientific problem.

Mid-term Exam
    -The description goes here

Next Generation Sequencing: Transcriptome Analysis, and RNA-Seq
    -Upon completion of this module, you will be able to: describe how transcriptome data were generated; master the algorithm used in transcriptome analysis; explore how the RNA-seq data were analyzed. This module is required before entering Module 9.

Prediction and Analysis of Noncoding RNA
    -Upon completion of this module, you will be able to: Analyze non-coding RNAs from transcriptome data; identify long noncoding RNA (lncRNA) from NGS data and predict their functions.

Ontology and Identification of Molecular Pathways
    -Upon completion of this module, you will be able to: define ontology and gene ontology, explore KEGG pathway databses; examine annotations in Gene Ontology; identify pathways with KOBAS and apply the pipeline to drug addition study.

Bioinformatics Database and Software Resources
    -Upon completion of this module, you will be able to describe the most important bioinformatic resources including databases and software tools; explore both centralized resources such as NCBI, EBI, UCSC genome browser and lots of individual resources; associate all your bioinformatic problems with certain resources to refer to.

Origination of New Genes
    -Upon completion of this case study module, you will be able to: experience how to apply bioinformatic data, methods and analyses to study an important problem in evolutionary biology; examine how to detect and study the origination, evolution and function of species-specific new genes; create phylogenetic trees with your own data (not required) with Dr. Manyuan Long, a world-renowned pioneer and expert on new genes  from University of Chicago.

Evolution function analysis of DNA methyltransferase
    -Upon completion of this case study module, you will be able to: experience how to use bioinformatic methods to study the function and evolution of DNA methylases; share with Dr. Gang Pei, president of Tongji University and member of the Chinese Academy of Science, the experiences in scientific research and thought about MOOC.

Final Exam
    -The description goes here"
https://www.classcentral.com/course/python-data-processing-7751,"This course (The English copy of ""用Python玩转数据"" )  is mainly for non-computer majors. It starts with the basic syntax of Python, to how to acquire data in Python locally and from network, to how to present data, then to how to conduct basic and advanced statistic analysis and visualization of data, and finally to how to design a simple GUI to present and process data, advancing level by level. 

This course, as a whole, based on Finance data and through the establishment of popular cases one after another, enables learners to more vividly feel the simplicity, elegance, and robustness of Python. Also, it discusses the fast, convenient and efficient data processing capacity of Python in humanities and social sciences fields like literature, sociology and journalism and science and engineering fields like mathematics and biology, in addition to business fields. Similarly, it may also be flexibly applied into other fields.

The course has been updated. Updates in the new version are : 

1) the whole course has moved from Python 2.x to Python 3.x 
2) Added manual webpage fetching and parsing. Web API is also added. 
3) Improve the content order and enrich details of some content especially for some practice projects.

Note: videos are in Chinese (Simplified) with English subtitles. All other materials are in English.
      


          Welcome to learn Data Processing Using Python!
    -Hi, guys, welcome to learn “Data Processing Using Python”(The English version of ""用Python玩转数据"", url is https://www.coursera.org/learn/hipython/home/welcome)!In this course, I tell in a manner that enables non-computer majors to understand how to utilize this simple and easy programming language – Python to rapidly acquire, express, analyze and present data based on SciPy, Requests, Beautiful Soup libraries etc. Many cases are provided to enable you to easily and happily learn how to use Python to process data in many fields. 【Nov 18, 2019 @ @ @ @ @ @ @ Hi, all! The content is planned to be updated in the last two months. This update is relatively large, including practical operation and explanation of Python based cases, vector operation and broadcast ideas of numpy package and common applications, multiple links of data exploration and preprocessing (including in module 4), data analysis and data mining cases based on pandas, some of which are directly modified on the original video Some of them are presented in the form of expanded videos, especially the newly recorded videos, which have a lot of content to say, take a long time, and will be a little hard to learn. I have completed all the updates on Dec 16, 2019. Thank you for your patience. I hope you will enjoy the new version.】

Basics of Python
    -Hi, guys, welcome to learn Module 01 “Basics of Python”! I’ll first guide you to have a glimpse of its simplicity for learning as well as elegance and robustness. Less is more: the author of Python must know this idea well. After learning this module, you can master the basic language structures, data types, basic operations, conditions, loops, functions and modules in Python. With them, we can write some useful programs! 

Data Acquisition and Presentation
    -Welcome to learn Module 02 “Data Acquisition and Presentation”! After learning this module, you can master the modes of acquiring local data and network data in Python and use the basic and yet very powerful data structure sequence, string, list and tuple in Python to fast and effectively present data and simply process data. 

Powerful Data Structures and Python Extension Libraries
    -Welcome to learn Module 03 “Powerful Data Structures and Python Extension Libraries”! Have you felt you are closer to using Python to process data? After learning this module, you can master the intermediate-level and advanced uses of Python: data structure dictionaries and sets. In some applications, they can be very convenient. What’s special here is that, you can also feel the charm of such concise and efficient data structures: ndarray, Series and DataFrame in the most famous and widely applied scientific computing package SciPy in Python. 

Python Data Statistics and Mining
    -Welcome to learn Module 04 “Python data statistics and mining”! In this module, I will show you, over the entire process of data processing, the unique advantages of Python in data processing and analysis, and use many cases familiar to and loved by us to learn about and master methods and characteristics. After learning this module, you can preprocess the data and fast and effectively mine your desired or expected or unknown results from a large amount of data, and can also present those data in various images. In addition, the data statistics modes of all third party packages in Python are extraordinarily and surprisingly strong, but we, as average persons, can still understand and possess them. 

Object Orientation and Graphical User Interface
    -Welcome to Module 05 “Object Orientation and Graphical User Interface”! In this module, I will guide you to understand what object orientation is and the relationship between graphical user interface and object orientation. Learners are only required to understand the concepts so that you can more freely and easily pick up various new functions in future. No program writing is required here. Besides, you also need to master the basic framework of GUI, common components and layout management. After learning them, you will find development with GUI is actually not remote. It has an Easter egg, too ~~~"
https://www.classcentral.com/course/edx-data-science-and-agile-systems-engineering-19101,"Modern systems today must be designed for agility in order to outpace the competition. Concepts like Agile, DevOps, and Data Science were once considered only for the technology-based companies. Today that means every company. Because there is no greater currency than timely information for optimizing operations and meeting the needs of customers.
Modern product management requires that every development and operations value stream is identified and continuously improved. This means using Lean and DevOps principles to streamline handoffs and information flows across teams. It means reorienting towards self-service and automation wherever possible. And to avoid incrementalism, it means a robust Agile development process to keep innovations important and aggressive enough to make noticeable improvements in value delivery.
Agile systems in a DevOps environment requires that products are built completely differently from a traditional designs. Modularity, open set architectures, and flexible data management paradigms are a starting point. The evolutionary nature of the product with so much change enables functionality, design, and technology to drive and influence each other simultaneously. And beneath it all is a data collection and feedback loop essential for anticipating and reacting to business needs both for operations and marketing.
Data science and analytics are the lifeblood of any product organization, and enable product managers to tackle risks early. Luckily, new technologies allow us to collect and integrate data without extreme upfront constraints and onerous controls. This means all data is fair game, and when tagged and stored properly, can be made available at nearly any scale for preparation, visualization, analysis, and modeling.
We’ll teach you the paradigms, processes, and introduce some key technologies that make the data-driven product organization the optimal competitor in the market.



            Read more
          



Module 1: Agile Systems Engineering 
Module 2: DevOps Principles for Business Agility 
Module 3: Data Science for Product Risk Management 
Module 4: Implementing Data-Driven Controls using Technology and Teams"
https://www.classcentral.com/course/udacity-data-analysis-and-visualization-8566,"Data and visual analytics is an emerging field concerned with analyzing, modeling, and visualizing complex high dimensional data. This course will introduce students to the field by covering state­-of-­the-art modeling, analysis and visualization techniques. It will emphasize practical challenges involving complex real world data and include several case studies and hands-on work with the R programming language.Why Take This Course?You should take this course if you want to cover the state of the art in data modeling and visualization techniques using the R programming language.



Programming in RThe R Programming LanguageR Programming SyntaxR Programming and Data StructuresData AnalysisData PreprocessingData ProcessingData VisualizationRegressionLogistic RegressionLinear RegressionRegularization"
https://www.classcentral.com/course/edx-dynamic-programming-applications-in-machine-learning-and-genomics-10249,"If you look at two genes that serve the same purpose in two different species, how can you rigorously compare these genes in order to see how they have evolved away from each other?
In the first part of the course, part of the Algorithms and Data Structures MicroMasters program, we will see how the dynamic programming paradigm can be used to solve a variety of different questions related to pairwise and multiple string comparison in order to discover evolutionary histories.
In the second part of the course, we will see how a powerful machine learning approach, using a Hidden Markov Model, can dig deeper and find relationships between less obviously related sequences, such as areas of the rapidly mutating HIV genome.



Week 1: Pairwise Sequence Alignment
A review of dynamic programming, and applying it to basic string comparison algorithms.
Week 2: Advanced Sequence Alignment
Learn how to generalize your dynamic programming algorithm to handle a number of different cases, including the alignment of multiple strings.
Week 3: Introduction to Hidden Markov Models
Learn what a Hidden Markov model is and how to find the most likely sequence of events given a collection of outcomes and limited information.
Week 4: Machine Learning in Sequence Alignment
Formulate sequence alignment using a Hidden Markov model, and then generalize this model in order to obtain even more accurate alignments."
https://www.classcentral.com/course/programming-for-data-science-nanodegree-with-R--n-18204,"In this program, you’ll learn the most valuable programming tools and languages used by data scientists today. You’ll learn how to manipulate large datasets, perform version control, and access modern databases. You’ll be programming with R, one of the most popular programming languages used by Data Scientists today. This program is an ideal way to launch a career in data, and for experienced analysts, it’s an excellent opportunity to augment your existing skill set with in-demand programming skills.
Prepare for a data science career by learning the fundamental data programming tools: R, SQL, command line, and git.
      


           Prerequisite Knowledge There are no prerequisites for this program, aside from basic computer skills.See detailed requirements.Introduction to SQLLearn SQL fundamentals such as JOINs, Aggregations, and Subqueries. Learn how to use SQL to answer complex business problems. Investigate a DatabaseIntroduction to R ProgrammingLearn R programming fundamentals such as data structures, variables, loops, and functions. Learn to visualize data in the popular data visualization library ggplot2.Explore US Bikeshare DataIntroduction to Version ControlLearn how to use version control and share your work with other people in the data science industry.Post your work on Github"
https://www.classcentral.com/course/programming-for-data-science-nanodegree--nd104-18224,"In this program, you’ll learn the most valuable programming tools and languages used by data scientists today. You’ll learn how to manipulate large datasets, perform version control, and access modern databases. You’ll be programming with Python, one of the most popular programming languages used by Data Scientists today. This program is an ideal way to launch a career in data, and for experienced analysts, it’s an excellent opportunity to augment your existing skill set with in-demand programming skills.
Learn the fundamental programming tools for data professionals: Python, SQL, the Terminal and Git.
      


           Prerequisite Knowledge There are no prerequisites for this program, aside from basic computer skills.See detailed requirements.Introduction to SQLLearn SQL fundamentals such as JOINs, Aggregations, and Subqueries. Learn how to use SQL to answer complex business problems. Investigate a DatabaseIntroduction to Python ProgrammingLearn Python programming fundamentals such as data structures, variables, loops, and functions. Learn to work with data using libraries like NumPy and Pandas.Explore US Bikeshare DataIntroduction to Version ControlLearn how to use version control and share your work with other people in the data science industry.Post your work on Github"
https://www.classcentral.com/course/independent-statistical-computing-with-r-a-gentle-introduction-4545,"R is an open source software environment for statistical computing that is rapidly becoming the tool of choice for data analysis in the life sciences and elsewhere. It is developed by a large international community of scientists and programmers and is at the forefront of new developments in statistical computing. Additionally, R is the foundation of Bioconductor, a similar open-source project focussed on the development of bioinformatics analysis tools. Bioconductor rose to prominence when it became the standard environment for the analysis of microarray gene expression data, but it has maintained and extended this position with the advent of new technologies and the integration with different types of ‘omics data. As such, understanding its basic functionality is of benefit to undergraduates, graduates and researchers across diverse fields.
This short course provides a gentle introduction to the R software and programming environment. It should take you approximately 6-8 hours in total to work through the material. There are five sections: Introduction and basics, Variables and data types, Inbuilt functions, Data frames, Plotting. Materials are taught through pdf documents and videos, with quizzes and assignments provided to test your knowledge. Upon completion of the course you will understand how to manipulate data within R, perform basic data analysis procedures and create plots. This course provides a foundation for more advanced topics and techniques.
 



            Read more"
https://www.classcentral.com/course/big-data-r-hadoop-8056,"You will experience how to use RHadoop tool to manage and analyse big data.
This course will give you access to a virtual environment with installations of Hadoop, R and Rstudio to get hands-on experience with big data management. Several unique examples from statistical learning and related R code for map-reduce operations will be available for testing and learning.
Those with basic knowledge in statistical learning and R will better understand the methods behind and how to run them in parallel using map-reduce functions and Hadoop data storage. At the end of the course you will get access to RHadoop on a supercomputer at University of Ljubljana.
This course is designed for people interested in data science, computational statistics and machine learning and have basic experiences with them. It will be also useful for advanced undergraduate students and first year PhD students in data analysis, statistics or bioinformatics, who wish to understand how to manage big data with Hadoop using R programming language.
We expect that the learners will also have basic experiences with linux and bash and working experiences with R and matrix operations. They should be also capable to download and run virtual machine.
All software needed to actively participate the course is provided within the virtual machine that the followers are supposed to download and run on the local machine. No extra software is needed.
You will need a modest local machine with 15GB free disk space and 2GB RAM.



            Read more"
https://www.classcentral.com/course/edx-image-processing-and-analysis-for-life-scientists-12066,"Nowadays, image-based methods are indispensable for life scientists. Light microscopy especially, has evolved from sketched out observations by eye, to high throughput multi-plane, multi-channel, multi-position and multimode acquisitions that easily produce thousands of information-rich images that must be quantified somehow to answer biological questions. 
This course will teach you core concepts from image acquisition to image filtering and segmentation, to help you tackle simple image analysis workflows on your own. All examples use open source solutions, in order to allow you to be independent from commercial solutions. Emphasis is made on good practices and typical pitfalls in image analysis. At the end of this course, you will be able to adapt and reuse workflows to suit your specific needs and be equipped with the tools and knowledge to adapt and seek advice from the ever-growing image analyst community of which you will be a part now 
The course is taught by senior image analysts with longtime work experience in a service-oriented core facility.



Week 1: Digital Images
Introduction to digital image formation and how optical systems go from objects to images. 
Week 2: Colors
Review of human visual perception and the RGB color model. Introduction to the concepts of image bit-depth and lookup tables. 
Week3: Operating on Images
Introduction to image scaling, interpolation, and mathematical operations of images, and why certain bit-depths are more suitable than others. 
Week4: Filtering
Using image filtering to enhance or suppress features in an image for easing subsequent analysis. We cover linear, nonlinear and Fourier filtering with emphasis on examples. 
Week 5: Image Segmentation
Introduction to image segmentation and overview of available methods (thresholding, clustering, machine learning) and morphological operations. 
Week 6: Regions of Interest
Going from analyzed objects to regions of interest and results tables. Emphasis is made on how to best obtain unbiased measurements and produce a reusable image analysis workflows 
Week 7: Colors, and dimensionality reduction
Introduction to color models, and color deconvolution. Overview of the concept of dimensionality reduction through image projections and reslicing and application to measuring moving objects. 
Extra Week: ImageJ Macro Programming Prime
Presentation of basic programming principles applied to the ImageJ Macro Language. Crash course on variables arrays, loops, conditionals, available macro functions and writing custom functions."
https://www.classcentral.com/course/thaimooc------python-programming-for-data-science-15293,"คำอธิบายรายวิชา
ปัจจุบันความก้าวหน้าทางเทคโนโลยีคอมพิวเตอร์เป็นไปอย่างรวดเร็ว เกิดข้อมูลขนาดใหญ่บนระบบคอมพิวเตอร์ ส่งผลให้การพัฒนางานเพื่อการวิเคราะห์ข้อมูลทางด้านวิทยาการข้อมูลเป็นที่ต้องการอย่างมาก โดยภาษาไพทอนนั้นเป็นภาษาหนึ่งที่มีผู้สนใจต้องการเรียนรู้เพื่อนำไปใช้ประโยชน์ดังกล่าว การพัฒนารายวิชานี้จึงเป็นประโยชน์และเปิดโอกาสผู้ที่สนใจสามารถเข้ามาศึกษาได้
 

การแนะนำเครื่องมือการเขียนโปรแกรมและไลบรารีที่จำเป็น Jupyter Notebook
ทบทวนเทคนิคการเขียนโปรแกรมไพทอนพื้นฐานเช่น lambda, การจัดการไฟล์ .csv
กระบวนการของวิทยาศาสตร์ข้อมูล
การใช้ไลบรารี Pandas and Dataframe สำหรับการจัดการข้อมูลและเทคนิคการทำความสะอาดข้อมูลด้านวิทยาการข้อมูล
การใช้ไลบรารี Numpy เพื่อการคำนวณทางวิทยาศาสตร์ และ การใช้ไลบรารี Mathplotlib เพื่อการแสดงผลข้อมูล"
https://www.classcentral.com/course/thaimooc---r------r-programming-for-data-science-15109,"คำอธิบายรายวิชา การเขียนโปรแกรม R เบื้องต้น การสำรวจข้อมูล และการมองภาพข้อมูล การวิเคราะห์ความสัมพันธ์จากข้อมูลขนาดใหญ่ การจำแนกกลุ่มข้อมูล การพยากรณ์ การวิเคราะห์แบ่งกลุ่ม จำนวนชั่วโมงเรียนรู้ จำนวนชั่วโมงเรียนรู้ทั้งหมด 10 ชั่วโมงเรียนรู้ (จำนวนชั่วโมงสื่อวีดิทัศน์ 3 ชั่วโมง 30 นาที) วัตถุประสงค์การเรียนรู้ 1. ผู้เรียนสามารถอธิบายการใช้โปรแกรม R เบื้องต้น 2. ผู้เรียนสามารถอธิบายการใช้โปรแกรม R ในการจัดการข้อมูล สำรวจข้อมูล การมองภาพข้อมูล 3. ผู้เรียนสามารถอธิบายการใช้โปรแกรม R ในการวิเคราะห์ข้อมูลเบื้องต้น 4. ผู้เรียนสามารถอธิบายการใช้โปรแกรม R ในการวิเคราะห์ความสัมพันธ์จากข้อมูลขนาดใหญ่ การจำแนกกลุ่มข้อมูล การพยากรณ์ การวิเคราะห์แบ่งกลุ่ม และสรุปผลได้อย่างถูกต้อง คุณสมบัติผู้เรียน นักศึกษาระดับปริญญาตรี นักศึกษาระดับปริญญาโท และผู้ที่มีความสนใจเรียนเรื่องการใช้โปรแกรม R สำหรับวิทยาการข้อมูล เกณฑ์การวัดผล เข้าทำแบบทดสอบก่อนเรียน กิจกรรมในบทเรียน และแบบทดสอบหลังเรียน ผู้เรียนมีคะแนนรวมทั้งหมดไม่ต่ำกว่า 70% ถือว่าผ่านเกณฑ์เพื่อรับประกาศนียบัตรในระบบได้ ทีมผู้รับผิดชอบรายวิชา MOOC อาจารย์ผู้รับผิดชอบรายวิชาหลัก ผู้ช่วยศาสตราจารย์ ดร.พิมผกา ธานินพงศ์ ภาควิชาสถิติ คณะวิทยาศาสตร์ มหาวิทยาลัยเชียงใหม่ อาจารย์ผู้รับผิดชอบรายวิชาร่วม ดร.ภวัต ภักดิศรานุวัต ภาควิชาสถิติ คณะวิทยาศาสตร์ มหาวิทยาลัยเชียงใหม่ อาจารย์ผู้รับผิดชอบรายวิชาร่วม ดร.วิรินท์รดา วงค์รินทร์ ภาควิชาสถิติ คณะวิทยาศาสตร์ มหาวิทยาลัยเชียงใหม่ อาจารย์ผู้รับผิดชอบรายวิชาร่วม ดร.พิมพ์วรัชญ์ นันทพฤทธ์ ภาควิชาสถิติ คณะวิทยาศาสตร์ มหาวิทยาลัยเชียงใหม่ อาจารย์ผู้รับผิดชอบรายวิชาร่วม อาจารย์ อัญมณี กุมมาระกะ ภาควิชาสถิติ คณะวิทยาศาสตร์ มหาวิทยาลัยเชียงใหม่ ช่องทางการติดต่อทีมผู้สอน E-MAIL e-mail: pawat.pak@cmu.ac.th Creative commons สัญญาอนุญาตสิทธิ์ “สื่อการสอนนี้เป็นส่วนหนึ่งของโครงการ Thai MOOC (thaimooc.org) และเผยแพร่ภายใต้สัญญาอนุญาตสิทธิ์แบบ Creative Commons ด้วยเงื่อนไข CC BY NC SA”
      


            Read more"
https://www.classcentral.com/course/edx-statistical-analysis-in-bioinformatics-8163,"Improvements in modern biology have led to a rapid increase in sensitivity and measurability in experiments and have reached the point where it is often impossible for a scientist alone to sort through the large volume of data that is collected from just one experiment.
For example, individual data points collected from one gene expression study can easily number in the hundreds of thousands. These types of data sets are often referred to as ‘biological big data’ and require bioinformaticians to use statistical tools to gain meaningful information from them.
In this course, part of the Bioinformatics MicroMasters program, you will learn about the R language and environment and how to use it to perform statistical analyses on biological big datasets. 
This course is part of the Bioinformatics MicroMaster’s program from UMGC. Upon completion of the program and receipt of the verified MicroMaster’s certificate, learners may then transition into the full UMGC Master’s Program in Biotechnology with a specialization in Bioinformatics without any application process or testing. See the MicroMasters program page for more."
https://www.classcentral.com/course/serverless-machine-learning-gcp-8696,"This one-week accelerated on-demand course provides participants a a hands-on introduction to designing and building machine learning models on Google Cloud Platform. Through a combination of presentations, demos, and hand-on labs, participants will learn machine learning (ML) and TensorFlow concepts, and develop hands-on skills in developing, evaluating, and productionizing ML models.

OBJECTIVES

This course teaches participants the following skills:

  ● Identify use cases for machine learning

  ● Build an ML model using TensorFlow

  ● Build scalable, deployable ML models using Cloud ML

  ● Know the importance of preprocessing and combining features

  ● Incorporate advanced ML concepts into their models

  ● Productionize trained ML models


PREREQUISITES

To get the most of out of this course, participants should have:

  ● Completed Google Cloud Fundamentals- Big Data and Machine Learning course OR have equivalent experience

  ● Basic proficiency with common query language such as SQL

  ● Experience with data modeling, extract, transform, load activities

  ● Developing applications using a common programming language such Python

  ● Familiarity with Machine Learning and/or statistics

Google Account Notes:
• Google services are currently unavailable in China.
      


          Welcome to Serverless Machine Learning on Google Cloud Platform

Module 1: Getting Started with Machine Learning

Module 2: Building ML models with Tensorflow

Module 3: Scaling ML models with Cloud ML Engine

Module 4: Feature Engineering"
https://www.classcentral.com/course/harvardx-data-science-18421,"The demand for skilled data science practitioners in industry, academia, and government is rapidly growing. The HarvardX Data Science program prepares you with the necessary knowledge base and useful skills to tackle real-world data analysis challenges. The program covers concepts such as probability, inference, regression, and machine learning and helps you develop an essential skill set that includes R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with Unix/Linux, version control with git and GitHub, and reproducible document preparation with RStudio.
In each course, we use motivating case studies, ask specific questions, and learn by answering these through data analysis. Case studies include: Trends in World Health and Economics, US Crime Rates, The Financial Crisis of 2007-2008, Election Forecasting, Building a Baseball Team (inspired by Moneyball), and Movie Recommendation Systems.
Throughout the program, we will be using the R software environment. You will learn R, statistical concepts, and data analysis techniques simultaneously. We believe that you can better retain R knowledge when you learn how to solve a specific problem.



Courses under this program:Course 1: Data Science: R Basics
Build a foundation in R and learn how to wrangle, analyze, and visualize data.
Course 2: Data Science: Visualization
Learn basic data visualization principles and how to apply them using ggplot2.
Course 3: Data Science: Probability
Learn probability theory -- essential for a data scientist -- using a case study on the financial crisis of 2007-2008.
Course 4: Data Science: Inference and Modeling
Learn inference and modeling, two of the most widely used statistical tools in data analysis.
Course 5: Data Science: Productivity Tools
Keep your projects organized and produce reproducible reports using GitHub, git, Unix/Linux, and RStudio.
Course 6: Data Science: Wrangling
Learn to process and convert raw data into formats needed for analysis.
Course 7: Data Science: Linear Regression
Learn how to use R to implement linear regression, one of the most common statistical modeling approaches in data science.
Course 8: Data Science: Machine Learning
Build a movie recommendation system and learn the science behind one of the most popular and successful data science techniques.
Course 9: Data Science: Capstone
Show what you've learned from the Professional Certificate Program in Data Science."
https://www.classcentral.com/course/edx-analyzing-big-data-with-microsoft-r-7822,"The open-source programming language R has for a long time been popular (particularly in academia) for data processing and statistical analysis. Among R's strengths are that it's a succinct programming language and has an extensive repository of third party libraries for performing all kinds of analyses. Together, these two features make it possible for a data scientist to very quickly go from raw data to summaries, charts, and even full-blown reports. However, one deficiency with R is that traditionally it uses a lot of memory, both because it needs to load a copy of the data in its entirety as a data.frame object, and also because processing the data often involves making further copies (sometimes referred to as copy-on-modify). This is one of the reasons R has been more reluctantly received by industry compared to academia.
The main component of Microsoft R Server (MRS) is the RevoScaleR package, which is an R library that offers a set of functionalities for processing large datasets without having to load them all at once in the memory. RevoScaleR offers a rich set of distributed statistical and machine learning algorithms, which get added to over time. Finally, RevoScaleR also offers a mechanism by which we can take code that we developed on our laptop and deploy it on a remote server such as SQL Server or Spark (where the infrastructure is very different under the hood), with minimal effort.
In this course, we will show you how to use MRS to run an analysis on a large dataset and provide some examples of how to deploy it on a Spark cluster or a SQL Server database. Upon completion, you will know how to use R for big-data problems.
Since RevoScaleR is an R package, we assume that the course participants are familiar with R. A solid understanding of R data structures (vectors, matrices, lists, data frames, environments) is required. Familiarity with 3rd party packages such as dplyr is also helpful.edX offers financial assistance for learners who want to earn Verified Certificates but who may not be able to pay the fee. To apply for financial assistance, enroll in the course, then follow this link to complete an application for assistance.



            Read more
          




Familiarity with R"
https://www.classcentral.com/course/code-free-data-science-13380,"The Code Free Data Science class is designed for learners seeking to gain or expand their knowledge in the area of Data Science.  Participants will receive the basic training in effective predictive analytic approaches accompanying the growing discipline of Data Science without any programming requirements.  Machine Learning methods will be presented by utilizing the KNIME Analytics Platform to discover patterns and relationships in data. Predicting future trends and behaviors allows for proactive, data-driven decisions.  During the class learners will acquire new skills to apply predictive algorithms to real data, evaluate, validate and interpret the results without any pre requisites for any kind of programming.  Participants will gain the essential skills to design, build, verify and test predictive models.  
You Will Learn
•	How to design Data Science workflows without any programming involved
•	Essential Data Science skills to design, build, test and evaluate predictive models
•	Data Manipulation, preparation and Classification and clustering methods
•	Ways to apply Data Science algorithms to real data and evaluate and interpret the results
      


          Welcome to the world of Big Data
    -Welcome to the first module of the Code Free Data Science course. This first module will provide insight into Big Data Hype, its technologies opportunities and challenges.  We will take a deeper look into the Big Data Analytics  and methodology associated with Data Science approaches.

Introduction to KNIME Analytics Platform
    -This module will introduce the KNIME analytics platform.  Learners will be guided to download, install and setup KNIME.  We will explore and become familiar with the KNIME workflow editor and its components.  In this module we will create the very first basic workflow, and explore the kinds of analysis KNIME empowers users to perform.

Data Manipulation and Visualization

Machine Learning"
https://www.classcentral.com/course/r-capstone-7181,"R Programming Capstone
      


          Obtain and Clean the Data
    -The overall goal of the capstone project is to integrate the skills you have developed over the courses in this Specialization and to build a software package that can be used to work with the NOAA Significant Earthquakes dataset.

Building Geoms
    -Show us when earthquakes occurred in different countries, their magnitude, and their toll on human life.

Building a Leaflet Map
    -Show and annotate the earthquake epicenters.

Documentation and Packaging
    -Documentation is one of the most important and most commonly overlooked steps when writing software, but you're not going to let that happen in your project.

Deployment
    -The moment of truth. It's time to push your package to GitHub.

Final Assessment
    -It's time to submit your deployed package for evaluation and to evaluate the work of a few of your classmates."
https://www.classcentral.com/course/edx-data-science-readiness-assessment-7995,"Are you interested in pursuing a degree in Data Science, but unsure whether you have the necessary Math and Programming skills? This assessment will help you identify your current readiness in three core areas required for the study of Data Science; Calculus, Linear Algebra, and Programming.
You can take this assessment at your own pace and receive a private score report that identifies your readiness in each specific area. We will also provide, when necessary, recommendations for additional free online study.
This assessment is free, unproctored, and not offered for credit; it is designed for enrichment and self-assessment for anyone interested in pursuing data science as a career.​"
https://www.classcentral.com/course/swayam-data-analysis-decision-making-iii-13981,"This is the third part of the three part course (DADM-I, DADM-II, DADM-III) which covers ""Operations Research and its tools with applications"". In general Decision Analysis and Decision Making (DADM) covers three main areas which are: Multivariate Statistical Analysis with its applications, Other Decision Making Models like DEA, AHP, ANP, TOPSIS, etc., and Operations Research and its tools with applications. These three part DADM course will be more practical and application oriented rather than theoretical in nature.INTENDED AUDIENCE: Masters in Business Administration, Masters in Economics, Masters in Statistics/Mathematics, Masters in Industrial Engineering, Masters in Operations Research/Operations Management, PhD in related fields as mentioned abovePREREQUISITES: Probability & Statistics Operations ResearchINDUSTRY SUPPORT: Manufacturing industry, chemical industry, steel industry, cement industry, etc.



COURSE LAYOUT Week 1: Introduction, Ideas of Optimization and ModelingWeek 2: Linear Programming (LP) and related topicsWeek 3: Simplex Method, Interior point Method and related conceptsWeek 4: Non-Linear Programming (NLP)Week 5: Goal ProgrammingWeek 6: Stochastic Programming   Week 7: 0-1 Programming and other related methodsWeek 8: Polynomial OptimizationWeek 9: Reliability Based ProgrammingWeek 10: Robust OptimizationWeek 11: Other topics like Parametric programming, etcWeek 12: Multi-objective Programming"
https://www.classcentral.com/course/data-science-environmental-modelling-11806,"Discover how data science can help us understand environmental change
Environmental and climate change impact our lives, but what role does data play in informing us about such changes to our world? On this online course, we examine and explore the use of statistics and data science in better understanding the environment we live in.
You will develop data science skills learning from experts and completing hands-on modelling activities using real world environmental data and the powerful programming language R.
You will also consider how data can help plan the use of renewable energy resources such as wind power.
This course is for people with an interest in environment and/or renewable energy and who wish to gain new skills in data science. It will also be suitable for those with an interest in data science and who wish to learn more about applications in environment and renewable energy. You don’t need to be an expert in R to take this course."
https://www.classcentral.com/course/edx-learning-analytics-fundamentals-9283,"The demand for data science and learning science skills has continued to increase as classrooms, labs, and organizations look to optimize their data and improve learning environments for students and employees. The UTArlingtonX Learning Analytics courses will give you the opportunity to gain invaluable knowledge and expertise in this growing field.
In this introductory course, you will develop a solid understanding of fundamental learning analytics theories and processes, and explore different types of educational data. You will gain experience working with educational data sets and the R programming language, and hear from a diverse set of voices in the field. Finally, you will also consider ethics and privacy issues, as well explore how to work as part of a team in a domain that is becoming increasingly cross-disciplinary.
By grasping these fundamental areas, you will have a better understanding of the field of learning analytics and be able to apply skills to any occupation that utilizes educational data.



Week 1: What are learning analytics?
Week 2: What types of data will you work with?
Week 3: What types of things will you do?
Week 4: How do you work as part of a team or within an organization?"
https://www.classcentral.com/course/edx-big-data-analytics-8158,"Gain essential skills in today’s digital age to store, process and analyse data to inform business decisions.
In this course, part of the Big Data MicroMasters program, you will develop your knowledge of big data analytics and enhance your programming and mathematical skills. You will learn to use essential analytic tools such as Apache Spark and R.
Topics covered in this course include:

cloud-based big data analysis;
predictive analytics, including probabilistic and statistical models;
application of large-scale data analysis;
analysis of problem space and data needs.

By the end of this course, you will be able to approach large-scale data science problems with creativity and initiative.



Section 1: Simple linear regression
Fit a simple linear regression between two variables in R;Interpret output from R;Use models to predict a response variable;Validate the assumptions of the model. 
Section 2: Modelling data
Adapt the simple linear regression model in R to deal with multiple variables;Incorporate continuous and categorical variables in their models;Select the best-fitting model by inspecting the R output. 
Section 3: Many models
Manipulate nested dataframes in R;Use R to apply simultaneous linear models to large data frames by stratifying the data;Interpret the output of learner models. 
Section 4: Classification
Adapt linear models to take into account when the response is a categorical variable;Implement Logistic regression (LR) in R;Implement Generalised linear models (GLMs) in R;Implement Linear discriminant analysis (LDA) in R. 
Section 5: Prediction using models
Implement the principles of building a model to do prediction using classification;Split data into training and test sets, perform cross validation and model evaluation metrics;Use model selection for explaining data with models;Analyse the overfitting and bias-variance trade-off in prediction problems. 
Section 6: Getting bigger
Set up and apply sparklyr;Use logical verbs in R by applying native sparklyr versions of the verbs. 
Section 7: Supervised machine learning with sparklyr
Apply sparklyr to machine learning regression and classification models;Use machine learning models for prediction;Illustrate how distributed computing techniques can be used for “bigger” problems. 
Section 8: Deep learning
Use massive amounts of data to train multi-layer networks for classification;Understand some of the guiding principles behind training deep networks, including the use of autoencoders, dropout, regularization, and early termination;Use sparklyr and H2O to train deep networks. 
Section 9: Deep learning applications and scaling up
Understand some of the ways in which massive amounts of unlabelled data, and partially labelled data, is used to train neural network models;Leverage existing trained networks for targeting new applications;Implement architectures for object classification and object detection and assess their effectiveness. 
Section 10: Bringing it all together
Consolidate your understanding of relationships between the methodologies presented in this course, theirrelative strengths, weaknesses and range of applicability of these methods."
https://www.classcentral.com/course/data-science-foundations-r-18756,"Ask the right questions, manipulate data sets, and create visualizations to communicate results. This Specialization covers foundational data science tools and techniques, including getting, cleaning, and exploring data, programming in R, and conducting reproducible research. Learners who complete this specialization will be prepared to take the Data Science: Statistics and Machine Learning specialization, in which they build a data product using real-world data. The five courses in this specialization are the very same courses that make up the first half of the Data Science Specialization. This specialization is presented for learners who want to start and complete the foundational part of the curriculum first, before moving onto the more advanced topics in Data Science: Statistics and Machine Learning.



          Course 1: The Data Scientist’s Toolbox- In this course you will get an introduction to the main tools and ideas in the data scientist's toolbox. The course gives an overview of the data, questions, and tools that data analysts and data scientists work with. There are two components to this course. The first is a conceptual introduction to the ideas behind turning data into actionable knowledge. The second is a practical introduction to the tools that will be used in the program like version control, markdown, git, GitHub, R, and RStudio.Course 2: R Programming- In this course you will learn how to program in R and how to use R for effective data analysis. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language. The course covers practical issues in statistical computing which includes programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting R code. Topics in statistical data analysis will provide working examples.Course 3: Getting and Cleaning Data- Before you can work with data you have to get some. This course will cover the basic ways that data can be obtained. The course will cover obtaining data from the web, from APIs, from databases and from colleagues in various formats. It will also cover the basics of data cleaning and how to make data “tidy”. Tidy data dramatically speed downstream data analysis tasks. The course will also cover the components of a complete data set including raw data, processing instructions, codebooks, and processed data. The course will cover the basics needed for collecting, cleaning, and sharing data.Course 4: Exploratory Data Analysis- This course covers the essential exploratory techniques for summarizing data. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data.Course 5: Reproducible Research- This course focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner. Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. This course will focus on literate statistical analysis tools which allow one to publish data analyses in a single document that allows others to easily execute the same analysis to obtain the same results."
https://www.classcentral.com/course/swayam-bio-informatics-algorithms-and-applications-12890,"Bioinformatics is an interdisciplinary field of science for analyzing and interpreting vast biological data using computational techniques. In this course, we aim to give a walkthrough of the major aspects of bioinformatics such as the development of databases, computationally derived hypothesis, algorithms, and computer-aided drug design. During the first section of the course, we will focus on DNA and protein sequence databases and analysis, secondary structures and 3D structural analysis. The second section will be devoted to applications such as prediction of protein structure, folding rates, stability upon mutation, and intermolecular interactions. Further, we will cover computer-aided drug design using docking and QSAR studies. This course is designed to nurture skills and knowledge required for aspiring students, young biologists and research scholars to develop algorithms and tools in bioinformatics.INTENDED AUDIENCE: Students, PhD scholars, teachers, industryPRE-REQUISITES: Basic knowledge of Biology and any computer language would be helpfulINDUSTRY SUPPORT: Cognizant, TCS 
      


COURSE LAYOUT Week 1: Introduction, DNA sequence analysis, DNA DatabasesWeek 2:  Protein structure and function, protein sequence databases, sequence alignmentWeek 3: PAM matrix, Global and local alignment, BLAST: features and scoresWeek 4: Multiple sequence alignment, Conservation score, phylogenetic treesWeek 5 : Protein sequence analysis, hydrophobicity profiles, non-redundant datasetsWeek 6: Protein secondary structures, Ramachandran plot, propensity, secondary structure predictionWeek 7: Protein tertiary structure, Protein Data Bank, visualization tools, structural classification, contact mapsWeek 8: Protein structural analysis, protein structure predictionWeek 9: Protein stability, energetic contributions, database, stabilizing residues, stability upon mutationsWeek 10 : Protein folding rates, proteins interactions, binding site residuesWeek 11: Computer aided drug design, docking, screening, QSARWeek 12: Development of algorithms, awk programming, machine learning techniques, applications using WEKA"
https://www.classcentral.com/course/edx-big-data-fundamentals-8156,"Organizations now have access to massive amounts of data and it’s influencing the way they operate. They are realizing in order to be successful they must leverage their data to make effective business decisions.
In this course, part of the Big Data MicroMasters program, you will learn how big data is driving organisational change and the key challenges organizations face when trying to analyse massive data sets.
You will learn fundamental techniques, such as data mining and stream processing. You will also learn how to design and implement PageRank algorithms using MapReduce, a programming paradigm that allows for massive scalability across hundreds or thousands of servers in a Hadoop cluster. You will learn how big data has improved web search and how online advertising systems work.
By the end of this course, you will have a better understanding of the various applications of big data methods in industry and research.



Section 1: The basics of working with big data 
Understand the four V’s of Big Data (Volume, Velocity, and Variety); Build models for data; Understand the occurrence of rare events in random data. 
Section 2: Web and social networks 
Understand characteristics of the web and social networks; Model social networks; Apply algorithms for community detection in networks. 
Section 3: Clustering big data 
Clustering social networks; Apply hierarchical clustering; Apply k-means clustering. 
Section 4: Google web search 
Understand the concept of PageRank; Implement the basic; PageRank algorithm for strongly connected graphs; Implement PageRank with taxation for graphs that are not strongly connected. 
Section 5: Parallel and distributed computing using MapReduce 
Understand the architecture for massive distributed and parallel computing; Apply MapReduce using Hadoop; Compute PageRank using MapReduce. 
Section 6: Computing similar documents in big data 
Measure importance of words in a collection of documents; Measure similarity of sets and documents; Apply local sensitivity hashing to compute similar documents. 
Section 7: Products frequently bought together in stores 
Understand the importance of frequent item sets; Design association rules; Implement the A-priori algorithm. 
Section 8: Movie and music recommendations 
Understand the differences of recommendation systems; Design content-based recommendation systems; Design collaborative filtering recommendation systems. 
Section 9: Google's AdWordsTM System 
Understand the AdWords System; Analyse online algorithms in terms of competitive ratio; Use online matching to solve the AdWords problem. 
Section 10: Mining rapidly arriving data streams 
Understand types of queries for data streams; Analyse sampling methods for data streams; Count distinct elements in data streams; Filter data streams."
https://www.classcentral.com/course/bigdatabrasil-4537,"Em 1988 foi criado o Sistema Único de Saúde (SUS), nosso sistema público e universal de saúde. Apesar da sua importância na diminuição das disparidades em saúde, o sistema de saúde tem
consistentemente liderado a lista de maiores problemas do país segundo as pesquisas de
opinião. Reclamações frequentes sobre o sistema privado têm contribuído para a conclusão de que a saúde é um
problema generalizado no país. O uso de big data pode trazer melhorias
significativas em termos de gestão, transparência e responsabilidade por parte dos
provedores de saúde, além de fornecer dados importantes para os pacientes.Este curso irá abordar tanto a obtenção de dados em saúde como a sua utilização para
melhorar a qualidade de tomada de decisões por parte de hospitais e das diferentes esferas do governo. Mais especificamente, abordaremos as fontes de dados públicos de saúde, o uso do R e do Python para a
análise de dados, metodologias de big data (como machine learning, redução de
dimensão e como evitar associações espúrias), noções de economia da saúde e o uso de metodologias causais em pesquisas científicas. Apesar de o foco do curso ser a área da saúde, os conhecimentos podem ser diretamente aplicados em outras áreas como a educação, biologia, economia e administração.Todas as aulas estão disponíveis, mesmo se a inscrição for feita após o término do curso.
      


Primeiro módulo: Perspectivas do uso de big data no Brasil (estudos multicêntricos, prontuários eletrônicos, medicina de precisão e internet das coisas).Segundo módulo: Linguagens de programação mais utilizadas em big data (R e Python).Terceiro módulo: Metodologias de big data (machine learning, redução de dimensão e controle de associações espúrias).Quarto módulo: Causalidade (propensity score, regressão descontínua, diferenças em diferenças, variáveis instrumentais e randomização mendeliana).Quinto módulo: As ""imperfeições"" da saúde pública - Introdução à Economia da Saúde (informação assimétrica, seleção adversa, risco moral e o problema do agente-principal).Sexto módulo: Limitações e conclusão (privacidade e o uso de big data para uma nova saúde pública)."
https://www.classcentral.com/course/columbiax-data-science-for-executives-18406,"Data science is making us smarter and more innovative in so many ways. How does it all work? In this Data Science and Analytics Professional Certificate program you will gain insight into the latest data science tools and their application in finance, health care, product development, sales and more. With real world examples, we will demonstrate how data science can improve corporate decision-making and performance, personalize medicine and advance your career goals.
Taught by a distinguished team of professors at Columbia University’s Data Science Institute, this program is perfect for anyone who wants to understand basic concepts in data science without getting into the weeds of programming. Aimed at organization leaders, business managers, health care professionals and anyone considering a career in data science, this program will steep learners in the fundamentals of statistics, machine learning and algorithms. It will also introduce emerging technologies such as the Internet of Things (IoT) , or wirelessly connected products, and techniques that allow computers to summarize mountains of text, audio and video. Concrete examples provided throughout the program will ensure that learners fully grasp and master key concepts.



Courses under this program:Course 1: Statistical Thinking for Data Science and Analytics
Learn how statistics plays a central role in the data science approach.
Course 2: Machine Learning for Data Science and Analytics
Learn the principles of machine learning and the importance of algorithms.
Course 3: Enabling Technologies for Data Science and Analytics: The Internet of Things
Discover the relationship between Big Data and the Internet of Things (IoT)."
https://www.classcentral.com/course/edx-sql-for-data-science-17900,"Much of the world's data lives in databases. SQL (or Structured Query Language) is a powerful programming language that is used for communicating with and extracting various data types from databases. A working knowledge of databases and SQL is necessary to advance as a data scientist or a machine learning specialist. The purpose of this course is to introduce relational database concepts and help you learn and apply foundational knowledge of the SQL language. It is also intended to get you started with performing SQL access in a data science environment.
The emphasis in this course is on hands-on, practical learning. As such, you will work with real databases, real data science tools, and real-world datasets. You will create a database instance in the cloud. Through a series of hands-on labs, you will practice building and running SQL queries. You will also learn how to access databases from Jupyter notebooks using SQL and Python.
No prior knowledge of databases, SQL, Python, or programming is required."
https://www.classcentral.com/course/miriadax-curso-practico-de-bioestadistica-con-r-1437,"Descripción Gracias por tu interés. Aunque este MOOC ya se realizó, si te inscribes podrás acceder a los contenidos más importantes y a los vídeos. Sin embargo, no podrás realizar ninguna de las actividades ni te podrás certificar. Cuando esté disponible una nueva edición podrás inscribirte para que obtengas la experiencia completa de un MOOC de Miríadax. Curso de Estadística aplicada a las Ciencias de la Salud. En esta primera parte el curso se centrará en la estadística descriptiva de una o más variables y de sus relaciones. El curso también introducirá el manejo del programa de análisis de datos R y el paque rk.Teaching con el que se realizarán los análisis estadísticos. 
      


          Módulo 0. Presentación del curso e instalación del software. Módulo 1. Introducción a la Estadística. Módulo 2. Organización y presentación de los datos. Distribuciones de frecuencias y diagramas. Módulo 3. Estadísticos descriptivos de una variable. Módulo 4. Estadística descriptiva de las relaciones entre variables. Regresión y correlación lineal. Módulo 5. Estadística descriptiva de las relaciones entre variables. Regresión no lineal y medidas de asociación entre variables cualitativas."
https://www.classcentral.com/course/data-science-python-18916,"The 5 courses in this University of Michigan specialization introduce learners to data science through the python programming language. This skills-based specialization is intended for learners who have a basic python or programming background, and want to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular python toolkits such as pandas, matplotlib, scikit-learn, nltk, and networkx to gain insight into their data.

Introduction to Data Science in Python (course 1), Applied Plotting, Charting & Data Representation in Python (course 2), and Applied Machine Learning in Python (course 3) should be taken in order and prior to any other course in the specialization. After completing those, courses 4 and 5 can be taken in any order. All 5 are required to earn a certificate.
      


          Course 1: Introduction to Data Science in Python- This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses. This course should be taken before any of the other Applied Data Science with Python courses: Applied Plotting, Charting & Data Representation in Python, Applied Machine Learning in Python, Applied Text Mining in Python, Applied Social Network Analysis in Python.Course 2: Applied Plotting, Charting & Data Representation in Python- This course will introduce the learner to information visualization basics, with a focus on reporting and charting using the matplotlib library. The course will start with a design and information literacy perspective, touching on what makes a good and bad visualization, and what statistical measures translate into in terms of visualizations. The second week will focus on the technology used to make visualizations in python, matplotlib, and introduce users to best practices when creating basic charts and how to realize design decisions in the framework. The third week will be a tutorial of functionality available in matplotlib, and demonstrate a variety of basic statistical charts helping learners to identify when a particular method is good for a particular problem. The course will end with a discussion of other forms of structuring and visualizing data. This course should be taken after Introduction to Data Science in Python and before the remainder of the Applied Data Science with Python courses: Applied Machine Learning in Python, Applied Text Mining in Python, and Applied Social Network Analysis in Python.Course 3: Applied Machine Learning in Python- This course will introduce the learner to applied machine learning, focusing more on the techniques and methods than on the statistics behind these methods. The course will start with a discussion of how machine learning is different than descriptive statistics, and introduce the scikit learn toolkit through a tutorial. The issue of dimensionality of data will be discussed, and the task of clustering data, as well as evaluating those clusters, will be tackled. Supervised approaches for creating predictive models will be described, and learners will be able to apply the scikit learn predictive modelling methods while understanding process issues related to data generalizability (e.g. cross validation, overfitting). The course will end with a look at more advanced techniques, such as building ensembles, and practical limitations of predictive models. By the end of this course, students will be able to identify the difference between a supervised (classification) and unsupervised (clustering) technique, identify which technique they need to apply for a particular dataset and need, engineer features to meet that need, and write python code to carry out an analysis. This course should be taken after Introduction to Data Science in Python and Applied Plotting, Charting & Data Representation in Python and before Applied Text Mining in Python and Applied Social Analysis in Python.Course 4: Applied Text Mining in Python- This course will introduce the learner to text mining and text manipulation basics. The course begins with an understanding of how text is handled by python, the structure of text both to the machine and to humans, and an overview of the nltk framework for manipulating text. The second week focuses on common manipulation needs, including regular expressions (searching for text), cleaning text, and preparing text for use by machine learning processes. The third week will apply basic natural language processing methods to text, and demonstrate how text classification is accomplished. The final week will explore more advanced methods for detecting the topics in documents and grouping them by similarity (topic modelling). This course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python.Course 5: Applied Social Network Analysis in Python- This course will introduce the learner to network analysis through tutorials using the NetworkX library. The course begins with an understanding of what network analysis is and motivations for why we might model phenomena as networks. The second week introduces the concept of connectivity and network robustness. The third week will explore ways of measuring the importance or centrality of a node in a network. The final week will explore the evolution of networks over time and cover models of network generation and the link prediction problem. This course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python."
https://www.classcentral.com/course/exploring-visualizing-iot-data-8022,"The value of IoT can be found within the analysis of data gathered from the system under observation, where insights gained can have direct impact on business and operational transformation.   Through analysis data correlation, patterns, trends, and other insight are discovered.  Insight leads to better communication between stakeholders, or actionable insights, which can be used to raise alerts or send commands, back to IoT devices.
With a focus on the topic of Exploratory Data Analysis, the course provides an in-depth look at mathematical foundations of basic statistical measures, and how they can be used in conjunction with advanced charting libraries to make use of the world’s best pattern recognition system – the human brain.  Learn how to work with the data, and depict it in ways that support visual inspections, and derive to inferences about the data. Identify interesting characteristics, patterns, trends, deviations or inconsistencies, and potential outliers.  The goal is that you are able to implement end-to-end analytic workflows at scale, from data acquisition to actionable insights. 
Through a series of lectures and exercises students get the needed skills to perform such analysis on any data, although we clearly focus on IoT Sensor Event Data.

After completing this course, you will be able to:
•	Describe how basic statistical measures, are used to reveal  patterns within the data 
•	Recognize data characteristics, patterns, trends, deviations or inconsistencies, and potential outliers.
•	Identify useful techniques for working with big data such as dimension reduction and feature selection methods 
•	Use advanced tools and charting libraries to:
      o	Automatically store data from IoT device(s) 
      o	improve efficiency of analysis of big-data with partitioning and parallel analysis 
      o	Visualize the data in an number of 2D and 3D formats (Box Plot, Run Chart, Scatter Plot, Pareto Chart, and Multidimensional Scaling)

For successful completion of the course, the following prerequisites are recommended: 
•	Basic programming skills in any programming language (python preferred)
•	A good grasp of basic algebra and algebraic equations
•	(optional) “A developer's guide to the Internet of Things (IoT)” - a Coursera course
•	Basic SQL is a plus

In order to complete this course, the following technologies will be used:
(These technologies are introduced in the course as necessary so no previous knowledge is required.)
•	IBM Watson IoT Platform (MQTT Message Broker as a Service, Device Management and Operational Rule Engine)
•	IBM Bluemix (Open Standard Platform Cloud)
•	Node-Red
•	Cloudant NoSQL (Apache CouchDB)
•	ApacheSpark
•	Languages: R, Scala and Python (focus on Python)

This course takes four weeks, 4-6h per week
      


            Read more
          



Introduction to exploratory analysisAnalysis of data starts with a hypothesis and through exploration, those hypothesis are tested.  Exploratory analysis in IoT considers large amounts of data, past or current, from multiple sources and summarizes its main characteristics.  Data is strategically inspected, cleaned, and models are created with the purpose of gaining insight, predicting  future data, and supporting decision making.     This learning module introduces methods for turning raw IoT data into insight Tools that support IoT solutionsData analysis for IoT indicates that you have to build a solution for performing scalable analytics, on a large amount of data that arrives in great volumes and velocity.  Such a solution needs to be supported by a number of tools.   This module introduces common and popular tools, and highlights how they help data analyst produce viable end-to-end  solutions. Mathematical Foundations on Exploratory Data AnalysisThis learning module explores mathematical foundations supporting Exploratory Data Analysis (EDA) techniques.   Data VisualizationThis learning module details a variety of methods for plotting IoT time series sensor data using different methods in order to gain insights of hidden patterns in your data"
https://www.classcentral.com/course/edx-essentials-of-data-literacy-17966,"In this course, you will practice analyzing, visualizing, and communicating with data, using real datasets and examples that are relevant to a variety of audiences and academic disciplines. Data is part of every field, but not everyone has had the opportunity to gain the skills necessary to find the data they need and use it in ways that add to their work. Whether you are in public health, healthcare, banking, law, education, graduate school, or a variety of other fields, there is a way to understand and make use of related data.
This four-week course will give you the opportunity to build and leverage your data skills for upward mobility at any stage in your career. It will take you through the five steps of the data lifecycle, using different case studies and contexts, and teach you how to analyze, manage, and communicate data, working in R to achieve basic R programming competencies.
Photo by NASA on Unsplash"
https://www.classcentral.com/course/swayam-data-analytics-with-python-17563,"We are looking forward to sharing many exciting stories and examples of analytics with all of you using python programming language. This course includes examples of analytics in a wide variety of industries, and we hope that students will learn how you can use analytics in their career and life. One of the most important aspects of this course is that you, the student, are getting hands-on experience creating analytics models; we, the course team, urge you to participate in the discussion forums and to use all the tools available to you while you are in the course!
INTENDED AUDIENCE : Management, Industrial Engineering and Computer Science Engineering StudentsPREREQUISITES : NillINDUSTRY SUPPORT : Any analytics company
      


COURSE LAYOUT
Week 1 : Introduction to data analytics and Python fundamentals
Week 2 : Introduction to probability
Week 3 : Sampling and sampling distributions
Week 4 : Hypothesis testing
Week 5 : Two sample testing and introduction to ANOVA
Week 6 : Two way ANOVA and linear regression
Week 7 : Linear regression and multiple regression
Week 8 : Concepts of MLE and Logistic regression
Week 9 : ROC and Regression Analysis Model Building
Week 10 : c2Test and introduction to cluster analysis
 
 
Week 11 : Clustering analysis
Week 12 : Classification and Regression Trees (CART)"
https://www.classcentral.com/course/edx-data-science-tools-17908,"In this course, you'll learn about Data Science tools like Jupyter Notebooks, RStudio IDE, and Watson Studio. You will learn what each tool is used for, what programming languages they can execute, their features and limitations and how data scientists use these tools today.
With the tools hosted in the cloud, you will be able to test each tool and follow instructions to run simple code in Python or R. To complete the course, you will create a final project with a Jupyter Notebook on IBM Watson Studio on Cloud and demonstrate your proficiency in preparing a notebook, writing Markdown, and sharing your work with your peers.
This hands-on course will get you up and running with some of the latest and greatest data science tools."
https://www.classcentral.com/course/microsoft-data-analysis-18453,"Please note on June 30, 2020, this program will be retiring and no longer available on edX. If you are interested in earning the Professional Certificate you must complete the program by June 30, 2020, in order to earn the certificate. 
Data Analysis: Essential Skills teaches core analytics principles, including how to manipulate and query large data sets. Students will work with T-SQL and use pivot tables for data visualization.
This program consists of three highly focused courses that you can take at your own pace.
Our approach combines self-directed online learning with hands-on labs, using the latest tools so you can hit the ground running. By enrolling in a shorter program, you can more quickly gain a standalone Professional Certificate and the valuable analytics skills that employers are looking for



Courses under this program:Course 1: Querying Data with Transact-SQL
From querying and modifying data in SQL Server or Azure SQL to programming with Transact-SQL, learn essential skills that employers need.
Course 2: Analyzing and Visualizing Data with Excel
Develop your skills with Excel, one of the common tools that data scientists depend on to gather, transform, analyze, and visualize data.
Course 3: Introduction to Data Analysis using Excel
Learn the basics of Excel, one of the most popular data analysis tools, to help visualize and gain insights from your data."
https://www.classcentral.com/course/swayam-bioinformatics-algorithms-and-applications-10031,"Bioinformatics is an interdisciplinary field of science for analyzing and interpreting vast biological data using computational techniques. In this course, we aim to give a walkthrough of the major aspects of bioinformatics such as the development of databases, computationally derived hypothesis, algorithms, and computer-aided drug design. During the first section of the course, we will focus on DNA and protein sequence databases and analysis, secondary structures and 3D structural analysis. The second section will be devoted to applications such as prediction of protein structure, folding rates, stability upon mutation, and intermolecular interactions. Further, we will cover computer-aided drug design using docking and QSAR studies. This course is designed to nurture skills and knowledge required for aspiring students, young biologists and research scholars to develop algorithms and tools in bioinformatics.



Week 1 : Introduction, DNA sequence analysis, DNA DatabasesWeek 2 : Protein structure and function, protein sequence databases, sequence alignmentWeek 3 : PAM matrix, Global and local alignment, BLAST: features and scoresWeek 4 : Multiple sequence alignment, Conservation score, phylogenetic treesWeek 5 : Protein sequence analysis, hydrophobicity profiles, non-redundant datasetsWeek 6 : Protein secondary structures, Ramachandran plot, propensity, secondary structure predictionWeek 7 : Protein tertiary structure, Protein Data Bank, visualization tools, structural classification, contact mapsWeek 8 : Protein structural analysis, protein structure predictionWeek 9 : Protein stability, energetic contributions, database, stabilizing residues, stability upon mutationsWeek 10 : Protein folding rates, proteins interactions, binding site residuesWeek 11 : Computer aided drug design, docking, screening, QSARWeek 12 : Development of algorithms, awk programming, machine learning techniques, applications using WEKA"
https://www.classcentral.com/course/edx-computational-thinking-and-big-data-8161,"Computational thinking is an invaluable skill that can be used across every industry, as it allows you to formulate a problem and express a solution in such a way that a computer can effectively carry it out.
In this course, part of the Big Data MicroMasters program, you will learn how to apply computational thinking in data science. You will learn core computational thinking concepts including decomposition, pattern recognition, abstraction, and algorithmic thinking.
You will also learn about data representation and analysis and the processes of cleaning, presenting, and visualizing data. You will develop skills in data-driven problem design and algorithms for big data.
The course will also explain mathematical representations, probabilistic and statistical models, dimension reduction and Bayesian models.
You will use tools such as R and Java data processing libraries in associated language environments.



Section 1: Data in R 
Identify the components of RStudio; Identify the subjects and types of variables in R; Summarise and visualise univariate data, including histograms and box plots. 
Section 2: Visualising relationships 
Produce plots in ggplot2 in R to illustrate the relationship between pairs of variables; Understand which type of plot to use for different variables; Identify methods to deal with large datasets. 
Section 3: Manipulating and joining data 
Organise different data types, including strings, dates and times; Filter subjects in a data frame, select individual variables, group data by variables and calculate summary statistics; Join separate dataframes into a single dataframe; Learn how to implement these methods in mapReduce. 
Section 4: Transforming data and dimension reduction 
Transform data so that it is more appropriate for modelling; Use various methods to transform variables, including q-q plots and Box-Cox transformation, so that they are distributed normally Reduce the number of variables using PCA; Learn how to implement these techniques into modelling data with linear models. 
Section 5: Summarising data 
Estimate model parameters, both point and interval estimates; Differentiate between the statistical concepts or parameters and statistics; Use statistical summaries to infer population characteristics; Utilise strings; Learn about k-mers in genomics and their relationship to perfect hash functions as an example of text manipulation. 
Section 6: Introduction to Java 
Use complex data structures; Implement your own data structures to organise data; Explain the differences between classes and objects; Motivate object-orientation. 
Section 7: Graphs 
Encode directed and undirected graphs in different data structures, such as matrices and adjacency lists; Execute basic algorithms, such as depth-first search and breadth-first search. 
Section 8: Probability 
Determine the probability of events occurring when the probability distribution is discrete; How to approximate. 
Section 9: Hashing 
Apply hash functions on basic data structures in Java; Implement your own hash functions and execute, these as well as built-in ones; Differentiate good from bad hash functions based on the concept of collisions. 
Section 10: Bringing it all together 
Understand the context of big data in programming."
https://www.classcentral.com/course/introduction-data-science-18764,"In this Specialization learners will develop foundational Data Science skills to prepare them for a career or further learning that involves more advanced topics in Data Science. The specialization entails understanding what is Data Science and the various kinds of activities that a Data Scientist performs. It will familiarize learners with various open source tools, like Jupyter notebooks, used by Data Scientists. It will teach you about methodology involved in tackling data science problems. The specialization also provides knowledge of relational database concepts and the use of SQL to query databases. Learners will complete hands-on labs and projects to apply their newly acquired skills and knowledge. Upon receiving the certificate for completion of the specialization, you will also receive an IBM Badge as a Specialist in Data Science Foundations. LIMITED TIME OFFER: Subscription is only $39 USD per month and gives you access to graded materials and a certificate.



          Course 1: What is Data Science? - The art of uncovering the insights and trends in data has been around since ancient times. The ancient Egyptians used census data to increase efficiency in tax collection and they accurately predicted the flooding of the Nile river every year. Since then, people working in data science have carved out a unique and distinct field for the work they do. This field is data science. In this course, we will meet some data science practitioners and we will get an overview of what data science is today. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 2: Open Source tools for Data Science- What are some of the most popular data science tools, how do you use them, and what are their features? In this course, you'll learn about Jupyter Notebooks, RStudio IDE, Apache Zeppelin and Data Science Experience. You will learn about what each tool is used for, what programming languages they can execute, their features and limitations. With the tools hosted in the cloud on Cognitive Class Labs, you will be able to test each tool and follow instructions to run simple code in Python, R or Scala. To end the course, you will create a final project with a Jupyter Notebook on IBM Data Science Experience and demonstrate your proficiency preparing a notebook, writing Markdown, and sharing your work with your peers. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 3: Data Science Methodology- Despite the recent increase in computing power and access to data over the last couple of decades, our ability to use the data within the decision making process is either lost or not maximized at all too often, we don't have a solid understanding of the questions being asked and how to apply the data correctly to the problem at hand. This course has one purpose, and that is to share a methodology that can be used within data science, to ensure that the data used in problem solving is relevant and properly manipulated to address the question at hand. Accordingly, in this course, you will learn: - The major steps involved in tackling a data science problem. - The major steps involved in practicing data science, from forming a concrete business or research problem, to collecting and analyzing data, to building a model, and understanding the feedback after model deployment. - How data scientists think! LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 4: Databases and SQL for Data Science- Much of the world's data resides in databases. SQL (or Structured Query Language) is a powerful language which is used for communicating with and extracting data from databases. A working knowledge of databases and SQL is a must if you want to become a data scientist. The purpose of this course is to introduce relational database concepts and help you learn and apply foundational knowledge of the SQL language. It is also intended to get you started with performing SQL access in a data science environment. The emphasis in this course is on hands-on and practical learning . As such, you will work with real databases, real data science tools, and real-world datasets. You will create a database instance in the cloud. Through a series of hands-on labs you will practice building and running SQL queries. You will also learn how to access databases from Jupyter notebooks using SQL and Python. No prior knowledge of databases, SQL, Python, or programming is required. Anyone can audit this course at no-charge. If you choose to take this course and earn the Coursera course certificate, you can also earn an IBM digital badge upon successful completion of the course. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate."
https://www.classcentral.com/course/executive-data-science-capstone-5088,"The Executive Data Science Capstone, the specialization’s culminating project, is an opportunity for people who have completed all four EDS courses to apply what they've learned to a real-world scenario developed in collaboration with Zillow, a data-driven online real estate and rental marketplace, and DataCamp, a web-based platform for data science programming. Your task will be to lead a virtual data science team and make key decisions along the way to demonstrate that you have what it takes to shepherd a complex analysis project from start to finish.  For the final project, you will prepare and submit a presentation, which will be evaluated and graded by your fellow capstone participants.

Course cover image by Luckey_sun. Creative Commons BY-SA https://flic.kr/p/bx1jvU
      


          Executive Data Science Capstone
    -It's time to put your skills to the test managing a data science project at Zillow, a data-driven online real estate and rental marketplace. Along the way, you'll make important decisions as you lead your team through the project."
https://www.classcentral.com/course/jhu-data-science-18692,"Ask the right questions, manipulate data sets, and create visualizations to communicate results. This Specialization covers the concepts and tools you'll need throughout the entire data science pipeline, from asking the right kinds of questions to making inferences and publishing results. In the final Capstone Project, you’ll apply the skills learned by building a data product using real-world data. At completion, students will have a portfolio demonstrating their mastery of the material.



          Course 1: The Data Scientist’s Toolbox- In this course you will get an introduction to the main tools and ideas in the data scientist's toolbox. The course gives an overview of the data, questions, and tools that data analysts and data scientists work with. There are two components to this course. The first is a conceptual introduction to the ideas behind turning data into actionable knowledge. The second is a practical introduction to the tools that will be used in the program like version control, markdown, git, GitHub, R, and RStudio.Course 2: R Programming- In this course you will learn how to program in R and how to use R for effective data analysis. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language. The course covers practical issues in statistical computing which includes programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting R code. Topics in statistical data analysis will provide working examples.Course 3: Getting and Cleaning Data- Before you can work with data you have to get some. This course will cover the basic ways that data can be obtained. The course will cover obtaining data from the web, from APIs, from databases and from colleagues in various formats. It will also cover the basics of data cleaning and how to make data “tidy”. Tidy data dramatically speed downstream data analysis tasks. The course will also cover the components of a complete data set including raw data, processing instructions, codebooks, and processed data. The course will cover the basics needed for collecting, cleaning, and sharing data.Course 4: Exploratory Data Analysis- This course covers the essential exploratory techniques for summarizing data. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data.Course 5: Reproducible Research- This course focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner. Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. This course will focus on literate statistical analysis tools which allow one to publish data analyses in a single document that allows others to easily execute the same analysis to obtain the same results.Course 6: Statistical Inference- Statistical inference is the process of drawing conclusions about populations or scientific truths from data. There are many modes of performing inference including statistical modeling, data oriented strategies and explicit use of designs and randomization in analyses. Furthermore, there are broad theories (frequentists, Bayesian, likelihood, design based, …) and numerous complexities (missing data, observed and unobserved confounding, biases) for performing inference. A practitioner can often be left in a debilitating maze of techniques, philosophies and nuance. This course presents the fundamentals of inference in a practical approach for getting things done. After taking this course, students will understand the broad directions of statistical inference and use this information for making informed choices in analyzing data.Course 7: Regression Models- Linear models, as their name implies, relates an outcome to a set of predictors of interest using linear assumptions. Regression models, a subset of linear models, are the most important statistical analysis tool in a data scientist’s toolkit. This course covers regression analysis, least squares and inference using regression models. Special cases of the regression model, ANOVA and ANCOVA will be covered as well. Analysis of residuals and variability will be investigated. The course will cover modern thinking on model selection and novel uses of regression models including scatterplot smoothing.Course 8: Practical Machine Learning- One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.Course 9: Developing Data Products- A data product is the production output from a statistical analysis. Data products automate complex analysis tasks or use technology to expand the utility of a data informed model, algorithm or inference. This course covers the basics of creating data products using Shiny, R packages, and interactive graphics. The course will focus on the statistical fundamentals of creating a data product that can be used to tell a story about data to a mass audience.Course 10: Data Science Capstone- The capstone project class will allow students to create a usable/public data product that can be used to show your skills to potential employers. Projects will be drawn from real-world problems and will be conducted with industry, government, and academic partners."
https://www.classcentral.com/course/adelaidex-big-data-18320,"Big data is changing the way businesses operate. Driven by a new scale of data collection that provides massive levels of information, businesses are now able to analyse and gather data insights to make better-informed decisions.
Data scientists and business analysts are in high-demand as companies look to use data to improve their business operations.
In this Big Data MicroMasters program, you will learn tools and analytical methods to use data for decision-making, collect and organise data at scale, and gain an understanding of how data analysis can help to inform change within organisations.
You’ll develop both the technical and computational skills that are in high demand across a range of industries. You’ll develop critical skills in programming for data science, computational thinking, algorithm design, big data fundamentals, and data-driven analysis, with plenty of opportunities to apply and explore your new learnings through a range of case studies.
Click here to view the program syllabus, including the start and end dates for all the course releases.



Courses under this program:Course 1: Programming for Data ScienceLearn how to apply fundamental programming concepts, computational thinking and data analysis techniques to solve real-world data science problems.Course 2: Computational Thinking and Big DataLearn the core concepts of computational thinking and how to collect, clean and consolidate large-scale datasets.Course 3: Big Data Fundamentals
Learn how big data is driving organisational change and essential analytical tools and techniques, including data mining and PageRank algorithms.
Course 4: Big Data AnalyticsLearn key technologies and techniques, including R and Apache Spark, to analyse large-scale data sets to uncover valuable business information.Course 5: Big Data Capstone ProjectFurther develop your knowledge of big data by applying the skills you have learned to a real-world data science project."
https://www.classcentral.com/course/data-driven-decision-making-18277,"Data becomes valuable when it allows us to make a decision or take action in the real world.
On this microcredential, you’ll work through practical programming exercises in R language to learn the process of tidying, harvesting and wrangling data and applying statistical models to simulate complex functions that solve a broad range of problems.
Using data visualisation techniques, you’ll learn to interpret and explain data to inform your decision-making process and communicate your message to others.
You’ll also explore the essential ethical, legal and organisational issues of data collection and management.



Courses under this program:Course 1: Wrangling and Workflow-Get an introduction to data science and learn how to make the most of your data through effective data wrangling and storytelling.Course 2: Modelling and Visualisation-Explore the role of data visualisation in data science, and learn how to use and apply data modelling techniques.Course 3: Formats, Ethics, and Storytelling-Evaluate the challenges of data variety, ethics, and privacy on this course in Monash University's data science microcredential."
https://www.classcentral.com/course/applied-data-science-18818,"This is an action-packed specialization is for data science enthusiasts who want to acquire practical skills for real world data problems. It appeals to anyone interested in pursuing a career in Data Science, and already has foundational skills (or has completed the Introduction to Applied Data Science specialization). You will learn Python - no prior programming knowledge necessary. You will then learn data visualization and data analysis. Through our guided lectures, labs, and projects you’ll get hands-on experience tackling interesting data problems. Make sure to take this specialization to solidify your Python and data science skills before diving deeper into big data, AI, and deep learning.

Upon completing all courses in the specialization and receiving the Specialization certificate, you will also receive an IBM Badge recognizing you as a Specialist in Applied Data Science.

LIMITED TIME OFFER: Subscription is only $39 USD per month and gives you access to graded materials and a certificate.
      


          Course 1: Python for Data Science and AI- This introduction to Python will kickstart your learning of Python for data science, as well as programming in general. This beginner-friendly Python course will take you from zero to programming in Python in a matter of hours. Module 1 - Python Basics o Your first program o Types o Expressions and Variables o String Operations Module 2 - Python Data Structures o Lists and Tuples o Sets o Dictionaries Module 3 - PythonProgramming Fundamentals o Conditions and Branching o Loops o Functions o Objects and Classes Module 4 - Working with Data in Python o Reading files with open o Writing files with open o Loading data with Pandas o Numpy Finally, you will create a project to test your skills.Course 2: Data Analysis with Python- Learn how to analyze data using Python. This course will take you from the basics of Python to exploring many different types of data. You will learn how to prepare data for analysis, perform simple statistical analysis, create meaningful data visualizations, predict future trends from data, and more! Topics covered: 1) Importing Datasets 2) Cleaning the Data 3) Data frame manipulation 4) Summarizing the Data 5) Building machine learning Regression models 6) Building data pipelines Data Analysis with Python will be delivered through lecture, lab, and assignments. It includes following parts: Data Analysis libraries: will learn to use Pandas, Numpy and Scipy libraries to work with a sample dataset. We will introduce you to pandas, an open-source library, and we will use it to load, manipulate, analyze, and visualize cool datasets. Then we will introduce you to another open-source library, scikit-learn, and we will use some of its machine learning algorithms to build smart models and make cool predictions. If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 3: Data Visualization with Python- ""A picture is worth a thousand words"". We are all familiar with this expression. It especially applies when trying to explain the insight obtained from the analysis of increasingly large datasets. Data visualization plays an essential role in the representation of both small and large-scale data. One of the key skills of a data scientist is the ability to tell a compelling story, visualizing data and findings in an approachable and stimulating way. Learning how to leverage a software tool to visualize data will also enable you to extract information, better understand the data, and make more effective decisions. The main goal of this Data Visualization with Python course is to teach you how to take data that at first glance has little meaning and present that data in a form that makes sense to people. Various techniques have been developed for presenting data visually but in this course, we will be using several data visualization libraries in Python, namely Matplotlib, Seaborn, and Folium. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 4: Applied Data Science Capstone- This capstone project course will give you a taste of what data scientists go through in real life when working with data. You will learn about location data and different location data providers, such as Foursquare. You will learn how to make RESTful API calls to the Foursquare API to retrieve data about venues in different neighborhoods around the world. You will also learn how to be creative in situations where data are not readily available by scraping web data and parsing HTML code. You will utilize Python and its pandas library to manipulate data, which will help you refine your skills for exploring and analyzing data. Finally, you will be required to use the Folium library to great maps of geospatial data and to communicate your results and findings. If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge upon successful completion of the course. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate."
https://www.classcentral.com/course/microsoft-data-science-core-18462,"Please note on June 30, 2020, this program will be retiring and no longer available on edX. If you are interested in earning the Professional Certificate you must complete the program by June 30, 2020, in order to earn the certificate. 
Data science if one of the hottest fields today. In this program, you will learn key data science tools, and widely-used programming languages from industry and academic experts.
This Professional Certificate program will help you develop the analytical and programming skills you need to take advantage of the 1.5 million career opportunities available now in data science.



Courses under this program:Course 1: Querying Data with Transact-SQL
From querying and modifying data in SQL Server or Azure SQL to programming with Transact-SQL, learn essential skills that employers need.
Course 2: Introduction to Python for Data Science
The ability to analyze data with Python is critical in data science. Learn the basics, and move on to create stunning visualizations.
Course 3: Essential Math for Machine Learning: Python Edition
Learn the essential mathematical foundations for machine learning and artificial intelligence."
https://www.classcentral.com/course/france-universite-numerique-numerique-et-recherche-en-sante-et-sciences-du-vivant-4511,"Ce MOOC donne les éléments pour s'adapter à l’évolution des pratiques de recherche en santé et sciences du vivant, prendre en main les outils numériques associes et saisir les enjeux nouveaux qu'il y introduit."
https://www.classcentral.com/course/more-data-mining-with-weka-8389,"Become an experienced data miner
This course introduces advanced data mining skills, following on from Data Mining with Weka. You’ll process a dataset with 10 million instances. You’ll mine a 250,000-word text dataset. You’ll analyze a supermarket dataset representing 5000 shopping baskets. You’ll learn about filters for preprocessing data, selecting attributes, classification, clustering, association rules, cost-sensitive evaluation. You’ll meet learning curves and automatically optimize learning parameters.  Weka originated at the University of Waikato in NZ, and Ian Witten has authored a leading book on data mining.
This course is aimed at anyone who deals in data. It follows on from Data Mining with Weka, and you should have completed that first (or have otherwise acquired a rudimentary knowledge of Weka). As with the previous course, it involves no computer programming, although you need some experience with using computers for everyday tasks. High-school maths is more than enough; some elementary statistics concepts (means and variances) are assumed.
Before the course starts, download the free Weka software. It runs on any computer, under Windows, Linux, or Mac. It has been downloaded millions of times and is being used all around the world.
(Note: Depending on your computer and system version, you may need admin access to install Weka.)"
https://www.classcentral.com/course/edx-knowledge-inference-and-structure-discovery-for-education-9288,"In this course, you will learn key methods for discovering how content can be divided into skills and concepts and how to measure student knowledge while it is changing – i.e. the student is learning.This course will also cover related methods for discovering structure in unlabeled data, such as factor analysis and clustering. It will also cover related methods for relationship mining including how to validly conduct correlation mining and how to automatically discover association rules and sequential rules.This mini-course does not assume prior programming knowledge beyond what you will already have learned in other courses in this MicroMasters, although advanced tools will be discussed for interested students.This course includes content also offered in the University of Pennsylvania’s edX MOOC, Big Data and Education, weeks 4, 5, and 7.
      


          Week 1: Structure Discovery: Clustering, Factor Analysis, and Knowledge StructuresWeek 2: Knowledge Inference: Bayesian Knowledge Tracing, Performance Factors Analysis, Item Response Theory, and Deep LearningWeek 3: Relationship Mining: Correlation Mining, Association Rule Mining, and Sequential Pattern Mining"
https://www.classcentral.com/course/learn-sql-basics-data-science-18613,"This Specialization is intended for a learner with no previous coding experience seeking to develop SQL query fluency. Through four progressively more difficult SQL projects with data science applications, you will cover topics such as SQL basics, data wrangling, SQL analysis, AB testing, distributed computing using Apache Spark, and more. These topics will prepare you to apply SQL creatively to analyze and explore data; demonstrate efficiency in writing queries; create data analysis datasets; conduct feature engineering, use SQL with other data analysis and machine learning toolsets; and use SQL with unstructured data sets.



          Course 1: SQL for Data Science- As data collection has increased exponentially, so has the need for people skilled at using and interacting with data; to be able to think critically, and provide insights to make better decisions and optimize their businesses. This is a data scientist, “part mathematician, part computer scientist, and part trend spotter” (SAS Institute, Inc.). According to Glassdoor, being a data scientist is the best job in America; with a median base salary of $110,000 and thousands of job openings at a time. The skills necessary to be a good data scientist include being able to retrieve and work with data, and to do that you need to be well versed in SQL, the standard language for communicating with database systems. This course is designed to give you a primer in the fundamentals of SQL and working with data so that you can begin analyzing it for data science purposes. You will begin to ask the right questions and come up with good answers to deliver valuable insights for your organization. This course starts with the basics and assumes you do not have any knowledge or skills in SQL. It will build on that foundation and gradually have you write both simple and complex queries to help you select data from tables. You'll start to work with different types of data like strings and numbers and discuss methods to filter and pare down your results. You will create new tables and be able to move data into them. You will learn common operators and how to combine the data. You will use case statements and concepts like data governance and profiling. You will discuss topics on data, and practice using real-world programming assignments. You will interpret the structure, meaning, and relationships in source data and use SQL as a professional to shape your data for targeted analysis purposes. Although we do not have any specific prerequisites or software requirements to take this course, a simple text editor is recommended for the final project. So what are you waiting for? This is your first step in landing a job in the best occupation in the US and soon the world!Course 2: Data Wrangling, Analysis and AB Testing with SQL- This course allows you to apply the SQL skills taught in “SQL for Data Science” to four increasingly complex and authentic data science inquiry case studies. We'll learn how to convert timestamps of all types to common formats and perform date/time calculations. We'll select and perform the optimal JOIN for a data science inquiry and clean data within an analysis dataset by deduping, running quality checks, backfilling, and handling nulls. We'll learn how to segment and analyze data per segment using windowing functions and use case statements to execute conditional logic to address a data science inquiry. We'll also describe how to convert a query into a scheduled job and how to insert data into a date partition. Finally, given a predictive analysis need, we'll engineer a feature from raw data using the tools and skills we've built over the course. The real-world application of these skills will give you the framework for performing the analysis of an AB test.Course 3: Distributed Computing with Spark SQL- This course is for students with SQL experience and now want to take the next step in gaining familiarity with distributed computing using Spark. Students will gain an understanding of when to use Spark and how Spark as an engine uniquely combines Data and AI technologies at scale. The four modules build on one another and by the end of the course the student will understand: Spark architecture, Spark DataFrame, optimizing reading/writing data, and how to build a machine learning model. The first module will introduce Spark, including how Spark works with distributed computing and what are Spark Dataframes. Module 2 covers the core concepts of Spark such as storage vs. computing, caching, partitions and Spark UI. The third module looks at Engineering Data Pipelines covering connecting to databases, schemas and type, file formats and writing good data. The final module looks at the application of Spark with Machine Learning through the business use case, a short introduction to what machine learning is, building and applying models and a final course conclusion. By understanding when to use Spark, either scaling out when the model or data is too large to process on a single machine, or having a need to simply speed up to get faster results, students will hone their SQL skills and become a more adept Data Scientist.Course 4: SQL for Data Science Capstone Project- Data science is a dynamic and growing career field that demands knowledge and skills-based in SQL to be successful. This course is designed to provide you with a solid foundation in applying SQL skills to analyze data and solve real business problems. Whether you have successfully completed the other courses in the Learn SQL Basics for Data Science Specialization or are taking just this course, this project is your chance to apply the knowledge and skills you have acquired to practice important SQL querying and solve problems with data. You will participate in your own personal or professional journey to create a portfolio-worthy piece from start to finish. You will choose a dataset and develop a project proposal. You will explore your data and perform some initial statistics you have learned through this specialization. You will uncover analytics for qualitative data and consider new metrics that make sense from the patterns that surface in your analysis. You will put all of your work together in the form of a presentation where you will tell the story of your findings. Along the way, you will receive feedback through the peer-review process. This community of fellow learners will provide additional input to help you refine your approach to data analysis with SQL and present your findings to clients and management."
https://www.classcentral.com/course/data-science-engineering-apache-spark-18485,"The Data Science and Engineering with Spark XSeries, created in partnership with Databricks, will teach students how to perform data science and data engineering at scale using Spark, a cluster computing system well-suited for large-scale machine learning tasks. It will also present an integrated view of data processing by highlighting the various components of data analysis pipelines, including exploratory data analysis, feature extraction, supervised learning, and model evaluation. Students will gain hands-on experience building and debugging Spark applications. Internal details of Spark and distributed machine learning algorithms will be covered, which will provide students with intuition about working with big data and developing code for a distributed environment.
This XSeries requires a programming background and experience with Python (or the ability to learn it quickly). All exercises will use PySpark (the Python API for Spark), but previous experience with Spark or distributed computing is NOT required. Familiarity with basic machine learning concepts and exposure to algorithms, probability, linear algebra and calculus are prerequisites for two of the courses in this series.



Courses under this program:Course 1: Big Data Analysis with Apache SparkLearn how to apply data science techniques using parallel programming in Apache Spark to explore big data.Course 2: Distributed Machine Learning with Apache SparkLearn the underlying principles required to develop scalable machine learning pipelines and gain hands-on experience using Apache Spark.Course 3: Introduction to Apache SparkLearn the fundamentals and architecture of Apache Spark, the leading cluster-computing framework among professionals."
https://www.classcentral.com/course/berkeleyx-foundations-of-data-science-18422,"This Professional Certificate gives you a new lens to explore the issues and problems that you care about. You’ll learn how to combine data with Python programming skills to ask questions and explore problems that you encounter in any field of study, in a future job, and even in everyday life.
This program will help you become a data scientist by teaching you how to analyze a diverse array of real data sets including economic data, geographic data and social networks. Typically, the information will be incomplete and there will be some uncertainty involved. You will then study inference, which will help you quantify uncertainty and measure the accuracy of your estimates. Finally, you will put all of your knowledge together and learn about prediction using machine learning.
The program focuses on a set of core concepts and techniques that have broad applicability. Unlike “bootcamps” for programmers, it presents data science as a way of thinking, in which interpretation and communication are as important as computation and statistical methods.
We all have to be able to think critically and make decisions based on data. Thus, the program aims to make data science accessible to everyone.
It is designed specifically for students who have not previously taken statistics or computer science courses. No prior programming experience is needed. The program is based on Data 8, Berkeley’s fastest-growing class, taken by 1,000+ students each semester.
You don’t have to download any software – a browser is all you need. Open up a window and prepare to have some fun.



            Read more
          



Courses under this program:Course 1: Foundations of Data Science: Computational Thinking with PythonLearn the basics of computational thinking, an essential skill in today’s data-driven world, using the popular programming language, Python.Course 2: Foundations of Data Science: Inferential Thinking by ResamplingLearn how to use inferential thinking to make conclusions about unknowns based on data in random samples.Course 3: Foundations of Data Science: Prediction and Machine LearningLearn how to use machine learning, with a focus on regression and classification, to automatically identify patterns in your data and make better predictions."
https://www.classcentral.com/course/usmx-umgc-bioinformatics-18322,"Modern biology generates massive quantities of big data. Hidden in this data might be the next blockbuster cancer therapy, the definitive proof that a certain gene is responsible for a disease, or the information needed to replicate a crucial biological process — and you could be on the team that discovers it.
Bioinformatics blends biology, computer science and mathematics and in this Bioinformatics MicroMasters program you’ll gain the cutting edge knowledge and experience that will give you significant career advantage in this fascinating field.
In this program, you will learn how to analyze DNA sequences to find mutations and anomalies, understand the important role protein structure plays in protein function and use statistical analysis tools, including R programming, to mine biological big data.
This program is ideal for those who want to learn more about the bioinformatics field and its effects on society at large or who would like to incorporate bioinformatics principles and tools into their laboratories.



Courses under this program:Course 1: DNA Sequences: Alignments and Analysis
Learn how to align and analyze DNA sequences using web and software based tools to find mutations and other anomalies in genes and genomic sequences.
Course 2: Proteins: Alignment, Analysis and Structure
Learn about proteins and the important role structure plays in their function as you learn how to analyze and align protein sequences.
Course 3: Statistical Analysis in Bioinformatics
Learn basic R programming to analyze biological big data to locate genes, perform simulations, and gauge the effect of specific markers."
https://www.classcentral.com/course/usmx-umgc-bioinformatics-18322,"Modern biology generates massive quantities of big data. Hidden in this data might be the next blockbuster cancer therapy, the definitive proof that a certain gene is responsible for a disease, or the information needed to replicate a crucial biological process — and you could be on the team that discovers it.
Bioinformatics blends biology, computer science and mathematics and in this Bioinformatics MicroMasters program you’ll gain the cutting edge knowledge and experience that will give you significant career advantage in this fascinating field.
In this program, you will learn how to analyze DNA sequences to find mutations and anomalies, understand the important role protein structure plays in protein function and use statistical analysis tools, including R programming, to mine biological big data.
This program is ideal for those who want to learn more about the bioinformatics field and its effects on society at large or who would like to incorporate bioinformatics principles and tools into their laboratories.



Courses under this program:Course 1: DNA Sequences: Alignments and Analysis
Learn how to align and analyze DNA sequences using web and software based tools to find mutations and other anomalies in genes and genomic sequences.
Course 2: Proteins: Alignment, Analysis and Structure
Learn about proteins and the important role structure plays in their function as you learn how to analyze and align protein sequences.
Course 3: Statistical Analysis in Bioinformatics
Learn basic R programming to analyze biological big data to locate genes, perform simulations, and gauge the effect of specific markers."
https://www.classcentral.com/course/gtx-analytics-essential-tools-and-methods-18323,"Gain an interdisciplinary understanding of the essential fundamentals of analytics, including analysis methods, analytical tools, such as R, Python and SQL, and business applications.
Using common analytics software and tools, statistical and machine learning methods, and data-intensive computing and visualization techniques, learners will gain the experience necessary to integrate all of these parts for maximum impact.
Project experience is also included as part of the MicroMasters program. Through these projects, learners will hone their skills with data collection, storage, analysis, and visualization tools, as well as gain instincts for how and when each tool should be used.
These projects provide hands-on experience with real-world business applications of analytics and a deeper understanding of how to apply analytics skills to make the biggest difference.



Courses under this program:Course 1: Introduction to Analytics Modeling
Learn essential analytics models and methods and how to appropriately apply them, using tools such as R, to retrieve desired insights.
Course 2: Computing for Data Analysis
A hands-on introduction to basic programming principles and practice relevant to modern data analysis, data mining, and machine learning.
Course 3: Data Analytics for Business
This course prepares students to understand business analytics and become leaders in these areas in business organizations."
https://www.classcentral.com/course/bioinformatics-18829,"Join Us in a Top 50 MOOC of All Time!

How do we sequence and compare genomes? How do we identify the genetic basis for disease? How do we construct an evolutionary Tree of Life for all species on Earth?

When you complete this Specialization, you will learn how to answer many questions in modern biology that have become inseparable from the computational approaches used to solve them. You will also obtain a toolkit of existing software resources built on these computational approaches and that are used by thousands of biologists every day in one of the fastest growing fields in science.

Although this Specialization centers on computational topics, you do not need to know how to program in order to complete it. If you are interested in programming, we feature an ""Honors Track"" (called ""hacker track"" in previous runs of the course). The Honors Track allows you to implement the bioinformatics algorithms that you will encounter along the way in dozens of automatically graded coding challenges. By completing the Honors Track, you will be a bioinformatics software professional!

Learn more about the Bioinformatics Specialization (including why we are wearing these crazy outfits) by watching our introductory video.

You can purchase the Specialization's print companion, Bioinformatics Algorithms: An Active Learning Approach, from the textbook website.

Our first course, ""Finding Hidden Messages in DNA"", was named a top-50 MOOC of all time by Class Central!
      


          Course 1: Finding Hidden Messages in DNA (Bioinformatics I)- Named a top 50 MOOC of all time by Class Central! This course begins a series of classes illustrating the power of computing in modern biology. Please join us on the frontier of bioinformatics to look for hidden messages in DNA without ever needing to put on a lab coat. In the first half of the course, we investigate DNA replication, and ask the question, where in the genome does DNA replication begin? We will see that we can answer this question for many bacteria using only some straightforward algorithms to look for hidden messages in the genome. In the second half of the course, we examine a different biological question, when we ask which DNA patterns play the role of molecular clocks. The cells in your body manage to maintain a circadian rhythm, but how is this achieved on the level of DNA? Once again, we will see that by knowing which hidden messages to look for, we can start to understand the amazingly complex language of DNA. Perhaps surprisingly, we will apply randomized algorithms, which roll dice and flip coins in order to solve problems. Finally, you will get your hands dirty and apply existing software tools to find recurring biological motifs within genes that are responsible for helping Mycobacterium tuberculosis go ""dormant"" within a host for many years before causing an active infection.Course 2: Genome Sequencing (Bioinformatics II)- You may have heard a lot about genome sequencing and its potential to usher in an era of personalized medicine, but what does it mean to sequence a genome? Biologists still cannot read the nucleotides of an entire genome as you would read a book from beginning to end. However, they can read short pieces of DNA. In this course, we will see how graph theory can be used to assemble genomes from these short pieces. We will further learn about brute force algorithms and apply them to sequencing mini-proteins called antibiotics. In the first half of the course, we will see that biologists cannot read the 3 billion nucleotides of a human genome as you would read a book from beginning to end. However, they can read shorter fragments of DNA. In this course, we will see how graph theory can be used to assemble genomes from these short pieces in what amounts to the largest jigsaw puzzle ever put together. In the second half of the course, we will discuss antibiotics, a topic of great relevance as antimicrobial-resistant bacteria like MRSA are on the rise. You know antibiotics as drugs, but on the molecular level they are short mini-proteins that have been engineered by bacteria to kill their enemies. Determining the sequence of amino acids making up one of these antibiotics is an important research problem, and one that is similar to that of sequencing a genome by assembling tiny fragments of DNA. We will see how brute force algorithms that try every possible solution are able to identify naturally occurring antibiotics so that they can be synthesized in a lab. Finally, you will learn how to apply popular bioinformatics software tools to sequence the genome of a deadly Staphylococcus bacterium that has acquired antibiotics resistance.Course 3: Comparing Genes, Proteins, and Genomes (Bioinformatics III)- Once we have sequenced genomes in the previous course, we would like to compare them to determine how species have evolved and what makes them different. In the first half of the course, we will compare two short biological sequences, such as genes (i.e., short sequences of DNA) or proteins. We will encounter a powerful algorithmic tool called dynamic programming that will help us determine the number of mutations that have separated the two genes/proteins. In the second half of the course, we will ""zoom out"" to compare entire genomes, where we see large scale mutations called genome rearrangements, seismic events that have heaved around large blocks of DNA over millions of years of evolution. Looking at the human and mouse genomes, we will ask ourselves: just as earthquakes are much more likely to occur along fault lines, are there locations in our genome that are ""fragile"" and more susceptible to be broken as part of genome rearrangements? We will see how combinatorial algorithms will help us answer this question. Finally, you will learn how to apply popular bioinformatics software tools to solve problems in sequence alignment, including BLAST.Course 4: Molecular Evolution (Bioinformatics IV)- In the previous course in the Specialization, we learned how to compare genes, proteins, and genomes. One way we can use these methods is in order to construct a ""Tree of Life"" showing how a large collection of related organisms have evolved over time. In the first half of the course, we will discuss approaches for evolutionary tree construction that have been the subject of some of the most cited scientific papers of all time, and show how they can resolve quandaries from finding the origin of a deadly virus to locating the birthplace of modern humans. In the second half of the course, we will shift gears and examine the old claim that birds evolved from dinosaurs. How can we prove this? In particular, we will examine a result that claimed that peptides harvested from a T. rex fossil closely matched peptides found in chickens. In particular, we will use methods from computational proteomics to ask how we could assess whether this result is valid or due to some form of contamination. Finally, you will learn how to apply popular bioinformatics software tools to reconstruct an evolutionary tree of ebolaviruses and identify the source of the recent Ebola epidemic that caused global headlines.Course 5: Genomic Data Science and Clustering (Bioinformatics V)- How do we infer which genes orchestrate various processes in the cell? How did humans migrate out of Africa and spread around the world? In this class, we will see that these two seemingly different questions can be addressed using similar algorithmic and machine learning techniques arising from the general problem of dividing data points into distinct clusters. In the first half of the course, we will introduce algorithms for clustering a group of objects into a collection of clusters based on their similarity, a classic problem in data science, and see how these algorithms can be applied to gene expression data. In the second half of the course, we will introduce another classic tool in data science called principal components analysis that can be used to preprocess multidimensional data before clustering in an effort to greatly reduce the number dimensions without losing much of the ""signal"" in the data. Finally, you will learn how to apply popular bioinformatics software tools to solve a real problem in clustering.Course 6: Finding Mutations in DNA and Proteins (Bioinformatics VI)- In previous courses in the Specialization, we have discussed how to sequence and compare genomes. This course will cover advanced topics in finding mutations lurking within DNA and proteins. In the first half of the course, we would like to ask how an individual's genome differs from the ""reference genome"" of the species. Our goal is to take small fragments of DNA from the individual and ""map"" them to the reference genome. We will see that the combinatorial pattern matching algorithms solving this problem are elegant and extremely efficient, requiring a surprisingly small amount of runtime and memory. In the second half of the course, we will learn how to identify the function of a protein even if it has been bombarded by so many mutations compared to similar proteins with known functions that it has become barely recognizable. This is the case, for example, in HIV studies, since the virus often mutates so quickly that researchers can struggle to study it. The approach we will use is based on a powerful machine learning tool called a hidden Markov model. Finally, you will learn how to apply popular bioinformatics software tools applying hidden Markov models to compare a protein against a related family of proteins.Course 7: Bioinformatics Capstone: Big Data in Biology- In this course, you will learn how to use the BaseSpace cloud platform developed by Illumina (our industry partner) to apply several standard bioinformatics software approaches to real biological data. In particular, in a series of Application Challenges will see how genome assembly can be used to track the source of a food poisoning outbreak, how RNA-Sequencing can help us analyze gene expression data on the tissue level, and compare the pros and cons of whole genome vs. whole exome sequencing for finding potentially harmful mutations in a human sample. Plus, hacker track students will have the option to build their own genome assembler and apply it to real data!"
https://www.classcentral.com/course/stepik----r-10534,"В рамках данного курса мы подробно разберем все основные этапы анализа данных при помощи R. Слушатели научатся без труда манипулировать данными, используя как стандартные методы R и Rstudio, так и специальные пакеты и библиотеки. Мы выясним, как применять основные методы статистического анализа: t-тест, корреляция, регрессия, дисперсионный и регрессионный анализ и др. Также мы научимся писать собственные функции в R. Особое внимание в курсе будет уделено визуализации получаемых результатов. 



Предобработка данных1.1 Общая информация о курсе1.2 Переменные1.3 Работа с data frame1.4 Элементы синтаксиса1.5 Описательные статистики1.6 Описательные статистики. Графики1.7 Сохранение результатов
Статистика в R. Часть 12.1 Анализ номинативных данных2.2 Сравнение двух групп2.3 Применение дисперсионного анализа2.4 Создание собственных функций
Статистика в R. Часть 23.1 Корреляция и простая линейная регрессия (МНК)3.2 Множественная линейная регрессия3.3 Множественная линейная регрессия. Отбор моделей3.4 Диагностика модели3.5 Диагностика модели. Продолжение3.6 Логистическая регрессия3.7 Экспорт результатов анализа из R3.8 Заключение"
https://www.classcentral.com/course/stepik----r--2-10540,"В первой части курса по анализу данных в R мы познакомились с основными этапами анализа данных в R: предобработка данных, применение статистических тестов, визуализация и презентация результатов анализа. В этом курсе мы углубимся во все вышеперечисленные этапы: научимся быстро и эффективно манипулировать с данными при помощи функций семейства apply и таких пакетов как dplyr и data.table. Мы более подробно обсудим процесс визуализации данных при помощи пакета ggplot2, а также научимся строить интерактивные графики. В последнем модуле курса мы поговорим о работе в R Markdown для создания отчетов о проделанной в R работе.
 



Продвинутая предобработка данных1.1 Общая информация о курсе1.2 Функции семейства apply. Часть 11.3 Функции семейства apply. Часть 21.4 Функции семейства apply. Часть 31.5 Работа с данными при помощи dplyr1.6 Работа с данными при помощи dplyr. Продолжение1.7 Data.table1.8 Data.table. Продолжение1.9 Дополнительные задачи
Подробнее о визуализации2.1 Грамматика ggplot2, функция qplot2.2 Функция ggplot и различные geoms2.3 Facet - способы группировки данных на графике2.4 Scale и Theme: оси, легенда, внешний вид графика2.5 Пример решения практической задачи2.6 Динамическая визуализация с plotly
R Markdown3.1 Здравствуйте, я ваш R Markdown!3.2 Погружаемся в детали: R слева, markdown справа3.3 Зоопарк возможностей: форматы, pandoc, html3.4 Заключение
Практические задачи4.1 Общая информация4.2 Оценка качества модели и интерпретация результатов4.3 Увлекательное путешествие в мир микроволновок4.4 Задачи"
https://www.classcentral.com/course/information-visualization-18898,"This specialization provides learners with the necessary knowledge and practical skills to develop a strong foundation in information visualization and to design and develop advanced applications for visual data analysis. The specialization is characterized by two main complementary features: (1) providing a strong understanding of visual perception and the theory of visual encoding to design and evaluate innovative visualization methods; (2) providing the necessary skills to develop advanced web-based applications for visual data analysis. The specialization is organized around four courses that cover fundamentals, applied perception, advanced visualization method and interactive visualization. The specialization is meant to prepare students to work on complex data science projects that require the development of interactive visual interfaces for data analysis. The courses can also be taken individually to improve relevant skills in visualization. For instance, the course on applied perception provides unique skills to evaluate and design innovative visualization in all sorts of scenarios.



          Course 1: Information Visualization: Foundations- The main goal of this specialization is to provide the knowledge and practical skills necessary to develop a strong foundation on information visualization and to design and develop advanced applications for visual data analysis. This course aims at introducing fundamental knowledge for information visualization. The main goal is to provide the students with the necessary “vocabulary” to describe visualizations in a way that helps them reason about what designs are appropriate for a given problem. This module also gives a broad overview of the field of visualization, introducing its goals, methods and applications. A learner with some or no previous knowledge in Information Visualization will get a sense of what visualization is, what it is for and in how many different situations it can be applied; will practice to describe data in a way that is useful for visualization design; will familiarize with fundamental charts to talk about the concept of visual encoding and decoding.Course 2: Information Visualization: Applied Perception- This module aims at introducing fundamental concepts of visual perception applied to information visualization. These concepts help the student ideate and evaluate visualization designs in terms of how well they leverage the capabilities of the human perceptual machinery.Course 3: Information Visualization: Programming with D3.js- In this course you will learn how to use D3.js to create powerful visualizations for web. Learning D3.js will enable you to create many different types of visualization and to visualize many different data types. It will give you the freedom to create something as simple as a bar chart as well your own new revolutionary technique. In this course we will cover the basics of creating visualizations with D3 as well as how to deal with tabular data, geography and networks. By the end of this course you will be able to: - Create bar and line charts - Create choropleth and symbol maps - Create node-link diagrams and tree maps - Implement zooming and brushing - Link two or more views through interaction The course mixes theoretical and practical lectures. We will show you step by step how to use the library to build actual visualizations and what theoretical concepts lie behind them. Throughout the course you will learn skills that will lead you to building a whole application by the end of the lectures (a fully working visualization system to visualize airlines routes). This course is the third one of the “Specialization in Information Visualization"". The course expects you to have some basic knowledge of programming as well as some basic visualization skills.Course 4: Information Visualization: Advanced Techniques- This course aims to introduce learners to advanced visualization techniques beyond the basic charts covered in Information Visualization: Fundamentals. These techniques are organized around data types to cover advance methods for: temporal and spatial data, networks and trees and textual data. In this module we also teach learners how to develop innovative techniques in D3.js. Learning Goals Goal: Analyze the design space of visualization solutions for various kinds of data visualization problems. Learn what designs are available for a given problem and what are their respective advantages and disadvantages. - Temporal - Spatial - Spatio-Temporal - Networks - Trees - Text This is the fourth course in the Information Visualization Specialization. The course expects you to have some basic knowledge of programming as well as some basic visualization skills (as those introduced in the first course of the specialization)."
https://www.classcentral.com/course/data-analysis-18648,"Learn SAS or Python programming, expand your knowledge of analytical methods and applications, and conduct original research to inform complex decisions. The Data Analysis and Interpretation Specialization takes you from data novice to data expert in just four project-based courses. You will apply basic data science tools, including data management and visualization, modeling, and machine learning using your choice of either SAS or Python, including pandas and Scikit-learn. Throughout the Specialization, you will analyze a research question of your choice and summarize your insights. In the Capstone Project, you will use real data to address an important issue in society, and report your findings in a professional-quality report. You will have the opportunity to work with our industry partners, DRIVENDATA and The Connection. Help DRIVENDATA solve some of the world's biggest social challenges by joining one of their competitions, or help The Connection better understand recidivism risk for people on parole in substance use treatment. Regular feedback from peers will provide you a chance to reshape your question. This Specialization is designed to help you whether you are considering a career in data, work in a context where supervisors are looking to you for data insights, or you just have some burning questions you want to explore. No prior experience is required. By the end you will have mastered statistical methods to conduct original research to inform complex decisions.



            Read more
          



          Course 1: Data Management and Visualization- Whether being used to customize advertising to millions of website visitors or streamline inventory ordering at a small restaurant, data is becoming more integral to success. Too often, we’re not sure how use data to find answers to the questions that will make us more successful in what we do. In this course, you will discover what data is and think about what questions you have that can be answered by the data – even if you’ve never thought about data before. Based on existing data, you will learn to develop a research question, describe the variables and their relationships, calculate basic statistics, and present your results clearly. By the end of the course, you will be able to use powerful data analysis tools – either SAS or Python – to manage and visualize your data, including how to deal with missing data, variable groups, and graphs. Throughout the course, you will share your progress with others to gain valuable feedback, while also learning how your peers use data to answer their own questions.Course 2: Data Analysis Tools- In this course, you will develop and test hypotheses about your data. You will learn a variety of statistical tests, as well as strategies to know how to apply the appropriate one to your specific data and question. Using your choice of two powerful statistical software packages (SAS or Python), you will explore ANOVA, Chi-Square, and Pearson correlation analysis. This course will guide you through basic statistical principles to give you the tools to answer questions you have developed. Throughout the course, you will share your progress with others to gain valuable feedback and provide insight to other learners about their work.Course 3: Regression Modeling in Practice- This course focuses on one of the most important tools in your data analysis arsenal: regression analysis. Using either SAS or Python, you will begin with linear regression and then learn how to adapt when two variables do not present a clear linear relationship. You will examine multiple predictors of your outcome and be able to identify confounding variables, which can tell a more compelling story about your results. You will learn the assumptions underlying regression analysis, how to interpret regression coefficients, and how to use regression diagnostic plots and other tools to evaluate the quality of your regression model. Throughout the course, you will share with others the regression models you have developed and the stories they tell you.Course 4: Machine Learning for Data Analysis- Are you interested in predicting future outcomes using your data? This course helps you do just that! Machine learning is the process of developing, testing, and applying predictive algorithms to achieve this goal. Make sure to familiarize yourself with course 3 of this specialization before diving into these machine learning concepts. Building on Course 3, which introduces students to integral supervised machine learning concepts, this course will provide an overview of many additional concepts, techniques, and algorithms in machine learning, from basic classification to decision trees and clustering. By completing this course, you will learn how to apply, test, and interpret machine learning algorithms as alternative methods for addressing your research questions.Course 5: Data Analysis and Interpretation Capstone- The Capstone project will allow you to continue to apply and refine the data analytic techniques learned from the previous courses in the Specialization to address an important issue in society. You will use real world data to complete a project with our industry and academic partners. For example, you can work with our industry partner, DRIVENDATA, to help them solve some of the world's biggest social challenges! DRIVENDATA at www.drivendata.org, is committed to bringing cutting-edge practices in data science and crowdsourcing to some of the world's biggest social challenges and the organizations taking them on. Or, you can work with our other industry partner, The Connection (www.theconnectioninc.org) to help them better understand recidivism risk for people on parole seeking substance use treatment. For more than 40 years, The Connection has been one of Connecticut’s leading private, nonprofit human service and community development agencies. Each month, thousands of people are assisted by The Connection’s diverse behavioral health, family support and community justice programs. The Connection’s Institute for Innovative Practice was created in 2010 to bridge the gap between researchers and practitioners in the behavioral health and criminal justice fields with the goal of developing maximally effective, evidence-based treatment programs. A major component of the Capstone project is for you to be able to choose the information from your analyses that best conveys results and implications, and to tell a compelling story with this information. By the end of the course, you will have a professional quality report of your findings that can be shown to colleagues and potential employers to demonstrate the skills you learned by completing the Specialization."
https://www.classcentral.com/course/genomic-data-science-18700,"With genomics sparks a revolution in medical discoveries, it becomes imperative to be able to better understand the genome, and be able to leverage the data and information from genomic datasets. Genomic Data Science is the field that applies statistics and data science to the genome. This Specialization covers the concepts and tools to understand, analyze, and interpret data from next generation sequencing experiments. It teaches the most common tools used in genomic data science including how to use the command line, along with a variety of software implementation tools like Python, R, Bioconductor, and Galaxy. This Specialization is designed to serve as both a standalone introduction to genomic data science or as a perfect compliment to a primary degree or postdoc in biology, molecular biology, or genetics, for scientists in these fields seeking to gain familiarity in data science and statistical tools to better interact with the data in their everyday work. To audit Genomic Data Science courses for free, visit https://www.coursera.org/jhu, click the course, click Enroll, and select Audit. Please note that you will not receive a Certificate of Completion if you choose to Audit.



          Course 1: Introduction to Genomic Technologies- This course introduces you to the basic biology of modern genomics and the experimental tools that we use to measure it. We'll introduce the Central Dogma of Molecular Biology and cover how next-generation sequencing can be used to measure DNA, RNA, and epigenetic patterns. You'll also get an introduction to the key concepts in computing and data science that you'll need to understand how data from next-generation sequencing experiments are generated and analyzed. This is the first course in the Genomic Data Science Specialization.Course 2: Genomic Data Science with Galaxy- Learn to use the tools that are available from the Galaxy Project. This is the second course in the Genomic Big Data Science Specialization.Course 3: Python for Genomic Data Science- This class provides an introduction to the Python programming language and the iPython notebook. This is the third course in the Genomic Big Data Science Specialization from Johns Hopkins University.Course 4: Algorithms for DNA Sequencing- We will learn computational methods -- algorithms and data structures -- for analyzing DNA sequencing data. We will learn a little about DNA, genomics, and how DNA sequencing is used. We will use Python to implement key algorithms and data structures and to analyze real genomes and DNA sequencing datasets.Course 5: Command Line Tools for Genomic Data Science- Introduces to the commands that you need to manage and analyze directories, files, and large sets of genomic data. This is the fourth course in the Genomic Big Data Science Specialization from Johns Hopkins University.Course 6: Bioconductor for Genomic Data Science- Learn to use tools from the Bioconductor project to perform analysis of genomic data. This is the fifth course in the Genomic Big Data Specialization from Johns Hopkins University.Course 7: Statistics for Genomic Data Science- An introduction to the statistics behind the most popular genomic data science projects. This is the sixth course in the Genomic Big Data Science Specialization from Johns Hopkins University.Course 8: Genomic Data Science Capstone- In this culminating project, you will deploy the tools and techniques that you've mastered over the course of the specialization. You'll work with a real data set to perform analyses and prepare a report of your findings."
https://www.classcentral.com/course/practical-data-science-matlab-18864,"Do you find yourself in an industry or field that increasingly uses data to answer questions? Are you working with an overwhelming amount of data and need to make sense of it? Do you want to avoid becoming a full-time software developer or statistician to do meaningful tasks with your data?

Completing this specialization will give you the skills and confidence you need to achieve practical results in Data Science quickly. Being able to visualize, analyze, and model data are some of the most in-demand career skills from fields ranging from healthcare, to the auto industry, to tech startups.

This specialization assumes you have domain expertise in a technical field and some exposure to computational tools, such as spreadsheets. To be successful in completing the courses, you should have some background in basic statistics (histograms, averages, standard deviation, curve fitting, interpolation).

Throughout this specialization, you will be using MATLAB. MATLAB is the go-to choice for millions of people working in engineering and science, and provides the capabilities you need to accomplish your data science tasks. You will be provided with free access to MATLAB for the duration of the specialization to complete your work.
      


          Course 1: Exploratory Data Analysis with MATLAB- In this course, you will learn to think like a data scientist and ask questions of your data. You will use interactive features in MATLAB to extract subsets of data and to compute statistics on groups of related data. You will learn to use MATLAB to automatically generate code so you can learn syntax as you explore. You will also use interactive documents, called live scripts, to capture the steps of your analysis, communicate the results, and provide interactive controls allowing others to experiment by selecting groups of data. These skills are valuable for those who have domain knowledge and some exposure to computational tools, but no programming background is required. To be successful in this course, you should have some knowledge of basic statistics (e.g., histograms, averages, standard deviation, curve fitting, interpolation). By the end of this course, you will be able to load data into MATLAB, prepare it for analysis, visualize it, perform basic computations, and communicate your results to others. In your last assignment, you will combine these skills to assess damages following a severe weather event and communicate a polished recommendation based on your analysis of the data. You will be able to visualize the location of these events on a geographic map and create sliding controls allowing you to quickly visualize how a phenomenon changes over time.Course 2: Data Processing and Feature Engineering with MATLAB- In this course, you will build on the skills learned in Exploratory Data Analysis with MATLAB to lay the foundation required for predictive modeling. This intermediate-level course is useful to anyone who needs to combine data from multiple sources or times and has an interest in modeling. These skills are valuable for those who have domain knowledge and some exposure to computational tools, but no programming background. To be successful in this course, you should have some background in basic statistics (histograms, averages, standard deviation, curve fitting, interpolation) and have completed Exploratory Data Analysis with MATLAB. Throughout the course, you will merge data from different data sets and handle common scenarios, such as missing data. In the last module of the course, you will explore special techniques for handling textual, audio, and image data, which are common in data science and more advanced modeling. By the end of this course, you will learn how to visualize your data, clean it up and arrange it for analysis, and identify the qualities necessary to answer your questions. You will be able to visualize the distribution of your data and use visual inspection to address artifacts that affect accurate modeling.Course 3: Predictive Modeling and Machine Learning with MATLAB- In this course, you will build on the skills learned in Exploratory Data Analysis with MATLAB and Data Processing and Feature Engineering with MATLAB to increase your ability to harness the power of MATLAB to analyze data relevant to the work you do. These skills are valuable for those who have domain knowledge and some exposure to computational tools, but no programming background. To be successful in this course, you should have some background in basic statistics (histograms, averages, standard deviation, curve fitting, interpolation) and have completed courses 1 through 2 of this specialization. By the end of this course, you will use MATLAB to identify the best machine learning model for obtaining answers from your data. You will prepare your data, train a predictive model, evaluate and improve your model, and understand how to get the most out of your models.Course 4: Data Science Project: MATLAB for the Real World- Like most subjects, practice makes perfect in Data Science. In the capstone project, you will apply the skills learned across courses in the Practical Data Science with MATLAB specialization to explore, process, analyze, and model data. You will choose your own pathway to answer key questions with the provided data. To complete the project, you must have mastery of the skills covered in other courses in the specialization. The project will test your ability to import and explore your data, prepare the data for analysis, train a predictive model, evaluate and improve your model, and communicate your results."
https://www.classcentral.com/course/big-data-18552,"Drive better business decisions with an overview of how big data is organized, analyzed, and interpreted. Apply your insights to real-world problems and questions. ********* Do you need to understand big data and how it will impact your business? This Specialization is for you. You will gain an understanding of what insights big data can provide through hands-on experience with the tools and systems used by big data scientists and engineers. Previous programming experience is not required! You will be guided through the basics of using Hadoop with MapReduce, Spark, Pig and Hive. By following along with provided code, you will experience how one can perform predictive modeling and leverage graph analytics to model problems. This specialization will prepare you to ask the right questions about data, communicate effectively with data scientists, and do basic exploration of large, complex datasets. In the final Capstone Project, developed in partnership with data software company Splunk, you’ll apply the skills you learned to do basic analyses of big data.



          Course 1: Introduction to Big Data- Interested in increasing your knowledge of the Big Data landscape? This course is for those new to data science and interested in understanding why the Big Data Era has come to be. It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems. It is for those who want to start thinking about how Big Data might be useful in their business or career. It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world! At the end of this course, you will be able to: * Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. * Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting. * Get value out of Big Data by using a 5-step process to structure your analysis. * Identify what are and what are not big data problems and be able to recast big data problems as data science questions. * Provide an explanation of the architectural components and programming models used for scalable big data analysis. * Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model. * Install and run a program using Hadoop! This course is for those new to data science. No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments. Hardware Requirements: (A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. Software Requirements: This course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.Course 2: Big Data Modeling and Management Systems- Once you’ve identified a big data issue to analyze, how do you collect, store and organize your data using Big Data solutions? In this course, you will experience various data genres and management tools appropriate for each. You will be able to describe the reasons behind the evolving plethora of new big data platforms from the perspective of big data management systems and analytical tools. Through guided hands-on tutorials, you will become familiar with techniques using real-time and semi-structured data examples. Systems and tools discussed include: AsterixDB, HP Vertica, Impala, Neo4j, Redis, SparkSQL. This course provides techniques to extract value from existing untapped data sources and discovering new data sources. At the end of this course, you will be able to: * Recognize different data elements in your own work and in everyday life problems * Explain why your team needs to design a Big Data Infrastructure Plan and Information System Design * Identify the frequent data operations required for various types of data * Select a data model to suit the characteristics of your data * Apply techniques to handle streaming data * Differentiate between a traditional Database Management System and a Big Data Management System * Appreciate why there are so many data management systems * Design a big data information system for an online game company This course is for those new to data science. Completion of Intro to Big Data is recommended. No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments. Refer to the specialization technical requirements for complete hardware and software specifications. Hardware Requirements: (A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. Software Requirements: This course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.Course 3: Big Data Integration and Processing- At the end of the course, you will be able to: *Retrieve data from example database and big data management systems *Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications *Identify when a big data problem needs data integration *Execute simple big data integration and processing on Hadoop and Spark platforms This course is for those new to data science. Completion of Intro to Big Data is recommended. No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments. Refer to the specialization technical requirements for complete hardware and software specifications. Hardware Requirements: (A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. Software Requirements: This course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.Course 4: Machine Learning With Big Data- Want to make sense of the volumes of data you have collected? Need to incorporate data-driven decisions into your process? This course provides an overview of machine learning techniques to explore, analyze, and leverage data. You will be introduced to tools and algorithms you can use to create machine learning models that learn from data, and to scale those models up to big data problems. At the end of the course, you will be able to: • Design an approach to leverage data using the steps in the machine learning process. • Apply machine learning techniques to explore and prepare data for modeling. • Identify the type of machine learning problem in order to apply the appropriate set of techniques. • Construct models that learn from data using widely available open source tools. • Analyze big data problems using scalable machine learning algorithms on Spark. Software Requirements: Cloudera VM, KNIME, SparkCourse 5: Graph Analytics for Big Data- Want to understand your data network structure and how it changes under different conditions? Curious to know how to identify closely interacting clusters within a graph? Have you heard of the fast-growing area of graph analytics and want to learn more? This course gives you a broad overview of the field of graph analytics so you can learn new ways to model, store, retrieve and analyze graph-structured data. After completing this course, you will be able to model a problem into a graph database and perform analytical tasks over the graph in a scalable manner. Better yet, you will be able to apply these techniques to understand the significance of your data sets for your own projects.Course 6: Big Data - Capstone Project- Welcome to the Capstone Project for Big Data! In this culminating project, you will build a big data ecosystem using tools and methods form the earlier courses in this specialization. You will analyze a data set simulating big data generated from a large number of users who are playing our imaginary game ""Catch the Pink Flamingo"". During the five week Capstone Project, you will walk through the typical big data science steps for acquiring, exploring, preparing, analyzing, and reporting. In the first two weeks, we will introduce you to the data set and guide you through some exploratory analysis using tools such as Splunk and Open Office. Then we will move into more challenging big data problems requiring the more advanced tools you have learned including KNIME, Spark's MLLib and Gephi. Finally, during the fifth and final week, we will show you how to bring it all together to create engaging and compelling reports and slide presentations. As a result of our collaboration with Splunk, a software company focus on analyzing machine-generated big data, learners with the top projects will be eligible to present to Splunk and meet Splunk recruiters and engineering leadership."
https://www.classcentral.com/course/ibm-data-science-18796,"Data Science has been ranked as one of the hottest professions and the demand for data practitioners is booming. This Professional Certificate from IBM is intended for anyone interested in developing skills and experience to pursue a career in Data Science or Machine Learning. This program consists of 9 courses providing you with latest job-ready skills and techniques covering a wide array of data science topics including: open source tools and libraries, methodologies, Python, databases, SQL, data visualization, data analysis, and machine learning. You will practice hands-on in the IBM Cloud using real data science tools and real-world data sets. It is a myth that to become a data scientist you need a Ph.D. This Professional Certificate is suitable for anyone who has some computer skills and a passion for self-learning. No prior computer science or programming knowledge is necessary. We start small, re-enforce applied learning, and build up to more complex topics. Upon successfully completing these courses you will have done several hands-on assignments and built a portfolio of data science projects to provide you with the confidence to plunge into an exciting profession in Data Science. In addition to earning a Professional Certificate from Coursera, you will also receive a digital Badge from IBM recognizing your proficiency in Data Science. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.



          Course 1: What is Data Science? - The art of uncovering the insights and trends in data has been around since ancient times. The ancient Egyptians used census data to increase efficiency in tax collection and they accurately predicted the flooding of the Nile river every year. Since then, people working in data science have carved out a unique and distinct field for the work they do. This field is data science. In this course, we will meet some data science practitioners and we will get an overview of what data science is today. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 2: Open Source tools for Data Science- What are some of the most popular data science tools, how do you use them, and what are their features? In this course, you'll learn about Jupyter Notebooks, RStudio IDE, Apache Zeppelin and Data Science Experience. You will learn about what each tool is used for, what programming languages they can execute, their features and limitations. With the tools hosted in the cloud on Cognitive Class Labs, you will be able to test each tool and follow instructions to run simple code in Python, R or Scala. To end the course, you will create a final project with a Jupyter Notebook on IBM Data Science Experience and demonstrate your proficiency preparing a notebook, writing Markdown, and sharing your work with your peers. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 3: Data Science Methodology- Despite the recent increase in computing power and access to data over the last couple of decades, our ability to use the data within the decision making process is either lost or not maximized at all too often, we don't have a solid understanding of the questions being asked and how to apply the data correctly to the problem at hand. This course has one purpose, and that is to share a methodology that can be used within data science, to ensure that the data used in problem solving is relevant and properly manipulated to address the question at hand. Accordingly, in this course, you will learn: - The major steps involved in tackling a data science problem. - The major steps involved in practicing data science, from forming a concrete business or research problem, to collecting and analyzing data, to building a model, and understanding the feedback after model deployment. - How data scientists think! LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 4: Python for Data Science and AI- This introduction to Python will kickstart your learning of Python for data science, as well as programming in general. This beginner-friendly Python course will take you from zero to programming in Python in a matter of hours. Module 1 - Python Basics o Your first program o Types o Expressions and Variables o String Operations Module 2 - Python Data Structures o Lists and Tuples o Sets o Dictionaries Module 3 - PythonProgramming Fundamentals o Conditions and Branching o Loops o Functions o Objects and Classes Module 4 - Working with Data in Python o Reading files with open o Writing files with open o Loading data with Pandas o Numpy Finally, you will create a project to test your skills.Course 5: Databases and SQL for Data Science- Much of the world's data resides in databases. SQL (or Structured Query Language) is a powerful language which is used for communicating with and extracting data from databases. A working knowledge of databases and SQL is a must if you want to become a data scientist. The purpose of this course is to introduce relational database concepts and help you learn and apply foundational knowledge of the SQL language. It is also intended to get you started with performing SQL access in a data science environment. The emphasis in this course is on hands-on and practical learning . As such, you will work with real databases, real data science tools, and real-world datasets. You will create a database instance in the cloud. Through a series of hands-on labs you will practice building and running SQL queries. You will also learn how to access databases from Jupyter notebooks using SQL and Python. No prior knowledge of databases, SQL, Python, or programming is required. Anyone can audit this course at no-charge. If you choose to take this course and earn the Coursera course certificate, you can also earn an IBM digital badge upon successful completion of the course. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 6: Data Analysis with Python- Learn how to analyze data using Python. This course will take you from the basics of Python to exploring many different types of data. You will learn how to prepare data for analysis, perform simple statistical analysis, create meaningful data visualizations, predict future trends from data, and more! Topics covered: 1) Importing Datasets 2) Cleaning the Data 3) Data frame manipulation 4) Summarizing the Data 5) Building machine learning Regression models 6) Building data pipelines Data Analysis with Python will be delivered through lecture, lab, and assignments. It includes following parts: Data Analysis libraries: will learn to use Pandas, Numpy and Scipy libraries to work with a sample dataset. We will introduce you to pandas, an open-source library, and we will use it to load, manipulate, analyze, and visualize cool datasets. Then we will introduce you to another open-source library, scikit-learn, and we will use some of its machine learning algorithms to build smart models and make cool predictions. If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 7: Data Visualization with Python- ""A picture is worth a thousand words"". We are all familiar with this expression. It especially applies when trying to explain the insight obtained from the analysis of increasingly large datasets. Data visualization plays an essential role in the representation of both small and large-scale data. One of the key skills of a data scientist is the ability to tell a compelling story, visualizing data and findings in an approachable and stimulating way. Learning how to leverage a software tool to visualize data will also enable you to extract information, better understand the data, and make more effective decisions. The main goal of this Data Visualization with Python course is to teach you how to take data that at first glance has little meaning and present that data in a form that makes sense to people. Various techniques have been developed for presenting data visually but in this course, we will be using several data visualization libraries in Python, namely Matplotlib, Seaborn, and Folium. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate.Course 8: Machine Learning with Python- This course dives into the basics of machine learning using an approachable, and well-known programming language, Python. In this course, we will be reviewing two main components: First, you will be learning about the purpose of Machine Learning and where it applies to the real world. Second, you will get a general overview of Machine Learning topics such as supervised vs unsupervised learning, model evaluation, and Machine Learning algorithms. In this course, you practice with real-life examples of Machine learning and see how it affects society in ways you may not have guessed! By just putting in a few hours a week for the next few weeks, this is what you’ll get. 1) New skills to add to your resume, such as regression, classification, clustering, sci-kit learn and SciPy 2) New projects that you can add to your portfolio, including cancer detection, predicting economic trends, predicting customer churn, recommendation engines, and many more. 3) And a certificate in machine learning to prove your competency, and share it anywhere you like online or offline, such as LinkedIn profiles and social media. If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge upon successful completion of the course.Course 9: Applied Data Science Capstone- This capstone project course will give you a taste of what data scientists go through in real life when working with data. You will learn about location data and different location data providers, such as Foursquare. You will learn how to make RESTful API calls to the Foursquare API to retrieve data about venues in different neighborhoods around the world. You will also learn how to be creative in situations where data are not readily available by scraping web data and parsing HTML code. You will utilize Python and its pandas library to manipulate data, which will help you refine your skills for exploring and analyzing data. Finally, you will be required to use the Folium library to great maps of geospatial data and to communicate your results and findings. If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge upon successful completion of the course. LIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate."
https://www.classcentral.com/course/swayam-scalable-data-science-14279,"Consider the following example problems: One is interested in computing summary statistics (word count distributions) for a set of words which occur in the same document in entire Wikipedia collection (5 million documents). Naive techniques, will run out of main memory on most computers. One needs to train an SVM classifier for text categorization, with unigram features (typically ~10 million) for hundreds of classes. One would run out of main memory, if they store uncompressed model parameters in main memory. One is interested in learning either a supervised model or find unsupervised patterns, but the data is distributed over multiple machines. Communication being the bottleneck, naïve methods to adapt existing algorithms to such a distributed setting might perform extremely poorly. In all the above situations, a simple data mining / machine learning task has been made more complicated due to large scale of input data, output results or both. In this course, we discuss algorithmic techniques as well as software paradigms which allow one to develop scalable algorithms and systems for the common data science tasks.INTENDED AUDIENCE : Computer Science and EngineeringPREREQUISITES : Algorithms, Machine LearningINDUSTRY SUPPORT : Google, Microsoft, Facebook, Amazon, Flipkart, LinkedIn etc. 
      


COURSE LAYOUT Week 1 : Background: Introduction (30 mins) Probability: Concentration inequalities, (30 mins) Linear algebra: PCA, SVD (30 mins) Optimization: Basics, Convex, GD. (30 mins) Machine Learning: Supervised, generalization, feature learning, clustering. (30 mins)Week 2 : Memory-efficient data structures: Hash functions, universal / perfect hash families (30 min) Bloom filters (30 mins) Sketches for distinct count (1 hr) Misra-Gries sketch. (30 min)Week 3 : Memory-efficient data structures (contd.): Count Sketch, Count-Min Sketch (1 hr) Approximate near neighbors search: Introduction, kd-trees etc (30 mins) LSH families, MinHash for Jaccard, SimHash for L2 (1 hr)Week 4 : Approximate near neighbors search: Extensions e.g. multi-probe, b-bit hashing, Data dependent variants (1.5 hr) Randomized Numerical Linear Algebra Random projection (1 hr)Week 5 : Randomized Numerical Linear Algebra CUR Decomposition (1 hr) Sparse RP, Subspace RP, Kitchen Sink (1.5 hr)Week 6 : Map-reduce and related paradigms Map reduce - Programming examples - (page rank, k-means, matrix multiplication) (1 hr) Big data: computation goes to data. + Hadoop ecosystem (1.5 hrs)Week 7 : Map-reduce and related paradigms (Contd.) Scala + Spark (1 hr) Distributed Machine Learning and Optimization: Introduction (30 mins) SGD + Proof (1 hr)Week 8 : Distributed Machine Learning and Optimization: ADMM + applications (1 hr) Clustering (1 hr) Conclusion (30 mins)"
https://www.classcentral.com/course/mitx-statistics-and-data-science-18342,"Demand for professionals skilled in data, analytics, and machine learning is exploding. A recent report by IBM and Burning Glass states that there will be 364K new job openings in data-driven professions by 2020 in the US. Data scientists bring value to organizations across industries because they are able to solve complex challenges with data and drive important decision-making processes. 39% of the most rigorous data science positions require a degree higher than a bachelor’s.
This MicroMasters program in Statistics and Data Science is comprised of four online courses and a virtually proctored exam that will provide you with the foundational knowledge essential to understanding the methods and tools used in data science, and hands-on training in data analysis and machine learning. You will dive into the fundamentals of probability and statistics, as well as learn, implement, and experiment with data analysis techniques and machine learning algorithms. This program will prepare you to become an informed and effective practitioner of data science who adds value to an organization. The program certificate can be applied, for admitted students, towards a PhD in Social and Engineering Systems (SES) through the MIT Institute for Data, Systems, and Society (IDSS) or may accelerate your path towards a Master’s degree at other universities around the world.
Anyone can enroll in this MicroMasters program. It is designed for learners that want to acquire sophisticated and rigorous training in data science without leaving their day job but without compromising quality. There is no application process but college-level calculus and comfort with mathematical reasoning and Python programming are highly recommended if you want to excel. All the courses are taught by MIT faculty at a similar pace and level of rigor as an on-campus course at MIT. This program brings MIT’s rigorous, high-quality curricula and hands-on learning approach to learners around the world – at scale.
For more detail on this program and credit pathways, please visit https://micromasters.mit.edu/ds/



            Read more
          



Courses under this program:Course 1: Probability - The Science of Uncertainty and Data
Build foundational knowledge of data science with this introduction to probabilistic models, including random processes and the basic elements of statistical inference -- Course 1 of 4 in the MITx MicroMasters program in Statistics and Data Science.
Course 2: Data Analysis in Social Science—Assessing Your KnowledgeLearn the methods for harnessing and analyzing data to answer questions of cultural, social, economic, and policy interest, and then assess that knowledge— Course 2 of 4 in the MITx MicroMasters program in Statistics and Data Science.Course 3: Fundamentals of Statistics
Develop a deep understanding of the principles that underpin statistical inference: estimation, hypothesis testing and prediction. -- Course 3 of 4 in the MITx MicroMasters program in Statistics and Data Science.
Course 4: Machine Learning with Python: from Linear Models to Deep Learning
An in-depth introduction to the field of machine learning, from linear models to deep learning and reinforcement learning, through hands-on Python projects. -- Course 4 of 4 in the MITx MicroMasters program in Statistics and Data Science.
Course 5: Capstone Exam in Statistics and Data Science
Solidify and demonstrate your knowledge and abilities in probability, data analysis, statistics, and machine learning in this culminating assessment. -- Final Requirement of the MITx MicroMasters Program in Statistics and Data Science."
https://www.classcentral.com/course/data-scientist-nanodegree--nd025-18219,"Data science skills are in high demand, and you'll finish this program with practical skills needed to land a data science job. You'll build projects designed with our industry partners using real-world data, and by the end of the program you will be able to build machine learning models includng supervised and unsupervised methods; create and run data pipelines; design experiments; build recommendation systems; deploy solutions to the cloud; and more. This program is an ideal way to move into a data science career, and by the end of the program you'll be ready to apply for data science jobs.Build effective machine learning models, run data pipelines, build recommendation systems, and deploy solutions to the cloud with industry-aligned projects.



Prerequisite Knowledge
We recommend students are familiar with machine learning concepts, like those in the Intro to Machine Learning Nanodegree Program. In addition, students should be familiar with Python programming, probability, and statistics. See detailed requirements.


Solving Data Science Problems
Learn the data science process, including how to build effective data visualizations, and how to communicate with various stakeholders.
Write a Data Science Blog Post


Software Engineering for Data Scientists
Develop software engineering skills that are essential for data scientists, such as creating unit tests and building classes.


Data Engineering for Data Scientists
Learn to work with data through the entire data science process, from running pipelines, transforming data, building models, and deploying solutions to the cloud.
Build Disaster Response Pipelines with Figure Eight


Experiment Design and Recommendations
Learn to design experiments and analyze A/B test results. Explore approaches for building recommendation systems.
Design a Recommendation Engine with IBM


Data Science Projects
Leverage what you’ve learned throughout the program to build your own open-ended Data Science project. This project will serve as a demonstration of your valuable abilities as a Data Scientist.
Data Science Capstone Project"
https://www.classcentral.com/course/clinical-data-science-18836,"Are you interested in how to use data generated by doctors, nurses, and the healthcare system to improve the care of future patients? If so, you may be a future clinical data scientist! This specialization provides learners with hands on experience in use of electronic health records and informatics tools to perform clinical data science. This series of six courses is designed to augment learner’s existing skills in statistics and programming to provide examples of specific challenges, tools, and appropriate interpretations of clinical data. By completing this specialization you will know how to: 1) understand electronic health record data types and structures, 2) deploy basic informatics methodologies on clinical data, 3) provide appropriate clinical and scientific interpretation of applied analyses, and 4) anticipate barriers in implementing informatics tools into complex clinical settings. You will demonstrate your mastery of these skills by completing practical application projects using real clinical data. This specialization is supported by our industry partnership with Google Cloud. Thanks to this support, all learners will have access to a fully hosted online data science computational environment for free! Please note that you must have access to a Google account (i.e., gmail account) to access the clinical data and computational environment.



          Course 1: Introduction to Clinical Data Science- This course will prepare you to complete all parts of the Clinical Data Science Specialization. In this course you will learn how clinical data are generated, the format of these data, and the ethical and legal restrictions on these data. You will also learn enough SQL and R programming skills to be able to complete the entire Specialization - even if you are a beginner programmer. While you are taking this course you will have access to an actual clinical data set and a free, online computational environment for data science hosted by our Industry Partner Google Cloud. At the end of this course you will be prepared to embark on your clinical data science education journey, learning how to take data created by the healthcare system and improve the health of tomorrow's patients.Course 2: Clinical Data Models and Data Quality Assessments- This course aims to teach the concepts of clinical data models and common data models. Upon completion of this course, learners will be able to interpret and evaluate data model designs using Entity-Relationship Diagrams (ERDs), differentiate between data models and articulate how each are used to support clinical care and data science, and create SQL statements in Google BigQuery to query the MIMIC3 clinical data model and the OMOP common data model.Course 3: Identifying Patient Populations- This course teaches you the fundamentals of computational phenotyping, a biomedical informatics method for identifying patient populations. In this course you will learn how different clinical data types perform when trying to identify patients with a particular disease or trait. You will also learn how to program different data manipulations and combinations to increase the complexity and improve the performance of your algorithms. Finally, you will have a chance to put your skills to the test with a real-world practical application where you develop a computational phenotyping algorithm to identify patients who have hypertension. You will complete this work using a real clinical data set while using a free, online computational environment for data science hosted by our Industry Partner Google Cloud.Course 4: Clinical Natural Language Processing- This course teaches you the fundamentals of clinical natural language processing (NLP). In this course you will learn the basic linguistic principals underlying NLP, as well as how to write regular expressions and handle text data in R. You will also learn practical techniques for text processing to be able to extract information from clinical notes. Finally, you will have a chance to put your skills to the test with a real-world practical application where you develop text processing algorithms to identify diabetic complications from clinical notes. You will complete this work using a free, online computational environment for data science hosted by our Industry Partner Google Cloud.Course 5: Predictive Modeling and Transforming Clinical Practice- This course teaches you the fundamentals of transforming clinical practice using predictive models. This course examines specific challenges and methods of clinical implementation, that clinical data scientists must be aware of when developing their predictive models.Course 6: Advanced Clinical Data Science- This course prepares you to deal with advanced clinical data science topics and techniques including temporal and research quality analysis."
https://www.classcentral.com/course/data-science-18849,"Learn scalable data management, evaluate big data technologies, and design effective visualizations.

This Specialization covers intermediate topics in data science. You will gain hands-on experience with scalable SQL and NoSQL data management solutions, data mining algorithms, and practical statistical and machine learning concepts. You will also learn to visualize data and communicate results, and you’ll explore legal and ethical issues that arise in working with big data. In the final Capstone Project, developed in partnership with the digital internship platform Coursolve, you’ll apply your new skills to a real-world data science project.
      


          Course 1: Data Manipulation at Scale: Systems and Algorithms- Data analysis has replaced data acquisition as the bottleneck to evidence-based decision making --- we are drowning in it. Extracting knowledge from large, heterogeneous, and noisy datasets requires not only powerful computing resources, but the programming abstractions to use them effectively. The abstractions that emerged in the last decade blend ideas from parallel databases, distributed systems, and programming languages to create a new class of scalable data analytics platforms that form the foundation for data science at realistic scales. In this course, you will learn the landscape of relevant systems, the principles on which they rely, their tradeoffs, and how to evaluate their utility against your requirements. You will learn how practical systems were derived from the frontier of research in computer science and what systems are coming on the horizon. Cloud computing, SQL and NoSQL databases, MapReduce and the ecosystem it spawned, Spark and its contemporaries, and specialized systems for graphs and arrays will be covered. You will also learn the history and context of data science, the skills, challenges, and methodologies the term implies, and how to structure a data science project. At the end of this course, you will be able to: Learning Goals: 1. Describe common patterns, challenges, and approaches associated with data science projects, and what makes them different from projects in related fields. 2. Identify and use the programming models associated with scalable data manipulation, including relational algebra, mapreduce, and other data flow models. 3. Use database technology adapted for large-scale analytics, including the concepts driving parallel databases, parallel query processing, and in-database analytics 4. Evaluate key-value stores and NoSQL systems, describe their tradeoffs with comparable systems, the details of important examples in the space, and future trends. 5. “Think” in MapReduce to effectively write algorithms for systems including Hadoop and Spark. You will understand their limitations, design details, their relationship to databases, and their associated ecosystem of algorithms, extensions, and languages. write programs in Spark 6. Describe the landscape of specialized Big Data systems for graphs, arrays, and streamsCourse 2: Practical Predictive Analytics: Models and Methods- Statistical experiment design and analytics are at the heart of data science. In this course you will design statistical experiments and analyze the results using modern methods. You will also explore the common pitfalls in interpreting statistical arguments, especially those associated with big data. Collectively, this course will help you internalize a core set of practical and effective machine learning methods and concepts, and apply them to solve some real world problems. Learning Goals: After completing this course, you will be able to: 1. Design effective experiments and analyze the results 2. Use resampling methods to make clear and bulletproof statistical arguments without invoking esoteric notation 3. Explain and apply a core set of classification methods of increasing complexity (rules, trees, random forests), and associated optimization methods (gradient descent and variants) 4. Explain and apply a set of unsupervised learning concepts and methods 5. Describe the common idioms of large-scale graph analytics, including structural query, traversals and recursive queries, PageRank, and community detectionCourse 3: Communicating Data Science Results- Important note: The second assignment in this course covers the topic of Graph Analysis in the Cloud, in which you will use Elastic MapReduce and the Pig language to perform graph analysis over a moderately large dataset, about 600GB. In order to complete this assignment, you will need to make use of Amazon Web Services (AWS). Amazon has generously offered to provide up to $50 in free AWS credit to each learner in this course to allow you to complete the assignment. Further details regarding the process of receiving this credit are available in the welcome message for the course, as well as in the assignment itself. Please note that Amazon, University of Washington, and Coursera cannot reimburse you for any charges if you exhaust your credit. While we believe that this assignment contributes an excellent learning experience in this course, we understand that some learners may be unable or unwilling to use AWS. We are unable to issue Course Certificates for learners who do not complete the assignment that requires use of AWS. As such, you should not pay for a Course Certificate in Communicating Data Results if you are unable or unwilling to use AWS, as you will not be able to successfully complete the course without doing so. Making predictions is not enough! Effective data scientists know how to explain and interpret their results, and communicate findings accurately to stakeholders to inform business decisions. Visualization is the field of research in computer science that studies effective communication of quantitative results by linking perception, cognition, and algorithms to exploit the enormous bandwidth of the human visual cortex. In this course you will learn to recognize, design, and use effective visualizations. Just because you can make a prediction and convince others to act on it doesn’t mean you should. In this course you will explore the ethical considerations around big data and how these considerations are beginning to influence policy and practice. You will learn the foundational limitations of using technology to protect privacy and the codes of conduct emerging to guide the behavior of data scientists. You will also learn the importance of reproducibility in data science and how the commercial cloud can help support reproducible research even for experiments involving massive datasets, complex computational infrastructures, or both. Learning Goals: After completing this course, you will be able to: 1. Design and critique visualizations 2. Explain the state-of-the-art in privacy, ethics, governance around big data and data science 3. Use cloud computing to analyze large datasets in a reproducible way.Course 4: Data Science at Scale - Capstone Project- In the capstone, students will engage on a real world project requiring them to apply skills from the entire data science pipeline: preparing, organizing, and transforming data, constructing a model, and evaluating results. Through a collaboration with Coursolve, each Capstone project is associated with partner stakeholders who have a vested interest in your results and are eager to deploy them in practice. These projects will not be straightforward and the outcome is not prescribed -- you will need to tolerate ambiguity and negative results! But we believe the experience will be rewarding and will better prepare you for data science projects in practice."
https://www.classcentral.com/course/plant-bioinformatic-methods-18763,"The past 15 years have been exciting ones in plant biology. Hundreds of plant genomes have been sequenced, RNA-seq has enabled transcriptome-wide expression profiling, and a proliferation of ""-seq""-based methods has permitted protein-protein and protein-DNA interactions to be determined cheaply and in a high-throughput manner. These data sets in turn allow us to generate hypotheses at the click of a mouse or tap of a finger.The Plant Bioinformatics Specialization on Coursera introduces core bioinformatic competencies and resources, such as NCBI's Genbank, Blast, multiple sequence alignments, phylogenetics in Bioinformatic Methods I, followed by protein-protein interaction, structural bioinformatics and RNA-seq analysis in Bioinformatic Methods II. In Plant Bioinformatics we cover 33 plant-specific online tools from genome browsers to transcriptomic data mining to promoter/network analyses and others. Last, a Plant Bioinformatics Capstone uses these tools to hypothesize a biological role for a gene of unknown function, summarized in a written lab report.This specialization is useful to any modern plant molecular biologist wanting to get a feeling for the incredible scope of data available to researchers. A small amount of R programming is introduced in Bioinformatic Methods II, but most of the tools are web applications. It is recommended that you have access to a laptop or desktop computer for running these as they may not work as mobile applications on your phone or tablet.



            Read more
          



          Course 1: Bioinformatic Methods I- Large-scale biology projects such as the sequencing of the human genome and gene expression surveys using RNA-seq, microarrays and other technologies have created a wealth of data for biologists. However, the challenge facing scientists is analyzing and even accessing these data to extract useful information pertaining to the system being studied. This course focuses on employing existing bioinformatic resources – mainly web-based programs and databases – to access the wealth of data to answer questions relevant to the average biologist, and is highly hands-on. Topics covered include multiple sequence alignments, phylogenetics, gene expression data analysis, and protein interaction networks, in two separate parts. The first part, Bioinformatic Methods I (this one), deals with databases, Blast, multiple sequence alignments, phylogenetics, selection analysis and metagenomics. The second part, Bioinformatic Methods II, covers motif searching, protein-protein interactions, structural bioinformatics, gene expression data analysis, and cis-element predictions. This pair of courses is useful to any student considering graduate school in the biological sciences, as well as students considering molecular medicine. Both provide an overview of the many different bioinformatic tools that are out there. These courses are based on one taught at the University of Toronto to upper-level undergraduates who have some understanding of basic molecular biology. If you're not familiar with this, something like https://learn.saylor.org/course/bio101 might be helpful. No programming is required for this course. Bioinformatic Methods I is regularly updated, and was last updated for January 2019.Course 2: Bioinformatic Methods II- Large-scale biology projects such as the sequencing of the human genome and gene expression surveys using RNA-seq, microarrays and other technologies have created a wealth of data for biologists. However, the challenge facing scientists is analyzing and even accessing these data to extract useful information pertaining to the system being studied. This course focuses on employing existing bioinformatic resources – mainly web-based programs and databases – to access the wealth of data to answer questions relevant to the average biologist, and is highly hands-on. Topics covered include multiple sequence alignments, phylogenetics, gene expression data analysis, and protein interaction networks, in two separate parts. The first part, Bioinformatic Methods I, dealt with databases, Blast, multiple sequence alignments, phylogenetics, selection analysis and metagenomics. This, the second part, Bioinformatic Methods II, will cover motif searching, protein-protein interactions, structural bioinformatics, gene expression data analysis, and cis-element predictions. This pair of courses is useful to any student considering graduate school in the biological sciences, as well as students considering molecular medicine. These courses are based on one taught at the University of Toronto to upper-level undergraduates who have some understanding of basic molecular biology. If you're not familiar with this, something like https://learn.saylor.org/course/bio101 might be helpful. No programming is required for this course although some command line work (though within a web browser) occurs in the 5th module. Bioinformatic Methods II is regularly updated, and was last updated for March 2019.Course 3: Plant Bioinformatics- The past 15 years have been exciting ones in plant biology. Hundreds of plant genomes have been sequenced, RNA-seq has enabled transcriptome-wide expression profiling, and a proliferation of ""-seq""-based methods has permitted protein-protein and protein-DNA interactions to be determined cheaply and in a high-throughput manner. These data sets in turn allow us to generate hypotheses at the click of a mouse. For instance, knowing where and when a gene is expressed can help us narrow down the phenotypic search space when we don't see a phenotype in a gene mutant under ""normal"" growth conditions. Coexpression analyses and association networks can provide high-quality candidate genes involved in a biological process of interest. Using Gene Ontology enrichment analysis and pathway visualization tools can help us make sense of our own 'omics experiments and answer the question ""what processes/pathways are being perturbed in our mutant of interest?"" Structure: each of the 6 week hands-on modules consists of a ~2 minute intro, a ~20 minute theory mini-lecture, a 1.5 hour hands-on lab, an optional ~20 minute lab discussion if experiencing difficulties with lab, and a ~2 minute summary. Tools covered: Module 1: GENOMIC DBs / PRECOMPUTED GENE TREES / PROTEIN TOOLS. Araport, TAIR, Gramene, EnsemblPlants Compara, PLAZA; SUBA4 and Cell eFP Browser, 1001 Genomes Browser Module 2: EXPRESSION TOOLS. eFP Browser / eFP-Seq Browser, Araport, Genevestigator, TravaDB, NCBI Genome Data Viewer for exploring RNA-seq data for many plant species other than Arabidopsis, MPSS database for small RNAs Module 3: COEXPRESSION TOOLS. ATTED II, Expression Angler, AraNet, AtCAST2 Module 4: PROMOTER ANALYSIS. Cistome, Athena, ePlant Module 5: GO ENRICHMENT ANALYSIS AND PATHWAY VIZUALIZATION. AgriGO, AmiGO, Classification SuperViewer, TAIR, g:profiler, AraCyc, MapMan (optional: Plant Reactome) Module 6: NETWORK EXPLORATION. Arabidopsis Interactions Viewer 2, ePlant, TF2Network, Virtual Plant, GeneMANIA [Material updated in June 2019]Course 4: Plant Bioinformatics Capstone- The past 15 years have been exciting ones in plant biology. Hundreds of plant genomes have been sequenced, RNA-seq has enabled transcriptome-wide expression profiling, and a proliferation of ""-seq""-based methods has permitted protein-protein and protein-DNA interactions to be determined cheaply and in a high-throughput manner. These data sets in turn allow us to generate hypotheses at the click of a mouse or tap of a finger. In Plant Bioinformatics on Coursera.org, we covered 33 plant-specific online tools from genome browsers to transcriptomic data mining to promoter/network analyses and others, and in this Plant Bioinformatics Capstone we'll use these tools to hypothesize a biological role for a gene of unknown function, summarized in a written lab report. This course is part of a Plant Bioinformatics Specialization on Coursera, which introduces core bioinformatic competencies and resources, such as NCBI's Genbank, Blast, multiple sequence alignments, phylogenetics in Bioinformatic Methods I, followed by protein-protein interactions, structural bioinformatics and RNA-seq analysis in Bioinformatic Methods II, in addition to the plant-specific concepts and tools introduced in Plant Bioinformatics and the Plant Bioinformatics Capstone. This course/capstone was developed with funding from the University of Toronto's Faculty of Arts and Science Open Course Initiative Fund (OCIF) and was implemented by Eddi Esteban, Will Heikoop and Nicholas Provart. Asher Pasha programmed a gene ID randomizer."
https://www.classcentral.com/course/python-data-science-18393,"Data is at the heart of our digital economy and Data Science has been ranked as the hottest profession of the 21st century. This 5 course Data Science with Python Professional Certificate program is aimed at preparing you for a career in Data Science and Machine Learning.
You will start by learning Python, the most popular language for Data Science. You will then develop skills for Data Analysis and Data Visualization and also get a practical introduction in Machine Learning. Finally you will apply and demonstrate your knowledge of Data Science and Machine Learning with a Capstone Project involving a real life business problem.
Taught by experts, the focus in this program is on hands-on learning and job readiness. As such you will work with real datasets and will be given no-charge access to tools like Jupyter notebooks in the IBM Cloud. You will utilize popular Python toolkits and libraries such as pandas, numpy, matplotlib, seaborn, folium, scipy, scikitlearn, etc.
Start developing data and analytical skills today and launch your career in Data Science, whether you are new to the job market or already in the workforce and looking to upskill yourself. No prior computer programming experience required.



Courses under this program:Course 1: Python Basics for Data Science
This Python course provides a beginner-friendly introduction to Python for Data Science. Practice through lab exercises, and you'll be ready to create your first Python scripts on your own!
Course 2: Analyzing Data with Python
In this course, you will learn how to analyze data in Python using multi-dimensional arrays in numpy, manipulate DataFrames in pandas, use SciPy library of mathematical routines, and perform machine learning using scikit-learn!
Course 3: Visualizing Data with Python
Data visualization is the graphical representation of data in order to interactively and efficiently convey insights to clients, customers, and stakeholders in general.
Course 4: Machine Learning with Python: A Practical Introduction
Machine Learning can be an incredibly beneficial tool to uncover hidden insights and predict future trends. This Machine Learning with Python course will give you all the tools you need to get started with supervised and unsupervised learning.
Course 5: Data Science and Machine Learning Capstone Project
Create a project that you can use to showcase your Data Science skills to prospective employers. Apply various data science and machine learning techniques to analyze and visualize a data set involving a real life business scenario and build a predictive model."
https://www.classcentral.com/course/ibm-data-science-18394,"Data science and machine learning skills continue to be in highest demand across industries, and the need for data practitioners is booming. Upon completing this Professional Certificate program, you will be armed with the skills and experience you need to start your career in data science and machine learning.
Through hands-on assignments and high-quality instruction, you will build a portfolio using real data science tools and real-world problems and data sets. The curriculum will cover a wide range of data science topics including: open source tools and libraries, methodologies, Python, databases, SQL, data visualization, data analysis, and machine learning. There is no requirement for prior computer science or programming knowledge in order to take this program.
Anyone with some computer skills and a passion for self-learning can succeed as we begin small and build up to more complex problems and topics.
With the tremendous need for data science and data analyst professionals in the market today, this program will jumpstart your path in data science and prepare you with a portfolio of data science deliverables to give you the confidence to take the plunge and start your data science career.



Courses under this program:Course 1: Introduction to Data Science
Learn about the would of data science first-hand from real data scientists.
Course 2: Data Science Tools
Learn about the most popular data science tools, including how to use them and what their features are.
Course 3: The Data Science Method
Learn about the methodology, practices and requirements behind data science to better understand how to problem solve with data and ensure data is relevant and properly manipulated to address a variety of real-world projects and business scenarios.
Course 4: SQL for Data Science
Learn how to use and apply the powerful language of SQL to better communicate and extract data from databases - a must for anyone working in the data science field.
Course 5: Python Basics for Data Science
This Python course provides a beginner-friendly introduction to Python for Data Science. Practice through lab exercises, and you'll be ready to create your first Python scripts on your own!
Course 6: Analyzing Data with Python
In this course, you will learn how to analyze data in Python using multi-dimensional arrays in numpy, manipulate DataFrames in pandas, use SciPy library of mathematical routines, and perform machine learning using scikit-learn!
Course 7: Visualizing Data with Python
Data visualization is the graphical representation of data in order to interactively and efficiently convey insights to clients, customers, and stakeholders in general.
Course 8: Machine Learning with Python: A Practical Introduction
Machine Learning can be an incredibly beneficial tool to uncover hidden insights and predict future trends. This Machine Learning with Python course will give you all the tools you need to get started with supervised and unsupervised learning.
Course 9: Data Science and Machine Learning Capstone Project
Create a project that you can use to showcase your Data Science skills to prospective employers. Apply various data science and machine learning techniques to analyze and visualize a data set involving a real life business scenario and build a predictive model."
https://www.classcentral.com/course/mathematics-for-data-science-18856,"Behind numerous standard models and constructions in Data Science there is mathematics that makes things work. It is important to understand it to be successful in Data Science. In this specialisation we will cover wide range of mathematical tools and see how they arise in Data Science. We will cover such crucial fields as Discrete Mathematics, Calculus, Linear Algebra and Probability. To make your experience more practical we accompany mathematics with examples and problems arising in Data Science and show how to solve them in Python.
      


          Course 1: Discrete Math and Analyzing Social Graphs- The main goal of this course is to introduce topics in Discrete Mathematics relevant to Data Analysis. We will start with a brief introduction to combinatorics, the branch of mathematics that studies how to count. Basics of this topic are critical for anyone working in Data Analysis or Computer Science. We will illustrate new knowledge, for example, by counting the number of features in data or by estimating the time required for a Python program to run. Next, we will apply our knowledge in combinatorics to study basic Probability Theory. Probability is everywhere in Data Analysis and we will study it in much more details later. Our goals for probability section in this course will be to give initial flavor of this field. Finally, we will study the combinatorial structure that is the most relevant for Data Analysis, namely graphs. Graphs can be found everywhere around us and we will provide you with numerous examples. We will mainly concentrate in this course on the graphs of social networks. We will provide you with relevant notions from the graph theory, illustrate them on the graphs of social networks and will study their basic properties. In the end of the course we will have a project related to social network graphs. As prerequisites we assume only basic math (e.g., we expect you to know what is a square or how to add fractions), basic programming in Python (functions, loops, recursion), common sense and curiosity. Our intended audience are all people that work or plan to work in Data Analysis, starting from motivated high school students.Course 2: Calculus and Optimization for Machine Learning- Hi! Our course aims to provide necessary background in Calculus sufficient for up-following Data Science courses. Course starts with basic introduction to concepts concerning functional mappings. Later students are assumed to study limits (in case of sequences, single- and multivariate functions), differentiability (once again starting from single variable up to multiple cases), integration, thus sequentially building up a base for the basic optimisation. To provide an understanding of the practical skills set being taught, the course introduces the final programming project considering the usage of optimisation routine in machine learning. Additional materials provided during the course include interactive plots in GeoGebra environment used during lectures, bonus reading materials with more general methods and more complicated basis for discussed themes.Course 3: First Steps in Linear Algebra for Machine Learning- The main goal of the course is to explain the main concepts of linear algebra that are used in data analysis and machine learning. Another goal is to improve the student’s practical skills of using linear algebra methods in machine learning and data analysis. You will learn the fundamentals of working with data in vector and matrix form, acquire skills for solving systems of linear algebraic equations and finding the basic matrix decompositions and general understanding of their applicability. This course is suitable for you if you are not an absolute beginner in Matrix Analysis or Linear Algebra (for example, have studied it a long time ago, but now want to take the first steps in the direction of those aspects of Linear Algebra that are used in Machine Learning). Certainly, if you are highly motivated in study of Linear Algebra for Data Sciences this course could be suitable for you as well.Course 4: Probability Theory, Statistics and Exploratory Data Analysis- Exploration of Data Science requires certain background in probability and statistics. This course introduces you to the necessary sections of probability theory and statistics, guiding you from the very basics all way up to the level required for jump starting your ascent in Data Science. The core concept of the course is random variable — i.e. variable whose values are determined by random experiment. Random variables are used as a model for data generation processes we want to study. Properties of the data are deeply linked to the corresponding properties of random variables, such as expected value, variance and correlations. Dependencies between random variables are crucial factor that allows us to predict unknown quantities based on known values, which forms the basis of supervised machine learning. We begin with the notion of independent events and conditional probability, then introduce two main classes of random variables: discrete and continuous and study their properties. Finally, we learn different types of data and their connection with random variables. While introducing you to the theory, we'll pay special attention to practical aspects for working with probabilities, sampling, data analysis, and data visualization in Python. This course requires basic knowledge in Discrete mathematics (combinatorics) and calculus (derivatives, integrals)."
https://www.classcentral.com/course/advanced-data-science-ibm-18680,"As a coursera certified specialization completer you will have a proven deep understanding on massive parallel data processing, data exploration and visualization, and advanced machine learning & deep learning. You'll understand the mathematical foundations behind all machine learning & deep learning algorithms. You can apply knowledge in practical use cases, justify architectural decisions, understand the characteristics of different algorithms, frameworks & technologies & how they impact model performance & scalability. If you choose to take this specialization and earn the Coursera specialization certificate, you will also earn an IBM digital badge. To find out more about IBM digital badges follow the link ibm.biz/badging.



          Course 1: Fundamentals of Scalable Data Science- Apache Spark is the de-facto standard for large scale data processing. This is the first course of a series of courses towards the IBM Advanced Data Science Specialization. We strongly believe that is is crucial for success to start learning a scalable data science platform since memory and CPU constraints are to most limiting factors when it comes to building advanced machine learning models. In this course we teach you the fundamentals of Apache Spark using python and pyspark. We'll introduce Apache Spark in the first two weeks and learn how to apply it to compute basic exploratory and data pre-processing tasks in the last two weeks. Through this exercise you'll also be introduced to the most fundamental statistical measures and data visualization technologies. This gives you enough knowledge to take over the role of a data engineer in any modern environment. But it gives you also the basis for advancing your career towards data science. Please have a look at the full specialization curriculum: https://www.coursera.org/specializations/advanced-data-science-ibm If you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge. To find out more about IBM digital badges follow the link ibm.biz/badging. After completing this course, you will be able to: • Describe how basic statistical measures, are used to reveal patterns within the data • Recognize data characteristics, patterns, trends, deviations or inconsistencies, and potential outliers. • Identify useful techniques for working with big data such as dimension reduction and feature selection methods • Use advanced tools and charting libraries to: o improve efficiency of analysis of big-data with partitioning and parallel analysis o Visualize the data in an number of 2D and 3D formats (Box Plot, Run Chart, Scatter Plot, Pareto Chart, and Multidimensional Scaling) For successful completion of the course, the following prerequisites are recommended: • Basic programming skills in python • Basic math • Basic SQL (you can get it easily from https://www.coursera.org/learn/sql-data-science if needed) In order to complete this course, the following technologies will be used: (These technologies are introduced in the course as necessary so no previous knowledge is required.) • Jupyter notebooks (brought to you by IBM Watson Studio for free) • ApacheSpark (brought to you by IBM Watson Studio for free) • Python This course takes four weeks, 4-6h per weekCourse 2: Advanced Machine Learning and Signal Processing- >>> By enrolling in this course you agree to the End User License Agreement as set out in the FAQ. Once enrolled you can access the license in the Resources area Course 3: Applied AI with DeepLearning- >>> By enrolling in this course you agree to the End User License Agreement as set out in the FAQ. Once enrolled you can access the license in the Resources area Course 4: Advanced Data Science Capstone- This project completer has proven a deep understanding on massive parallel data processing, data exploration and visualization, advanced machine learning and deep learning and how to apply his knowledge in a real-world practical use case where he justifies architectural decisions, proves understanding the characteristics of different algorithms, frameworks and technologies and how they impact model performance and scalability. Please note: You are requested to create a short video presentation at the end of the course. This is mandatory to pass. You don't need to share the video in public."
https://www.classcentral.com/course/gcp-data-machine-learning-18608,"This online specialization provides participants a hands-on introduction to designing and building data pipelines on Google Cloud Platform. Through a combination of presentations, demos, and hand-on labs, participants will learn how to design data processing systems, build end-to-end data pipelines, analyze data and derive insights. The course covers structured, unstructured, and streaming data. This course teaches the following skills: • Design and build data pipelines on Google Cloud Platform • Lift and shift your existing Hadoop workloads to the Cloud using Cloud Dataproc. • Process batch and streaming data by implementing autoscaling data pipelines on Cloud Dataflow • Manage your data Pipelines with Data Fusion and Cloud Composer. • Derive business insights from extremely large datasets using Google BigQuery • Learn how to use pre-built ML APIs on unstructured data and build different kinds of ML models using BigQuery ML. • Enable instant insights from streaming data This class is intended for developers who are responsible for: • Extracting, Loading, Transforming, cleaning, and validating data • Designing pipelines and architectures for data processing • Integrating analytics and machine learning capabilities into data pipelines • Querying datasets, visualizing query results and creating reports >>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<



            Read more
          



          Course 1: Google Cloud Platform Big Data and Machine Learning Fundamentals- This 2-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities. At the end of this course, participants will be able to: • Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform • Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform • Employ BigQuery and Cloud Datalab to carry out interactive data analysis • Choose between Cloud SQL, BigTable and Datastore • Train and use a neural network using TensorFlow • Choose between different data processing products on the Google Cloud Platform Before enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following: • A common query language such as SQL • Extract, transform, load activities • Data modeling • Machine learning and/or statistics • Programming in Python Google Account Notes: • Google services are currently unavailable in China.Course 2: Modernizing Data Lakes and Data Warehouses with GCP- The two key components of any data pipeline are data lakes and warehouses. This course highlights use-cases for each type of storage and dives into the available data lake and warehouse solutions on Google Cloud Platform in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a cloud environment. Learners will get hands-on experience with data lakes and warehouses on Google Cloud Platform using QwikLabs.Course 3: Building Batch Data Pipelines on GCP- Data pipelines typically fall under one of the Extra-Load, Extract-Load-Transform or Extract-Transform-Load paradigms. This course describes which paradigm should be used and when for batch data. Furthermore, this course covers several technologies on Google Cloud Platform for data transformation including BigQuery, executing Spark on Cloud Dataproc, pipeline graphs in Cloud Data Fusion and serverless data processing with Cloud Dataflow. Learners will get hands-on experience building data pipeline components on Google Cloud Platform using QwikLabs.Course 4: Building Resilient Streaming Analytics Systems on GCP- *Note: this is a new course with updated content from what you may have seen in the previous version of this Specialization. Processing streaming data is becoming increasingly popular as streaming enables businesses to get real-time metrics on business operations. This course covers how to build streaming data pipelines on Google Cloud Platform. Cloud Pub/Sub is described for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Cloud Dataflow, and how to store processed records to BigQuery or Cloud Bigtable for analysis. Learners will get hands-on experience building streaming data pipeline components on Google Cloud Platform using QwikLabs.Course 5: Smart Analytics, Machine Learning, and AI on GCP- Incorporating machine learning into data pipelines increases the ability of businesses to extract insights from their data. This course covers several ways machine learning can be included in data pipelines on Google Cloud Platform depending on the level of customization required. For little to no customization, this course covers AutoML. For more tailored machine learning capabilities, this course introduces AI Platform Notebooks and BigQuery Machine Learning. Also, this course covers how to productionalize machine learning solutions using Kubeflow. Learners will get hands-on experience building machine learning models on Google Cloud Platform using QwikLabs."
https://www.classcentral.com/course/executive-data-science-18529,"Assemble the right team, ask the right questions, and avoid the mistakes that derail data science projects.

In four intensive courses, you will learn what you need to know to begin assembling and leading a data science enterprise, even if you have never worked in data science before. You’ll get a crash course in data science so that you’ll be conversant in the field and understand your role as a leader. You’ll also learn how to recruit, assemble, evaluate, and develop a team with complementary skill sets and roles. You’ll learn the structure of the data science pipeline, the goals of each stage, and how to keep your team on target throughout. Finally, you’ll learn some down-to-earth practical skills that will help you overcome the common challenges that frequently derail data science projects.
      


          Course 1: A Crash Course in Data Science- By now you have definitely heard about data science and big data. In this one-week class, we will provide a crash course in what these terms mean and how they play a role in successful organizations. This class is for anyone who wants to learn what all the data science action is about, including those who will eventually need to manage data scientists. The goal is to get you up to speed as quickly as possible on data science without all the fluff. We've designed this course to be as convenient as possible without sacrificing any of the essentials. This is a focused course designed to rapidly get you up to speed on the field of data science. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know. 1. How to describe the role data science plays in various contexts 2. How statistics, machine learning, and software engineering play a role in data science 3. How to describe the structure of a data science project 4. Know the key terms and tools used by data scientists 5. How to identify a successful and an unsuccessful data science project 3. The role of a data science manager Course cover image by r2hox. Creative Commons BY-SA: https://flic.kr/p/gdMuhTCourse 2: Building a Data Science Team- Data science is a team sport. As a data science executive it is your job to recruit, organize, and manage the team to success. In this one-week course, we will cover how you can find the right people to fill out your data science team, how to organize them to give them the best chance to feel empowered and successful, and how to manage your team as it grows. This is a focused course designed to rapidly get you up to speed on the process of building and managing a data science team. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know. 1. The different roles in the data science team including data scientist and data engineer 2. How the data science team relates to other teams in an organization 3. What are the expected qualifications of different data science team members 4. Relevant questions for interviewing data scientists 5. How to manage the onboarding process for the team 6. How to guide data science teams to success 7. How to encourage and empower data science teams Commitment: 1 week of study, 4-6 hours Course cover image by JaredZammit. Creative Commons BY-SA. https://flic.kr/p/5vuWZzCourse 3: Managing Data Analysis- This one-week course describes the process of analyzing data and how to manage that process. We describe the iterative nature of data analysis and the role of stating a sharp question, exploratory data analysis, inference, formal statistical modeling, interpretation, and communication. In addition, we will describe how to direct analytic activities within a team and to drive the data analysis process towards coherent and useful results. This is a focused course designed to rapidly get you up to speed on the process of data analysis and how it can be managed. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know how to…. 1. Describe the basic data analysis iteration 2. Identify different types of questions and translate them to specific datasets 3. Describe different types of data pulls 4. Explore datasets to determine if data are appropriate for a given question 5. Direct model building efforts in common data analyses 6. Interpret the results from common data analyses 7. Integrate statistical findings to form coherent data analysis presentations Commitment: 1 week of study, 4-6 hours Course cover image by fdecomite. Creative Commons BY https://flic.kr/p/4HjmvDCourse 4: Data Science in Real Life- Have you ever had the perfect data science experience? The data pull went perfectly. There were no merging errors or missing data. Hypotheses were clearly defined prior to analyses. Randomization was performed for the treatment of interest. The analytic plan was outlined prior to analysis and followed exactly. The conclusions were clear and actionable decisions were obvious. Has that every happened to you? Of course not. Data analysis in real life is messy. How does one manage a team facing real data analyses? In this one-week course, we contrast the ideal with what happens in real life. By contrasting the ideal, you will learn key concepts that will help you manage real life analyses. This is a focused course designed to rapidly get you up to speed on doing data science in real life. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward. After completing this course you will know how to: 1, Describe the “perfect” data science experience 2. Identify strengths and weaknesses in experimental designs 3. Describe possible pitfalls when pulling / assembling data and learn solutions for managing data pulls. 4. Challenge statistical modeling assumptions and drive feedback to data analysts 5. Describe common pitfalls in communicating data analyses 6. Get a glimpse into a day in the life of a data analysis manager. The course will be taught at a conceptual level for active managers of data scientists and statisticians. Some key concepts being discussed include: 1. Experimental design, randomization, A/B testing 2. Causal inference, counterfactuals, 3. Strategies for managing data quality. 4. Bias and confounding 5. Contrasting machine learning versus classical statistical inference Course promo: https://www.youtube.com/watch?v=9BIYmw5wnBI Course cover image by Jonathan Gross. Creative Commons BY-ND https://flic.kr/p/q1vudbCourse 5: Executive Data Science Capstone- The Executive Data Science Capstone, the specialization’s culminating project, is an opportunity for people who have completed all four EDS courses to apply what they've learned to a real-world scenario developed in collaboration with Zillow, a data-driven online real estate and rental marketplace, and DataCamp, a web-based platform for data science programming. Your task will be to lead a virtual data science team and make key decisions along the way to demonstrate that you have what it takes to shepherd a complex analysis project from start to finish. For the final project, you will prepare and submit a presentation, which will be evaluated and graded by your fellow capstone participants. Course cover image by Luckey_sun. Creative Commons BY-SA https://flic.kr/p/bx1jvU"
https://www.classcentral.com/course/gcp-data-engineering-18598,"This program provides the skills you need to advance your career in data engineering and recommends training to support your preparation for the industry-recognized Google Cloud Professional Data Engineer certification. Through a combination of presentations, demos, and labs, you will enable data-driven decision making by collecting, transforming, and publishing data; and you'll gain real world experience through a number of hands-on Qwiklabs projects. You'll also have the opportunity to practice key job skills, including designing, building, and running data processing systems; and operationalizing machine-learning models. Upon successful completion of this program, you will earn a certificate of completion to share with your professional network and potential employers. If you would like to become Google Cloud certified and demonstrate your proficiency to design and build data processing systems and operationalize machine learning models on Google Cloud Platform, you will need to register for, and pass the official Google Cloud certification exam. You can find more details on how to register and additional resources to support your preparation at cloud.google.com/certifications.



          Course 1: Google Cloud Platform Big Data and Machine Learning Fundamentals- This 2-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities. At the end of this course, participants will be able to: • Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform • Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform • Employ BigQuery and Cloud Datalab to carry out interactive data analysis • Choose between Cloud SQL, BigTable and Datastore • Train and use a neural network using TensorFlow • Choose between different data processing products on the Google Cloud Platform Before enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following: • A common query language such as SQL • Extract, transform, load activities • Data modeling • Machine learning and/or statistics • Programming in Python Google Account Notes: • Google services are currently unavailable in China.Course 2: Modernizing Data Lakes and Data Warehouses with GCP- The two key components of any data pipeline are data lakes and warehouses. This course highlights use-cases for each type of storage and dives into the available data lake and warehouse solutions on Google Cloud Platform in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a cloud environment. Learners will get hands-on experience with data lakes and warehouses on Google Cloud Platform using QwikLabs.Course 3: Building Batch Data Pipelines on GCP- Data pipelines typically fall under one of the Extra-Load, Extract-Load-Transform or Extract-Transform-Load paradigms. This course describes which paradigm should be used and when for batch data. Furthermore, this course covers several technologies on Google Cloud Platform for data transformation including BigQuery, executing Spark on Cloud Dataproc, pipeline graphs in Cloud Data Fusion and serverless data processing with Cloud Dataflow. Learners will get hands-on experience building data pipeline components on Google Cloud Platform using QwikLabs.Course 4: Building Resilient Streaming Analytics Systems on GCP- *Note: this is a new course with updated content from what you may have seen in the previous version of this Specialization. Processing streaming data is becoming increasingly popular as streaming enables businesses to get real-time metrics on business operations. This course covers how to build streaming data pipelines on Google Cloud Platform. Cloud Pub/Sub is described for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Cloud Dataflow, and how to store processed records to BigQuery or Cloud Bigtable for analysis. Learners will get hands-on experience building streaming data pipeline components on Google Cloud Platform using QwikLabs.Course 5: Smart Analytics, Machine Learning, and AI on GCP- Incorporating machine learning into data pipelines increases the ability of businesses to extract insights from their data. This course covers several ways machine learning can be included in data pipelines on Google Cloud Platform depending on the level of customization required. For little to no customization, this course covers AutoML. For more tailored machine learning capabilities, this course introduces AI Platform Notebooks and BigQuery Machine Learning. Also, this course covers how to productionalize machine learning solutions using Kubeflow. Learners will get hands-on experience building machine learning models on Google Cloud Platform using QwikLabs.Course 6: Preparing for the Google Cloud Professional Data Engineer Exam- From the course: ""The best way to prepare for the exam is to be competent in the skills required of the job."" This course uses a top-down approach to recognize knowledge and skills already known, and to surface information and skill areas for additional preparation. You can use this course to help create your own custom preparation plan. It helps you distinguish what you know from what you don't know. And it helps you develop and practice skills required of practitioners who perform this job. The course follows the organization of the Exam Guide outline, presenting highest-level concepts, ""touchstones"", for you to determine whether you feel confident about your knowledge of that area and its dependent concepts, or if you want more study. You also will learn about and have the opportunity to practice key job skills, including cognitive skills such as case analysis, identifying technical watchpoints, and developing proposed solutions. These are job skills that are also exam skills. You will also test your basic abilities with Activity Tracking Challenge Labs. And you will have many sample questions similar to those on the exam, including solutions. The end of the course contains an ungraded practice exam quiz, followed by a graded practice exam quiz that simulates the exam-taking experience."
